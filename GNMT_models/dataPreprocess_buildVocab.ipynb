{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "data_preprocess_build_vocab.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "q1voNxllpe7X",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KeZi1feDpmC4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "w8E95xGVp4LV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "3f560d86-15fc-499c-a16e-db4aec57aafc"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mdrive\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9bSQ4Twip6Dz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "063cecf9-fdbc-4744-cb08-7f3ccbec8c84"
      },
      "cell_type": "code",
      "source": [
        "cd drive"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6Lw3z-wPp8Ac",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Qt6mA4pWqqls",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Split the training set into small txt files"
      ]
    },
    {
      "metadata": {
        "id": "gq7MGm-nqp4G",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def split_files_en(big_filename, limit):\n",
        "    file_count = 0\n",
        "    url_list = []\n",
        "\n",
        "    with open(big_filename) as f:\n",
        "        for line in f:\n",
        "            url_list.append(line)\n",
        "            if len(url_list) < limit:\n",
        "                continue\n",
        "            file_name = \"./small_txt/\" + str(file_count) + \"_en.txt\"\n",
        "        \n",
        "            with open(file_name, 'w') as file:\n",
        "                for url in url_list[:-1]:\n",
        "                    file.write(url)\n",
        "                file.write(url_list[-1].strip())\n",
        "                url_list = []\n",
        "                file_count += 1\n",
        "\n",
        "    if url_list:\n",
        "        file_name = str(file_count) + \".txt\"\n",
        "        with open(file_name, 'w') as file:\n",
        "            for url in url_list:\n",
        "                file.write(url)\n",
        "    print(\"done\")\n",
        "    \n",
        "    \n",
        "def split_files_fr(big_filename, limit):\n",
        "    file_count = 0\n",
        "    url_list = []\n",
        "\n",
        "    with open(big_filename) as f:\n",
        "        for line in f:\n",
        "            url_list.append(line)\n",
        "            if len(url_list) < limit:\n",
        "                continue\n",
        "            file_name = \"../small_txt/\" + str(file_count) + \"_fr.txt\"\n",
        "        \n",
        "            with open(file_name, 'w') as file:\n",
        "                for url in url_list[:-1]:\n",
        "                    file.write(url)\n",
        "                file.write(url_list[-1].strip())\n",
        "                url_list = []\n",
        "                file_count += 1\n",
        "\n",
        "    if url_list:\n",
        "        file_name = str(file_count) + \".txt\"\n",
        "        with open(file_name, 'w') as file:\n",
        "            for url in url_list:\n",
        "                file.write(url)\n",
        "    print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sw4vj4uSqu0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "0db70b79-f995-4aea-d412-a93698c51b32"
      },
      "cell_type": "code",
      "source": [
        "#### count the number of lines in the text\n",
        "\n",
        "en_count = -1\n",
        "for en_count,line in enumerate(open(r\"./translation_data/europarl-v7.fr-en.en\")):\n",
        "    pass\n",
        "en_count += 1\n",
        "print(en_count)\n",
        "\n",
        "fr_count = -1\n",
        "for fr_count,line in enumerate(open(r\"./translation_data/europarl-v7.fr-en.fr\")):\n",
        "    pass\n",
        "fr_count += 1\n",
        "print(fr_count)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2007723\n",
            "2007723\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pu0omoIyqwI3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "limit = 150000\n",
        "bigfilename_en = \"./europarl-v7.fr-en.en\"\n",
        "#bigfilename_fr = \"./europarl-v7.fr-en.fr\"\n",
        "\n",
        "split_files_en(bigfilename_en, limit)\n",
        "#split_files_fr(bigfilename_fr, limit)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8qeAlIJuqxu0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Build vocab dictionary"
      ]
    },
    {
      "metadata": {
        "id": "jonMGjIrsWOT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZZV_E5FVqCaK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def train_vocab_en(n):  \n",
        "    # get vocab in train which show more than n times\n",
        "    \n",
        "    vocab = {'<PAD>': 0, '<OOV>': 1}\n",
        "    \n",
        "    word_count = collections.defaultdict(float)\n",
        "    \n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "    \n",
        "    for i in range(13):\n",
        "      data = _read_words(\"./small_txt/\" + str(i) + \"_en.txt\")\n",
        "      data = en_tokenize(data)\n",
        "    \n",
        "      for token in data:\n",
        "        word_count[token] += 1.0\n",
        "                          \n",
        "    total_word = set(word_count)\n",
        "    \n",
        "    for word in total_word:\n",
        "        if word_count[word] > n:\n",
        "            vocab[word] = len(vocab)  # word only show less than n times became oov\n",
        "            \n",
        "    return vocab\n",
        "  \n",
        "def train_vocab_fr(n):  \n",
        "    # get vocab in train which show more than n times\n",
        "    \n",
        "    vocab = {'<PAD>': 0, '<OOV>': 1}\n",
        "    \n",
        "    word_count = collections.defaultdict(float)\n",
        "    \n",
        "    en_tokenize = MosesTokenizer('fr')\n",
        "    \n",
        "    for i in range(13):\n",
        "      data = _read_words(\"./small_txt/\" + str(i) + \"_fr.txt\")\n",
        "      data = en_tokenize(data)\n",
        "    \n",
        "      for token in data:\n",
        "        word_count[token] += 1.0\n",
        "                          \n",
        "    total_word = set(word_count)\n",
        "    \n",
        "    print(total_word)\n",
        "    for word in total_word:\n",
        "        if word_count[word] > n:\n",
        "            vocab[word] = len(vocab)  # word only show less than n times became oov\n",
        "            \n",
        "    return vocab"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SamJie1aqLhd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_en = train_vocab_en(5)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NvOEWJWs04fF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "fdc6276b-2f00-4aac-81b6-a527beaefd68"
      },
      "cell_type": "code",
      "source": [
        "len(vocab_en)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "38841"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "metadata": {
        "id": "ZvK0FftlqPhc",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "vocab_fr = train_vocab_fr(10)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5GTfi50PegYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## save dict to txt file\n",
        "\n",
        "f = open(\"./dictionary/en_word_to_id_new.txt\", \"wb\")\n",
        "pickle.dump(vocab_en, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MVGzwpJKqV7N",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## save dict to txt file\n",
        "\n",
        "f = open(\"./en_word_to_id.txt\", \"wb\")\n",
        "pickle.dump(vocab_en, f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"./fr_word_to_id.txt\", \"wb\")\n",
        "pickle.dump(vocab_fr, f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QtaAsF2cqZwX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## read dict from txt file\n",
        "\n",
        "f = open(\"./en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"./fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siVY68GnrRAn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "6348d0ca-3e10-4fca-c646-4d5963e1607a"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "\n",
        "fr_word_to_id['<beg>'] = len(fr_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30772"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "metadata": {
        "id": "wB_DUYv-sGfw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Word_to_id"
      ]
    },
    {
      "metadata": {
        "id": "DoSBKzOksF-i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YHQKDA-gtiZj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)\n",
        "              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)\n",
        "            \n",
        "    return data, each_sen_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GOEOqYsBruh6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length=30)\n",
        "\n",
        "    out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "In7yncjr2UfT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "asQpSeMStFX6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt = 0\n",
        "        \n",
        "en_file = \"./small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"./small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "print(en_file)\n",
        "print(fr_file)\n",
        "        \n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "hTnAcjJ1M0lM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Load test data"
      ]
    },
    {
      "metadata": {
        "id": "iNd0x973M3gv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_en_test = \"./translation_test/newstest2008.en\"\n",
        "\n",
        "file_fr_test = \"./translation_test/newstest2008.fr\"\n",
        "\n",
        "en_test, en_test_len = generate_input_en(file_en_test, en_word_to_id, max_length=30)\n",
        "\n",
        "fr_test, fr_test_len = generate_input_en(file_fr_test, fr_word_to_id, max_length=30)\n",
        "\n",
        "file_en_test = \"./translation_test/newstest2009.en\"\n",
        "\n",
        "file_fr_test = \"./translation_test/newstest2009.fr\"\n",
        "\n",
        "en_test_2, en_test_len_2 = generate_input_en(file_en_test, en_word_to_id, max_length=30)\n",
        "\n",
        "fr_test_2, fr_test_len_2 = generate_input_en(file_fr_test, fr_word_to_id, max_length=30)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}