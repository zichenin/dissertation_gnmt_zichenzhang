{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Copy_of_Gene_translation_cnn_0812 (1).ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "BSBf8rYS9hX9",
        "gJjC0NuO9oE4",
        "E546I60IPRWK",
        "DqH5fygZc0yl",
        "qYw-PZxCWtMM",
        "7fbx0ui5Ls9v",
        "mDFnWZP5F3OC",
        "6CKhxVAVMQQb",
        "CcCYvSrNMpDp"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rMcrrnEBsjy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the drive path"
      ]
    },
    {
      "metadata": {
        "id": "66FgEFRN-AMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWRjvljv-GT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vn79dig8uT8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "9e94ed6e-6838-4a63-85f0-c01437c755a3"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MTeUgeB-seY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "66c26ef2-8bde-4d51-f8eb-4dea0107af7f"
      },
      "cell_type": "code",
      "source": [
        "cd drive/atten_resi_dcnn"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/atten_resi_dcnn\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B12HWS4xsx4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ]
    },
    {
      "metadata": {
        "id": "Ly79OANzc7RS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "842o8uvRtBnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "wjDy6-mfnnit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load vocab dict from txt file\n",
        "\n",
        "f = open(\"../small_txt/en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../small_txt/fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuETWPMbu2Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "090ffe4d-3ec1-43b6-8aea-e4c598c6c65a"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "en_eos = en_word_to_id['eos']\n",
        "fr_eos = fr_word_to_id['eos']\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30772\n",
            "39578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9ifxSnpu6zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output\n",
        "\n",
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list\n",
        "\n",
        "\n",
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y57-AuNCvDSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length=30)\n",
        "\n",
        "    #out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    #fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehsowT7hwyjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juNYK867gw3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_oov_id = en_word_to_id['<OOV>']\n",
        "fr_oov_id = fr_word_to_id['<OOV>']\n",
        "\n",
        "def dropout_func(decode_input, dropout_prob, oov_id):\n",
        "  for i in range(decode_input.shape[0]):\n",
        "    for j in range(decode_input.shape[1]):\n",
        "        for k in range(1,decode_input.shape[2]):\n",
        "            if np.random.uniform() > dropout_prob:\n",
        "                decode_input[i,j,k] = oov_id        \n",
        "  return decode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq37O5wR21AC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "zcHfesh3uCDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define parameters ######################\n",
        "\n",
        "max_length = 30\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "embed_size = 300\n",
        "\n",
        "infer_hidden_size = 1000\n",
        "\n",
        "latent_size = 200\n",
        "\n",
        "latent_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FraHS_clcvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ###################### generate sentence #######################\n",
        "batch_size = 1\n",
        "\n",
        "latent_num = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJNVkwnhdg9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define placeholder ######################\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input')         # batch_size x max_length\n",
        "\n",
        "target_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target')       # batch_size x max_length\n",
        "\n",
        "in_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'in_len')              # batch_size x 1\n",
        "\n",
        "out_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'out_len')            # batch_size x 1\n",
        "\n",
        "discount_placeholder = tf.placeholder(tf.float32, name='discount')\n",
        "\n",
        "lr_placeholder = tf.placeholder(tf.float32, name='learn_rate')\n",
        "\n",
        "input_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input_drop')   # batch_size x max_length\n",
        "\n",
        "target_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target_drop') # batch_size x max_length\n",
        "\n",
        "if_gene_placeholder = tf.placeholder(tf.bool, name='if_gene')\n",
        "\n",
        "latent_var_placeholder = tf.placeholder(tf.float32, [latent_num, batch_size, max_length, latent_size], 'la_var')       # batch_size x max_length x latent_size\n",
        "\n",
        "xavier_initializer = tf.contrib.layers.xavier_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dqobGQGkHNg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### embedding look-up for input sentences ####################\n",
        "\n",
        "with tf.variable_scope('en_embedding'):\n",
        "    en_embedding = tf.get_variable('en_embeding',[en_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    inputs = tf.nn.embedding_lookup(en_embedding, input_placeholder)                      # batch_size x max_length x embed_size\n",
        "    inputs_drop = tf.nn.embedding_lookup(en_embedding, input_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    \n",
        "\n",
        "with tf.variable_scope('fr_embedding'):\n",
        "    fr_embedding = tf.get_variable('fr_embeding',[fr_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    targets = tf.nn.embedding_lookup(fr_embedding, target_placeholder)                      # batch_size x max_length x embed_size\n",
        "    targets_drop = tf.nn.embedding_lookup(fr_embedding, target_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmvdTgj5tSpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Inference Model - Encoder\n",
        "\n",
        "$q(z_1, z_2, ... , z_T|x,y)$\n",
        "\n",
        "Similar to the encoder of RNNSearch"
      ]
    },
    {
      "metadata": {
        "id": "DSPN85dailPC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### Inference model  #######################\n",
        "\n",
        "encode_inputs = tf.transpose(tf.concat([inputs, targets], axis=2), (1,0,2))\n",
        "\n",
        "with tf.variable_scope('encode'):\n",
        "    #basic_cell =tf.contrib.rnn.GRUCell(infer_hidden_size)\n",
        "    basic_cell = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state = basic_cell.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs, encode_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell, \n",
        "                                                                   cell_bw=basic_cell, \n",
        "                                                                   inputs=encode_inputs,                                                                \n",
        "                                                                   initial_state_fw=init_state,\n",
        "                                                                   initial_state_bw=init_state,\n",
        "                                                                   dtype=tf.float32,\n",
        "                                                                   time_major=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6h0aG1lX1ug4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo0Ycpq9invD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ##################### Inference model  #######################\n",
        "\n",
        "# ##################### bi-direction lstm of source sentence ######################\n",
        "\n",
        "# encode_inputs_x = tf.transpose(inputs, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_x'):\n",
        "#     basic_cell_x = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_x = basic_cell_x.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_x, encode_state_x = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_x, \n",
        "#                                                                        cell_bw=basic_cell_x, \n",
        "#                                                                        inputs=encode_inputs_x,\n",
        "#                                                                        sequence_length=in_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_x,\n",
        "#                                                                        initial_state_bw=init_state_x,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_x: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##################### bi-direction lstm of target sentence ######################\n",
        "\n",
        "# encode_inputs_y = tf.transpose(targets, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_y'):\n",
        "#     basic_cell_y = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_y = basic_cell_y.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_y, encode_state_y = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_y, \n",
        "#                                                                        cell_bw=basic_cell_y, \n",
        "#                                                                        inputs=encode_inputs_y,\n",
        "#                                                                        sequence_length=out_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_y,\n",
        "#                                                                        initial_state_bw=init_state_y,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_y: max_length x batch_size x infer_hidden_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYfWtHL8irsr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "# en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "# en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "# #en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size)) # batch_size*max_length x 2*infer_hidden_size\n",
        "\n",
        "# ##################### concatenate the state of encoder of x and y ######################\n",
        "\n",
        "# fw_bw_en_state_x = tf.concat((encode_state_x[0][1],encode_state_x[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state_y = tf.concat((encode_state_y[0][1],encode_state_y[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state = tf.concat((fw_bw_en_state_x, fw_bw_en_state_y), 1)             # en_max_length x  4*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state = tf.tile(tf.expand_dims(fw_bw_en_state, axis=1), (1,30,1))\n",
        "\n",
        "\n",
        "# fw_bw_en = tf.concat((fw_bw_en_state, en_outputs_tran), axis=2)\n",
        "\n",
        "# fw_bw_en = tf.reshape(fw_bw_en, (batch_size*max_length, 6*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Yq7w0u-iyl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('encode_projection'):\n",
        "    W_1 = tf.get_variable('W_1',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_1 = tf.get_variable('b_1',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_2 = tf.get_variable('W_2',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_2 = tf.get_variable('b_2',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gz4FQdg_i4b8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fw_bw_en_outputs_norm = tf.contrib.layers.batch_norm(fw_bw_en_outputs_resh, center=True, scale=True)\n",
        "\n",
        "la_mean = tf.matmul(en_outputs_resh, W_1) + b_1                              # batch_size*max_length x latent_size \n",
        "\n",
        "la_log_var = tf.matmul(en_outputs_resh, W_2) + b_2                           # batch_size*max_length x latent_size \n",
        "la_var = tf.exp(la_log_var)\n",
        "la_std = tf.sqrt(la_var)\n",
        "\n",
        "kl_div_loss = 1 + la_log_var - tf.square(la_mean) - la_var                      # batch_size*max_length x latent_size\n",
        "kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, axis=1)                         # batch_size*max_length x 1\n",
        "kl_div_loss = tf.reshape(kl_div_loss, (batch_size, max_length))                 # batch_size x max_length\n",
        "kl_div_loss = tf.reduce_sum(kl_div_loss, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2-aUlz0i7Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# latent_variables_v = []\n",
        "# for _ in range(latent_num):\n",
        "#   eposida = tf.random_normal(tf.shape(la_std), mean=0.0,stddev=1)\n",
        "#   latent_variables_sample = la_mean + la_std*eposida\n",
        "#   latent_variables_sample = tf.reshape(latent_variables_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "#   latent_variables_v.append(latent_variables_sample)\n",
        "\n",
        "# def if_true():\n",
        "#   latent_v = []\n",
        "#   for h in range(latent_num):\n",
        "#     latent_v.append(latent_var_placeholder[h])\n",
        "#   return latent_v\n",
        "\n",
        "# def if_false():\n",
        "#   return latent_variables_v\n",
        "\n",
        "# latent_variables = tf.cond(if_gene_placeholder, if_true, if_false)\n",
        "\n",
        "# if latent_num == 1:\n",
        "#   new_latent_variables = []\n",
        "#   new_latent_variables.append(latent_variables)\n",
        "#   latent_variables = new_latent_variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDVWXdubzXh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_v = []\n",
        "for h in range(latent_num):\n",
        "  latent_v.append(latent_var_placeholder[h])\n",
        "    \n",
        "latent_variables = latent_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on6adzC8518v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Generation Model - Decoder\n",
        "\n",
        "$p_\\theta(x|z_1, z_2, ... , z_T)$\n",
        "\n",
        "$p_\\theta(y|z_1, z_2, ... , z_T)$"
      ]
    },
    {
      "metadata": {
        "id": "tiEDu_jlXEtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filter_num = 150\n",
        "\n",
        "filter_size = 3\n",
        "\n",
        "filter_size_only_pre = 2\n",
        "\n",
        "filter_size_pad = filter_size - filter_size_only_pre\n",
        "\n",
        "filter_zero_pad = tf.zeros(shape=[filter_size_pad, embed_size+latent_size, filter_num], dtype=tf.float32)\n",
        "filter_zero_pad_2 = tf.zeros(shape=[1, filter_size_pad, filter_num, filter_num], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xjcUuz8EITGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## X decoder"
      ]
    },
    {
      "metadata": {
        "id": "Mn0rAJKkpdDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "atten_size = 1000\n",
        "maxout_size = 500\n",
        "\n",
        "with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "    f_x_1 = tf.get_variable(\"x_filter_1\", shape=[3, latent_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_1_dia = tf.concat([f_x_1, \n",
        "                           tf.zeros((2,latent_size+latent_size,filter_num))], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, latent_size+filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((1,latent_size+filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_2[1],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((1,latent_size+filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_2[2],(1,latent_size+filter_num,filter_num)),\n",
        "                           tf.zeros((4,latent_size+filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, latent_size+filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((3,latent_size+filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_3[1],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((3,latent_size+filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_3[2],(1,latent_size+filter_num,filter_num)),\n",
        "                           tf.zeros((8,latent_size+filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, latent_size+filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((7,latent_size+filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_4[1],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((7,latent_size+filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_4[2],(1,latent_size+filter_num,filter_num)),\n",
        "                           tf.zeros((16,latent_size+filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_x'):\n",
        "    proj_w_x = tf.get_variable('project_w_x', [maxout_size,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### sequence weight of x\n",
        "squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length\n",
        "\n",
        "#### attention model ####\n",
        "with tf.variable_scope('encode_projection_x'):\n",
        "    U_a_x = tf.get_variable('U_a_x',[latent_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_a_x = tf.get_variable('W_a_x',[embed_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_a_x = tf.get_variable('V_a_x',[atten_size,1], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### maxout model ####\n",
        "with tf.variable_scope('x_probability'):\n",
        "    U_o_x = tf.get_variable('U_o_x',[filter_num, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_o_x = tf.get_variable('V_o_x',[latent_size,  2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_o_x = tf.get_variable('C_o_x',[latent_size, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tx5_58s2o8PQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "WaXi_1 = tf.matmul(tf.reshape(x_with_beg[:,:30,:], (batch_size*max_length, embed_size)), W_a_x)   # batch_size*max_length x atten_size\n",
        "\n",
        "WaXi_1 = tf.reshape(WaXi_1, (batch_size, max_length, atten_size))                               #  batch_size x max_length x atten_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmCR8bcFp1Ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder(WaXi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_x)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaX0_allsteps = tf.tile(WaXi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaX0_allsteps + UaZj), V_a_x), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "      \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2),\n",
        "                                  f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(tf.concat((de_latent, x_out_conv_dia_1), axis=2),\n",
        "                                  f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(tf.concat((de_latent, x_out_conv_dia_2), axis=2), \n",
        "                                  f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(tf.concat((de_latent, x_out_conv_dia_3), axis=2), \n",
        "                                  f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(x_out_conv_dia, U_o_x) #+ tf.matmul(de_latent_resh, V_o_x) + tf.matmul(context_c_resh, C_o_x)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  x_out_project = tf.matmul(t_1, proj_w_x) + proj_b_x                                                           # batch_size*max_length x embed_size \n",
        "  \n",
        "  target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "  logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "  prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_x\n",
        "\n",
        "log_liki_x_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_x_to.append(x_decoder(WaXi_1,latent_variables[l]))\n",
        "log_liki_x_to = tf.stack(log_liki_x_to, axis=0)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4a_a-BWwobx-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder_gene(WaXi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_x)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaX0_allsteps = tf.tile(WaXi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaX0_allsteps + UaZj), V_a_x), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(tf.concat((de_latent, x_out_conv_dia_1), axis=2),\n",
        "                                  f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(tf.concat((de_latent, x_out_conv_dia_2), axis=2), \n",
        "                                  f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(tf.concat((de_latent, x_out_conv_dia_3), axis=2), \n",
        "                                  f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  #context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(x_out_conv_dia, U_o_x) + tf.matmul(de_latent_resh, V_o_x) #+ tf.matmul(context_c_resh, C_o_x)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  x_out_project = tf.matmul(t_1, proj_w_x) + proj_b_x                                                           # batch_size*max_length x embed_size \n",
        "  \n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "                                               \n",
        "  return logits_x\n",
        "\n",
        "logits_gene_x_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_x_to.append(x_decoder_gene(WaXi_1,latent_variables[l]))\n",
        "logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLbV5QibKQQK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Y decoder"
      ]
    },
    {
      "metadata": {
        "id": "PVd3Oe-0KRjT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "    f_y_1 = tf.get_variable(\"y_filter_1\", shape=[3, latent_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_1_dia = tf.concat([f_y_1, \n",
        "                           tf.zeros((2, latent_size+latent_size,filter_num))], axis=0)  \n",
        "                                    \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, latent_size+filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,latent_size+filter_num, filter_num)), \n",
        "                           tf.zeros((1,latent_size+filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_2[1],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((1,latent_size+filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_2[2],(1,latent_size+filter_num,filter_num)),\n",
        "                           tf.zeros((4,latent_size+filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, latent_size+filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((3,latent_size+filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_3[1],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((3,latent_size+filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_3[2],(1,latent_size+filter_num,filter_num)),\n",
        "                           tf.zeros((8,latent_size+filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, latent_size+filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((7,latent_size+filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_4[1],(1,latent_size+filter_num,filter_num)), \n",
        "                           tf.zeros((7,latent_size+filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_4[2],(1,latent_size+filter_num,filter_num)),\n",
        "                           tf.zeros((16,latent_size+filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_y'):\n",
        "    proj_w_y = tf.get_variable('project_w_y', [maxout_size,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "#### sequence weight of y\n",
        "squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length\n",
        "\n",
        "#### attention model ####\n",
        "with tf.variable_scope('encode_projection_y'):\n",
        "    U_a_y = tf.get_variable('U_a_y',[latent_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_a_y = tf.get_variable('W_a_y',[embed_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_a_y = tf.get_variable('V_a_y',[atten_size,1], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### maxout model ####\n",
        "with tf.variable_scope('y_probability'):\n",
        "    U_o_y = tf.get_variable('U_o_y',[filter_num, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_o_y = tf.get_variable('V_o_y',[latent_size,  2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_o_y = tf.get_variable('C_o_y',[latent_size, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLMIovKTKp52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "WaYi_1 = tf.matmul(tf.reshape(y_with_beg[:,:30,:], (batch_size*max_length, embed_size)), W_a_y)   # batch_size*max_length x atten_size\n",
        "\n",
        "WaYi_1 = tf.reshape(WaYi_1, (batch_size, max_length, atten_size))                               #  batch_size x max_length x atten_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9l_wgIkK6Fv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder(WaYi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_y)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaY0_allsteps = tf.tile(WaYi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaY0_allsteps + UaZj), V_a_y), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_y_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(tf.concat((de_latent, y_out_conv_dia_1), axis=2), \n",
        "                                  f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(tf.concat((de_latent, y_out_conv_dia_2), axis=2),  \n",
        "                                  f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(tf.concat((de_latent, y_out_conv_dia_3), axis=2),  \n",
        "                                  f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  \n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(y_out_conv_dia, U_o_y) #+ tf.matmul(de_latent_resh, V_o_y) + tf.matmul(context_c_resh, C_o_y)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  y_out_project = tf.matmul(t_1, proj_w_y) + proj_b_y                                                           # batch_size*max_length x embed_size   \n",
        "  \n",
        "  \n",
        "  target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "  logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "  prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_y\n",
        "\n",
        "log_liki_y_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_y_to.append(y_decoder(WaYi_1,latent_variables[l]))\n",
        "log_liki_y_to = tf.stack(log_liki_y_to, axis=0)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OJUa6WaaJlmj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder_gene(WaYi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_y)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaY0_allsteps = tf.tile(WaYi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaY0_allsteps + UaZj), V_a_y), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_y_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(tf.concat((de_latent, y_out_conv_dia_1), axis=2), \n",
        "                                  f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(tf.concat((de_latent, y_out_conv_dia_2), axis=2),  \n",
        "                                  f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(tf.concat((de_latent, y_out_conv_dia_3), axis=2),  \n",
        "                                  f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  \n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(y_out_conv_dia, U_o_y) #+ tf.matmul(de_latent_resh, V_o_y) + tf.matmul(context_c_resh, C_o_y)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  y_out_project = tf.matmul(t_1, proj_w_y) + proj_b_y                                                           # batch_size*max_length x embed_size   \n",
        "  \n",
        "  \n",
        "  target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "  \n",
        "  return logits_y\n",
        "\n",
        "logits_gene_y_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_y_to.append(y_decoder_gene(WaYi_1,latent_variables[l]))\n",
        "logits_gene_y_to = tf.stack(logits_gene_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSBf8rYS9hX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Generation Model for source sentence $p_\\theta(x|z_1, z_2, ... , z_T)$\n"
      ]
    },
    {
      "metadata": {
        "id": "UNQlel3UksZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### concat beg token with input\n",
        "\n",
        "# #beg_token_x = tf.zeros((1,embed_size))\n",
        "# beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "# x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "# x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "# x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "# #x_input_cnn_1 = tf.concat([latent_variables_1,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "# #x_input_cnn_2 = tf.concat([latent_variables_2,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "# #x_input_cnn_4D = tf.expand_dims(x_input_cnn, axis=1)                                        # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "# x_input_cnn = []\n",
        "# for l in range(latent_num):\n",
        "#   x_input_cnn.append(tf.concat([latent_variables[l],x_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4x0f5AYajCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "#     f_x_1 = tf.get_variable(\"x_filter_1\", shape=[2, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_1_dia = tf.concat([f_x_1, \n",
        "#                            tf.zeros((1,embed_size+latent_size,filter_num))], axis=0)                                     \n",
        "#     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "#     f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size, filter_num)), \n",
        "#                            tf.reshape(f_x_2[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_x_2[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((4,filter_num+latent_size,filter_num)),], axis=0)\n",
        "#     # 9 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_x_3[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_x_3[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((8,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 13 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_x_4[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_x_4[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((16,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 21 x filter_num x filter_num\n",
        "    \n",
        "# #     f_x_5 = tf.get_variable(\"x_filter_5\", shape=[2, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "# #     f_x_5_dia = tf.concat([f_x_5, \n",
        "# #                            tf.zeros((1,filter_num+latent_size,filter_num))], axis=0)                                     \n",
        "# #     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "\n",
        "    \n",
        "# #### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "# with tf.variable_scope('projection_x'):\n",
        "#     proj_w_x = tf.get_variable('project_w_x', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "# #### sequence weight of x\n",
        "# squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQIFBsJnkJWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder(de_input, de_latent):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "#                                   f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(tf.concat((x_out_conv_dia_1, de_latent), axis=2),\n",
        "#                                   f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(tf.concat((x_out_conv_dia_2, de_latent), axis=2), \n",
        "#                                   f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(tf.concat((x_out_conv_dia_3, de_latent), axis=2), \n",
        "#                                   f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "#   x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "#   logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "#   x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "#   prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "#   prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "#   prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "#   prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "#   log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "#   log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "#   return log_liki_x\n",
        "\n",
        "# log_liki_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   log_liki_x_to.append(x_decoder(x_input_cnn[l],latent_variables[l]))\n",
        "# log_liki_x_to = tf.stack(log_liki_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qX8_07vbQoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "    \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_gated_conv = x_out_conv_dia[:,:250]*tf.nn.sigmoid(x_out_conv_dia[:,250:])\n",
        "  \n",
        "#   x_out_project = tf.matmul(x_out_gated_conv, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0))) \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l], latent_variables[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4gCPfEbETNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJjC0NuO9oE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Generation Model for target sentence $p_\\theta(y|z_1, z_2, ... , z_T)$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-H6P3ob3kPlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### concat beg token with target\n",
        "\n",
        "# #beg_token_y = tf.zeros((1,embed_size))\n",
        "# beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "# y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "# y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "# y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "# y_input_cnn = []\n",
        "# for l in range(latent_num):\n",
        "#   y_input_cnn.append(tf.concat([latent_variables[l],y_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW1zf5TWPQnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "#     f_y_1 = tf.get_variable(\"y_filter_1\", shape=[2, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_1_dia = tf.concat([f_y_1, \n",
        "#                            tf.zeros((1,embed_size+latent_size,filter_num))], axis=0)  \n",
        "                                    \n",
        "#     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "#     f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_y_2[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_y_2[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((4,filter_num+latent_size,filter_num)),], axis=0)\n",
        "#     # 9 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_y_3[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_y_3[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((8,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 13 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_y_4[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_y_4[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((16,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "# #### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "# with tf.variable_scope('projection_y'):\n",
        "#     proj_w_y = tf.get_variable('project_w_y', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "# #### sequence weight of y\n",
        "# squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpRsIIdCj5so",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def y_decoder(de_input,de_latent):\n",
        "  \n",
        "#   y_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "#                                   f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_2 = tf.nn.conv1d(tf.concat((y_out_conv_dia_1, de_latent), axis=2), \n",
        "#                                   f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_3 = tf.nn.conv1d(tf.concat((y_out_conv_dia_2, de_latent), axis=2), \n",
        "#                                   f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_4 = tf.nn.conv1d(tf.concat((y_out_conv_dia_3, de_latent), axis=2), \n",
        "#                                   f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "#   y_out_project = tf.matmul(y_out_conv_dia, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "\n",
        "#   target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "#   logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "#   logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "#   y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "#   prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "#   prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "#   prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "#   prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "#   log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "#   log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "#   return log_liki_y\n",
        "\n",
        "# log_liki_y_to = []\n",
        "# for l in range(latent_num):\n",
        "#   log_liki_y_to.append(y_decoder(y_input_cnn[l],latent_variables[l]))\n",
        "# log_liki_y_to = tf.stack(log_liki_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jb2g_kS1cKxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The lower bound of log-joint-likelihood, to maximize"
      ]
    },
    {
      "metadata": {
        "id": "5wHFYwB9jkDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nega_log_liki_x_y = 0\n",
        "\n",
        "nega_elbo = 0\n",
        "\n",
        "for l in range(latent_num):\n",
        "  nega_log_liki_x_y = nega_log_liki_x_y + tf.reduce_mean(- log_liki_x_to[l] - log_liki_y_to[l])\n",
        "  nega_elbo = nega_elbo - log_liki_x_to[l] - log_liki_y_to[l]\n",
        "  \n",
        "nega_log_liki_x_y = nega_log_liki_x_y/latent_num\n",
        "nega_elbo = nega_elbo/latent_num + discount_placeholder*kl_div_loss\n",
        "objective = tf.reduce_mean(nega_elbo) \n",
        "kl_div_loss_batch_mean = tf.reduce_mean(kl_div_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z8P6-XM38MV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# L2 reguralization for trainable variables\n",
        "#train_variables = tf.trainable_variables()\n",
        "#regularization_cost = tf.reduce_sum([tf.nn.l2_loss(variable) for variable in train_variables])\n",
        "#regular_rate = 0.00001\n",
        "#+ regular_rate*regularization_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwMB6m32Yzfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizer = tf.train.AdamOptimizer(lr_placeholder)\n",
        "\n",
        "# gvs, var = zip(*optimizer.compute_gradients(objective))\n",
        "\n",
        "# #checked_gvs = [tf.where(tf.is_nan(grad), tf.zeros_like(grad), grad) for grad in gvs]\n",
        "\n",
        "# cliped_gvs, _ = tf.clip_by_global_norm(gvs, 1)\n",
        "\n",
        "# opt = optimizer.apply_gradients(zip(cliped_gvs, var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4Glrs-GxHeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "#gvs = optimizer.compute_gradients(objective)\n",
        "#capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
        "#opt = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASMl3Fgnsyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### save the model\n",
        "def save_model(session, path):\n",
        "    if not os.path.exists(\"./result_0826/\"):\n",
        "        os.mkdir('./result_0826/')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(session, path)\n",
        "\n",
        "path1 = './result_0826/model_each_epch.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E546I60IPRWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "Bxq1rcyggfA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "\n",
        "\n",
        "def text_save(content,filename,mode='a'):\n",
        "    # Try to save a list variable in txt file.\n",
        "    file = open(filename,mode)\n",
        "    for i in range(len(content)):\n",
        "        file.write(str(content[i])+'\\n')\n",
        "    file.close()\n",
        "    \n",
        "def text_read(filename):\n",
        "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
        "    try:\n",
        "        file = open(filename,'r')\n",
        "    except IOError:\n",
        "        error = []\n",
        "        return error\n",
        "    content = file.readlines()\n",
        " \n",
        "    for i in range(len(content)):\n",
        "        content[i] = content[i][:len(content[i])-1]\n",
        " \n",
        "    file.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RovgeCM_9-i3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8461
        },
        "outputId": "3894eda2-957d-4146-d9d2-98dc6a8f46c2"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "total_step=0\n",
        "learning_rate = 0.001\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "#         en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "#         fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "        \n",
        "        en_input_drop_batches = en_input_batches\n",
        "        fr_output_drop_batches = fr_output_batches\n",
        "        \n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            discount_rate = 0.0002*total_step\n",
        "            if discount_rate >1:\n",
        "              discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "        \n",
        "text_save(elbo_results, './result_0826/elbo_results.txt')\n",
        "text_save(kl_results, './result_0826/kl_results.txt')\n",
        "text_save(likei_results, './result_0826/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/0_en.txt\n",
            "../small_txt/0_fr.txt\n",
            "22.764576\n",
            "475.69962\n",
            "475.69962\n",
            "0.0\n",
            "232.28618\n",
            "243.41344\n",
            "210.12387\n",
            "265.85178\n",
            "270.05423\n",
            "0.02\n",
            "129.37152\n",
            "136.48026\n",
            "406.2445\n",
            "228.95917\n",
            "245.20894\n",
            "0.04\n",
            "111.30313\n",
            "117.65603\n",
            "374.27502\n",
            "202.92\n",
            "225.37653\n",
            "0.060000000000000005\n",
            "92.911385\n",
            "110.00862\n",
            "356.55762\n",
            "163.06728\n",
            "191.59189\n",
            "0.08\n",
            "76.312096\n",
            "86.75519\n",
            "361.1279\n",
            "142.39305\n",
            "178.50584\n",
            "0.1\n",
            "66.55909\n",
            "75.83396\n",
            "339.39102\n",
            "115.11521\n",
            "155.84213\n",
            "0.12000000000000001\n",
            "54.04713\n",
            "61.068085\n",
            "312.65018\n",
            "102.66548\n",
            "146.43651\n",
            "0.14\n",
            "49.744545\n",
            "52.920937\n",
            "297.96753\n",
            "94.76157\n",
            "142.43637\n",
            "0.16\n",
            "46.514175\n",
            "48.247402\n",
            "289.20663\n",
            "87.72434\n",
            "139.78154\n",
            "0.18000000000000002\n",
            "43.265995\n",
            "44.45834\n",
            "284.16867\n",
            "89.71938\n",
            "146.55313\n",
            "0.2\n",
            "44.25364\n",
            "45.465748\n",
            "259.871\n",
            "79.90745\n",
            "137.07909\n",
            "0.22\n",
            "39.514637\n",
            "40.39281\n",
            "279.8526\n",
            "82.24827\n",
            "149.41289\n",
            "0.24000000000000002\n",
            "40.671253\n",
            "41.577015\n",
            "270.17697\n",
            "82.67362\n",
            "152.91963\n",
            "0.26\n",
            "42.528095\n",
            "40.145535\n",
            "253.01779\n",
            "75.14741\n",
            "145.9924\n",
            "0.28\n",
            "37.06853\n",
            "38.078873\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/1_en.txt\n",
            "../small_txt/1_fr.txt\n",
            "256.90076\n",
            "80.114044\n",
            "157.13289\n",
            "0.2998\n",
            "40.925205\n",
            "39.18883\n",
            "259.34222\n",
            "77.83278\n",
            "160.77042\n",
            "0.31980000000000003\n",
            "38.743282\n",
            "39.089493\n",
            "250.96364\n",
            "71.70432\n",
            "156.98177\n",
            "0.3398\n",
            "35.11532\n",
            "36.589005\n",
            "247.71867\n",
            "78.2638\n",
            "167.39297\n",
            "0.3598\n",
            "39.23114\n",
            "39.03265\n",
            "242.46825\n",
            "72.84203\n",
            "164.93149\n",
            "0.3798\n",
            "35.330208\n",
            "37.511837\n",
            "225.32541\n",
            "71.31458\n",
            "161.3997\n",
            "0.39980000000000004\n",
            "34.02403\n",
            "37.290565\n",
            "250.88638\n",
            "78.3311\n",
            "183.65323\n",
            "0.4198\n",
            "36.63794\n",
            "41.693165\n",
            "231.47452\n",
            "71.626144\n",
            "173.42863\n",
            "0.4398\n",
            "34.227825\n",
            "37.39832\n",
            "208.86351\n",
            "72.98528\n",
            "169.02074\n",
            "0.45980000000000004\n",
            "34.946564\n",
            "38.038723\n",
            "221.7999\n",
            "73.00925\n",
            "179.42883\n",
            "0.4798\n",
            "35.592567\n",
            "37.41669\n",
            "227.1591\n",
            "79.14564\n",
            "192.67976\n",
            "0.4998\n",
            "39.19807\n",
            "39.94756\n",
            "199.66342\n",
            "72.807106\n",
            "176.59215\n",
            "0.5198\n",
            "36.783493\n",
            "36.02361\n",
            "207.0435\n",
            "75.21547\n",
            "186.97755\n",
            "0.5398000000000001\n",
            "37.441658\n",
            "37.77381\n",
            "204.67104\n",
            "78.72173\n",
            "193.29659\n",
            "0.5598000000000001\n",
            "38.7402\n",
            "39.98152\n",
            "196.0967\n",
            "75.8554\n",
            "189.55226\n",
            "0.5798\n",
            "36.902042\n",
            "38.953358\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/2_en.txt\n",
            "../small_txt/2_fr.txt\n",
            "206.19766\n",
            "89.88295\n",
            "213.51909\n",
            "0.5996\n",
            "46.469673\n",
            "43.413273\n",
            "189.96185\n",
            "80.8343\n",
            "198.53467\n",
            "0.6196\n",
            "41.205338\n",
            "39.62896\n",
            "187.6135\n",
            "81.0718\n",
            "201.06941\n",
            "0.6396000000000001\n",
            "40.149094\n",
            "40.92271\n",
            "191.1224\n",
            "88.74749\n",
            "214.81183\n",
            "0.6596000000000001\n",
            "44.912663\n",
            "43.83484\n",
            "172.77841\n",
            "85.35902\n",
            "202.77922\n",
            "0.6796\n",
            "42.554703\n",
            "42.80431\n",
            "173.0977\n",
            "89.7077\n",
            "210.80685\n",
            "0.6996\n",
            "46.306557\n",
            "43.401157\n",
            "176.83636\n",
            "98.89428\n",
            "226.14572\n",
            "0.7196\n",
            "49.530537\n",
            "49.363724\n",
            "169.39766\n",
            "98.13537\n",
            "223.42188\n",
            "0.7396\n",
            "49.09426\n",
            "49.041122\n",
            "170.64479\n",
            "103.878845\n",
            "233.50063\n",
            "0.7596\n",
            "53.138023\n",
            "50.74083\n",
            "172.15393\n",
            "110.97836\n",
            "245.18954\n",
            "0.7796000000000001\n",
            "57.87139\n",
            "53.106968\n",
            "158.65936\n",
            "108.878\n",
            "235.74205\n",
            "0.7996000000000001\n",
            "54.474213\n",
            "54.403778\n",
            "151.00728\n",
            "104.10166\n",
            "227.86723\n",
            "0.8196\n",
            "53.714664\n",
            "50.387\n",
            "145.7923\n",
            "108.79683\n",
            "231.20404\n",
            "0.8396\n",
            "56.066406\n",
            "52.73041\n",
            "138.19275\n",
            "112.78906\n",
            "231.57954\n",
            "0.8596\n",
            "58.04938\n",
            "54.739677\n",
            "140.4018\n",
            "125.58419\n",
            "249.08162\n",
            "0.8796\n",
            "63.24878\n",
            "62.33541\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 3\n",
            "../small_txt/3_en.txt\n",
            "../small_txt/3_fr.txt\n",
            "134.78664\n",
            "159.7653\n",
            "280.99243\n",
            "0.8994000000000001\n",
            "70.8178\n",
            "88.9475\n",
            "122.42899\n",
            "119.66507\n",
            "232.22629\n",
            "0.9194\n",
            "62.977608\n",
            "56.687466\n",
            "125.1369\n",
            "133.75145\n",
            "251.30504\n",
            "0.9394\n",
            "69.560036\n",
            "64.19143\n",
            "113.8175\n",
            "130.94226\n",
            "240.13876\n",
            "0.9594\n",
            "68.96733\n",
            "61.97492\n",
            "112.67788\n",
            "140.78453\n",
            "251.14124\n",
            "0.9794\n",
            "73.27309\n",
            "67.511444\n",
            "113.02089\n",
            "144.51962\n",
            "257.47272\n",
            "0.9994000000000001\n",
            "77.04866\n",
            "67.47096\n",
            "106.90956\n",
            "152.29378\n",
            "259.20337\n",
            "1\n",
            "81.08936\n",
            "71.204414\n",
            "105.03429\n",
            "145.57657\n",
            "250.61086\n",
            "1\n",
            "76.40565\n",
            "69.170944\n",
            "105.20985\n",
            "144.16154\n",
            "249.37137\n",
            "1\n",
            "76.16677\n",
            "67.99477\n",
            "97.76201\n",
            "143.39983\n",
            "241.16182\n",
            "1\n",
            "75.45411\n",
            "67.94571\n",
            "96.066826\n",
            "142.23984\n",
            "238.30664\n",
            "1\n",
            "74.20807\n",
            "68.03178\n",
            "94.34577\n",
            "143.16881\n",
            "237.51459\n",
            "1\n",
            "74.41696\n",
            "68.75186\n",
            "94.99797\n",
            "147.6666\n",
            "242.66457\n",
            "1\n",
            "78.64542\n",
            "69.02118\n",
            "98.33211\n",
            "141.1411\n",
            "239.4732\n",
            "1\n",
            "74.35009\n",
            "66.79101\n",
            "98.39456\n",
            "148.21558\n",
            "246.61014\n",
            "1\n",
            "79.26828\n",
            "68.947296\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 4\n",
            "../small_txt/4_en.txt\n",
            "../small_txt/4_fr.txt\n",
            "96.36505\n",
            "148.25873\n",
            "244.62375\n",
            "1\n",
            "79.63918\n",
            "68.61954\n",
            "94.4916\n",
            "139.39839\n",
            "233.89\n",
            "1\n",
            "74.61364\n",
            "64.78475\n",
            "96.67734\n",
            "149.00993\n",
            "245.68724\n",
            "1\n",
            "81.64754\n",
            "67.3624\n",
            "95.2831\n",
            "140.08905\n",
            "235.37215\n",
            "1\n",
            "72.863914\n",
            "67.225136\n",
            "96.78746\n",
            "150.01808\n",
            "246.80557\n",
            "1\n",
            "80.06808\n",
            "69.95001\n",
            "93.031815\n",
            "135.34149\n",
            "228.37332\n",
            "1\n",
            "72.218124\n",
            "63.123367\n",
            "97.53585\n",
            "144.7754\n",
            "242.31125\n",
            "1\n",
            "77.409294\n",
            "67.36611\n",
            "97.764984\n",
            "155.98561\n",
            "253.75058\n",
            "1\n",
            "83.60298\n",
            "72.38264\n",
            "100.17355\n",
            "145.15216\n",
            "245.3257\n",
            "1\n",
            "77.8137\n",
            "67.33847\n",
            "94.00845\n",
            "134.71332\n",
            "228.72177\n",
            "1\n",
            "70.92541\n",
            "63.78791\n",
            "95.181244\n",
            "146.06892\n",
            "241.25018\n",
            "1\n",
            "77.19518\n",
            "68.873726\n",
            "97.347244\n",
            "144.39853\n",
            "241.7458\n",
            "1\n",
            "78.27555\n",
            "66.123\n",
            "98.92915\n",
            "140.56792\n",
            "239.49707\n",
            "1\n",
            "76.058784\n",
            "64.509125\n",
            "93.242256\n",
            "139.85835\n",
            "233.1006\n",
            "1\n",
            "75.07726\n",
            "64.781105\n",
            "96.22848\n",
            "142.73764\n",
            "238.96611\n",
            "1\n",
            "75.77639\n",
            "66.96124\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCryB1PjBEYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8478
        },
        "outputId": "d1521eaa-10be-4dff-bd3e-d4f47e962f05"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "learning_rate = 0.001\n",
        "total_step = 0\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc + 5\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        en_input_drop_batches = en_input_batches\n",
        "        fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = 0.0002*total_step\n",
        "            #if discount_rate >1:\n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "\n",
        "text_save(elbo_results, './result_0826/elbo_results.txt')\n",
        "text_save(kl_results, './result_0826/kl_results.txt')\n",
        "text_save(likei_results, './result_0826/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/5_en.txt\n",
            "../small_txt/5_fr.txt\n",
            "86.9571\n",
            "122.44888\n",
            "209.40598\n",
            "1\n",
            "62.101078\n",
            "60.34781\n",
            "95.96292\n",
            "125.37893\n",
            "221.34184\n",
            "1\n",
            "65.51281\n",
            "59.8661\n",
            "97.2968\n",
            "127.046165\n",
            "224.34297\n",
            "1\n",
            "66.09076\n",
            "60.95542\n",
            "97.879036\n",
            "137.61234\n",
            "235.4914\n",
            "1\n",
            "72.72356\n",
            "64.88878\n",
            "98.31658\n",
            "130.83429\n",
            "229.15088\n",
            "1\n",
            "69.23351\n",
            "61.60078\n",
            "92.045715\n",
            "121.681786\n",
            "213.7275\n",
            "1\n",
            "63.618153\n",
            "58.063637\n",
            "97.78873\n",
            "129.71114\n",
            "227.49985\n",
            "1\n",
            "66.3245\n",
            "63.386627\n",
            "91.5323\n",
            "116.55217\n",
            "208.08447\n",
            "1\n",
            "59.454758\n",
            "57.097412\n",
            "94.71362\n",
            "121.21614\n",
            "215.92976\n",
            "1\n",
            "62.522045\n",
            "58.694103\n",
            "98.19692\n",
            "129.51477\n",
            "227.7117\n",
            "1\n",
            "66.475525\n",
            "63.039238\n",
            "99.42306\n",
            "136.9212\n",
            "236.34425\n",
            "1\n",
            "71.656364\n",
            "65.26485\n",
            "98.82485\n",
            "130.96149\n",
            "229.78635\n",
            "1\n",
            "68.87454\n",
            "62.086964\n",
            "97.001816\n",
            "126.90329\n",
            "223.90509\n",
            "1\n",
            "68.29193\n",
            "58.61136\n",
            "93.97874\n",
            "119.33479\n",
            "213.31352\n",
            "1\n",
            "61.195976\n",
            "58.138805\n",
            "98.05413\n",
            "129.56584\n",
            "227.61998\n",
            "1\n",
            "66.48437\n",
            "63.08149\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/6_en.txt\n",
            "../small_txt/6_fr.txt\n",
            "92.72874\n",
            "128.56293\n",
            "221.29169\n",
            "1\n",
            "64.73124\n",
            "63.8317\n",
            "92.911064\n",
            "121.00286\n",
            "213.91393\n",
            "1\n",
            "62.022606\n",
            "58.980247\n",
            "96.78649\n",
            "125.37713\n",
            "222.16364\n",
            "1\n",
            "66.03581\n",
            "59.34132\n",
            "94.31103\n",
            "131.89755\n",
            "226.20859\n",
            "1\n",
            "68.223145\n",
            "63.674404\n",
            "91.68377\n",
            "122.899025\n",
            "214.58278\n",
            "1\n",
            "62.848633\n",
            "60.050392\n",
            "96.29199\n",
            "125.800285\n",
            "222.09227\n",
            "1\n",
            "66.09711\n",
            "59.703182\n",
            "89.59488\n",
            "114.60564\n",
            "204.2005\n",
            "1\n",
            "58.95082\n",
            "55.65482\n",
            "87.774216\n",
            "116.56831\n",
            "204.34251\n",
            "1\n",
            "59.41788\n",
            "57.15043\n",
            "94.00219\n",
            "126.1082\n",
            "220.1104\n",
            "1\n",
            "64.395676\n",
            "61.712536\n",
            "94.637184\n",
            "125.997215\n",
            "220.63441\n",
            "1\n",
            "64.78996\n",
            "61.207253\n",
            "93.88289\n",
            "126.51043\n",
            "220.39333\n",
            "1\n",
            "63.973976\n",
            "62.53646\n",
            "91.63233\n",
            "117.13857\n",
            "208.7709\n",
            "1\n",
            "59.163586\n",
            "57.974995\n",
            "92.81605\n",
            "126.98696\n",
            "219.80301\n",
            "1\n",
            "63.8048\n",
            "63.182175\n",
            "86.69863\n",
            "110.978516\n",
            "197.67715\n",
            "1\n",
            "57.05469\n",
            "53.923805\n",
            "95.68998\n",
            "125.578636\n",
            "221.26862\n",
            "1\n",
            "64.85054\n",
            "60.72808\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/7_en.txt\n",
            "../small_txt/7_fr.txt\n",
            "94.88962\n",
            "122.353485\n",
            "217.24309\n",
            "1\n",
            "62.141254\n",
            "60.212234\n",
            "96.45099\n",
            "126.22928\n",
            "222.68027\n",
            "1\n",
            "65.71279\n",
            "60.51649\n",
            "90.95539\n",
            "114.62542\n",
            "205.58083\n",
            "1\n",
            "60.231754\n",
            "54.393665\n",
            "101.323006\n",
            "132.07806\n",
            "233.4011\n",
            "1\n",
            "68.74543\n",
            "63.332626\n",
            "95.63052\n",
            "130.15163\n",
            "225.78215\n",
            "1\n",
            "68.33966\n",
            "61.811974\n",
            "92.97825\n",
            "119.599754\n",
            "212.578\n",
            "1\n",
            "60.900127\n",
            "58.69963\n",
            "95.73201\n",
            "119.97842\n",
            "215.71043\n",
            "1\n",
            "61.440556\n",
            "58.537865\n",
            "96.751045\n",
            "126.20022\n",
            "222.95125\n",
            "1\n",
            "64.30646\n",
            "61.89375\n",
            "95.46711\n",
            "125.87918\n",
            "221.34628\n",
            "1\n",
            "66.20469\n",
            "59.674488\n",
            "96.10303\n",
            "127.393936\n",
            "223.49695\n",
            "1\n",
            "66.03533\n",
            "61.358604\n",
            "99.42213\n",
            "130.59166\n",
            "230.0138\n",
            "1\n",
            "66.846146\n",
            "63.745506\n",
            "91.64358\n",
            "119.005295\n",
            "210.64886\n",
            "1\n",
            "60.752316\n",
            "58.252975\n",
            "94.285355\n",
            "115.67861\n",
            "209.96396\n",
            "1\n",
            "58.530384\n",
            "57.148247\n",
            "94.68711\n",
            "126.22512\n",
            "220.91223\n",
            "1\n",
            "65.551895\n",
            "60.67322\n",
            "91.900856\n",
            "113.72192\n",
            "205.62277\n",
            "1\n",
            "59.21505\n",
            "54.50687\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 3\n",
            "../small_txt/8_en.txt\n",
            "../small_txt/8_fr.txt\n",
            "95.52894\n",
            "122.25318\n",
            "217.78214\n",
            "1\n",
            "63.54755\n",
            "58.705635\n",
            "93.94996\n",
            "119.68601\n",
            "213.63596\n",
            "1\n",
            "60.934803\n",
            "58.751205\n",
            "95.77289\n",
            "121.246346\n",
            "217.01926\n",
            "1\n",
            "63.3484\n",
            "57.897964\n",
            "92.79781\n",
            "121.793594\n",
            "214.5914\n",
            "1\n",
            "63.00636\n",
            "58.787243\n",
            "93.36858\n",
            "109.56499\n",
            "202.93355\n",
            "1\n",
            "56.62898\n",
            "52.936005\n",
            "100.48327\n",
            "123.51381\n",
            "223.99709\n",
            "1\n",
            "62.8721\n",
            "60.641712\n",
            "98.90336\n",
            "123.84792\n",
            "222.75127\n",
            "1\n",
            "62.44873\n",
            "61.39919\n",
            "97.316505\n",
            "124.90915\n",
            "222.22566\n",
            "1\n",
            "63.938023\n",
            "60.97114\n",
            "103.068245\n",
            "130.79865\n",
            "233.8669\n",
            "1\n",
            "67.45805\n",
            "63.34059\n",
            "95.27668\n",
            "116.55677\n",
            "211.83344\n",
            "1\n",
            "59.78584\n",
            "56.77092\n",
            "93.86516\n",
            "116.88329\n",
            "210.74846\n",
            "1\n",
            "60.916153\n",
            "55.967125\n",
            "92.68813\n",
            "115.80259\n",
            "208.49072\n",
            "1\n",
            "59.49861\n",
            "56.303974\n",
            "97.23937\n",
            "126.58127\n",
            "223.82065\n",
            "1\n",
            "64.980354\n",
            "61.600903\n",
            "97.07062\n",
            "120.40075\n",
            "217.47134\n",
            "1\n",
            "59.998608\n",
            "60.402145\n",
            "94.81504\n",
            "123.54217\n",
            "218.35722\n",
            "1\n",
            "63.70216\n",
            "59.840004\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 4\n",
            "../small_txt/9_en.txt\n",
            "../small_txt/9_fr.txt\n",
            "90.29177\n",
            "122.936066\n",
            "213.22783\n",
            "1\n",
            "63.42378\n",
            "59.51228\n",
            "95.13309\n",
            "118.75662\n",
            "213.8897\n",
            "1\n",
            "62.24775\n",
            "56.508858\n",
            "103.23465\n",
            "128.93994\n",
            "232.17459\n",
            "1\n",
            "66.8689\n",
            "62.07106\n",
            "92.675896\n",
            "118.33136\n",
            "211.00726\n",
            "1\n",
            "60.96621\n",
            "57.365135\n",
            "95.340515\n",
            "117.98476\n",
            "213.32527\n",
            "1\n",
            "61.077026\n",
            "56.907738\n",
            "92.71418\n",
            "120.21569\n",
            "212.92989\n",
            "1\n",
            "61.672577\n",
            "58.543114\n",
            "92.98422\n",
            "110.902374\n",
            "203.8866\n",
            "1\n",
            "57.442814\n",
            "53.459564\n",
            "96.91599\n",
            "118.79521\n",
            "215.71121\n",
            "1\n",
            "61.86711\n",
            "56.928112\n",
            "93.53966\n",
            "121.30712\n",
            "214.84676\n",
            "1\n",
            "62.625317\n",
            "58.681812\n",
            "92.69682\n",
            "114.01286\n",
            "206.70973\n",
            "1\n",
            "59.153057\n",
            "54.859806\n",
            "99.32798\n",
            "121.3304\n",
            "220.65839\n",
            "1\n",
            "63.586098\n",
            "57.74431\n",
            "91.1609\n",
            "112.259094\n",
            "203.42001\n",
            "1\n",
            "56.927013\n",
            "55.3321\n",
            "95.699104\n",
            "120.6447\n",
            "216.34381\n",
            "1\n",
            "63.06921\n",
            "57.57549\n",
            "94.98198\n",
            "113.72907\n",
            "208.71106\n",
            "1\n",
            "58.70393\n",
            "55.025143\n",
            "99.246056\n",
            "120.783516\n",
            "220.02957\n",
            "1\n",
            "60.926758\n",
            "59.856762\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DqH5fygZc0yl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load test data and test the model"
      ]
    },
    {
      "metadata": {
        "id": "T3EsytHdPkpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 1"
      ]
    },
    {
      "metadata": {
        "id": "Is72zrRCKFOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "dd0c45fa-2aa7-4ea4-cce9-55bd1e235869"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_1 = []\n",
        "nage_likeli_test_1 = []\n",
        "objecti_test_1 = []\n",
        "llx_test_1 = []\n",
        "lly_test_1 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 12\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                           \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_1.append(kl)\n",
        "            nage_likeli_test_1.append(nage_likeli)\n",
        "            objecti_test_1.append(objecti)\n",
        "            llx_test_1.append(llx_mean)\n",
        "            lly_test_1.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_1))\n",
        "print(np.mean(nage_likeli_test_1))\n",
        "print(np.mean(objecti_test_1))\n",
        "print(np.mean(llx_test_1))\n",
        "print(np.mean(lly_test_1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n",
            "94.483315\n",
            "136.12769\n",
            "230.611\n",
            "69.53048\n",
            "66.59721\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r7mH1flzQMlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 2"
      ]
    },
    {
      "metadata": {
        "id": "BAKb79bddTJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "add7b2cd-a5bb-4c3b-962f-2dca3b3609eb"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 11\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_2.append(kl)\n",
        "            nage_likeli_test_2.append(nage_likeli)\n",
        "            objecti_test_2.append(objecti)\n",
        "            llx_test_2.append(llx_mean)\n",
        "            lly_test_2.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n",
            "94.3826\n",
            "136.58858\n",
            "230.97116\n",
            "70.00631\n",
            "66.58226\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-HIuHoabk85N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 3"
      ]
    },
    {
      "metadata": {
        "id": "pNniNB7Qk5kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "1496bb95-5a46-4704-a629-47b10be13c26"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 10\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_2.append(kl)\n",
        "            nage_likeli_test_2.append(nage_likeli)\n",
        "            objecti_test_2.append(objecti)\n",
        "            llx_test_2.append(llx_mean)\n",
        "            lly_test_2.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n",
            "96.04816\n",
            "135.39476\n",
            "231.44292\n",
            "69.38279\n",
            "66.011986\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n31e9XAOQ5hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sub-Train Set"
      ]
    },
    {
      "metadata": {
        "id": "yn8UHiro-RTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "060eea79-0384-4304-ae6d-39cdd5cc6cb3"
      },
      "cell_type": "code",
      "source": [
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_3 = []\n",
        "nage_likeli_test_3 = []\n",
        "objecti_test_3 = []\n",
        "llx_test_3 = []\n",
        "lly_test_3 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 6\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_3.append(kl)\n",
        "            nage_likeli_test_3.append(nage_likeli)\n",
        "            objecti_test_3.append(objecti)\n",
        "            llx_test_3.append(llx_mean)\n",
        "            lly_test_3.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_3))\n",
        "print(np.mean(nage_likeli_test_3))\n",
        "print(np.mean(objecti_test_3))\n",
        "print(np.mean(llx_test_3))\n",
        "print(np.mean(lly_test_3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n",
            "94.56733\n",
            "123.067215\n",
            "217.63455\n",
            "63.49065\n",
            "59.576572\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qYw-PZxCWtMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numerical Results"
      ]
    },
    {
      "metadata": {
        "id": "YO6RbSRWWsxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "1e78fa0f-4d5b-406e-dd1f-6d416773d17d"
      },
      "cell_type": "code",
      "source": [
        "elbo_read = text_read('./result_0826/elbo_results.txt')\n",
        "elbo_read = [-float(elbo) for elbo in elbo_read]\n",
        "\n",
        "kl_read = text_read('./result_0826/kl_results.txt')\n",
        "kl_read = [float(kl) for kl in kl_read]\n",
        "\n",
        "likei_read = text_read('./result_0826/likei_results.txt')\n",
        "likei_read = [-float(likei) for likei in likei_read]\n",
        "\n",
        "plt.plot(kl_read, color = 'C0')\n",
        "plt.plot(likei_read, color = 'C1')\n",
        "plt.plot(elbo_read, color = 'C2')\n",
        "plt.legend(['kl divergence','nage_log_like_x_n_y', 'nage_elbo'], fontsize=12)\n",
        "plt.title(\"Encoder_2_GDCNN_300_dropout_0.8\", fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Encoder_2_GDCNN_300_dropout_0.8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 60
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFcCAYAAAAH/v1SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYf4/hoNLmw4bKJEAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7fd01af87940>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5TrHF_jAS4dD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "|                                       |  Sub-train     | Test 1        |   Test 2    |  \n",
        "|-----------------------------------|--------------------|-------------------|----------------|\n",
        "|Reconstruction loss   | -213.30219 | -202.64314 |-203.7571  | \n",
        "| Nega_logp(x\\z)          |  -104.58628 |-102.27634 |-103.01279   |\n",
        "| Nega_logp(y\\z)          |  -108.71591 | -100.36681 |--100.744286 |\n",
        "|KL Divergence             | 15.922356   |14.511001    |14.691068  |\n",
        "|ELBO                             | -229.22456 | -217.15414   |-218.44814  |"
      ]
    },
    {
      "metadata": {
        "id": "d9ljOnBFieQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sentence"
      ]
    },
    {
      "metadata": {
        "id": "Cj3oqBvyDN2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  6\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fbx0ui5Ls9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "JwAPQJRDL0Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oRw39fV6KGU6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sen_len=30\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def output_sentence(idd,x_de,y_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(1, 0, 0, 0))\n",
        "  print(en_bleu)\n",
        "  \n",
        "  ########## \"French\" ##########\n",
        "  origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"y:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, 30)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"y_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  fr_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(1, 0, 0, 0))\n",
        "  print(fr_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KZWM4OKxKLZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "ffff9ba8-5477-4e44-ad89-9476d9825bb2"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(9,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I appeal for an in @-@ depth debate on this subject .\n",
            "x_re:\n",
            "  I appeal for this in @-@ depth debate on this subject .\n",
            "0.9166666666666666\n",
            "y:\n",
            "  Je demande qu&apos; un débat approfondi soit mené sur ce sujet .\n",
            "y_re:\n",
            "  Je demande qu&apos; un débat européen a adopté sur ce sujet .\n",
            "0.75\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "12WI_QkfKv7l",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "37fb937a-a177-4c6f-9317-f77cc016516b"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(64,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I also wish to thank Mr Almunia for the assistance he has given Cyprus all this time in achieving this objective .\n",
            "x_re:\n",
            "  I also like to thank Mr Tajani for the work which has given this and this time , the this area .\n",
            "0.5909090909090909\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Je voudrais également remercier M . Almunia pour l&apos; aide qu&apos; il a apportée à Chypre pendant tout ce temps en vue d&apos; atteindre cet objectif .\n",
            "y_re:\n",
            "  Je voudrais également remercier M . Dalli pour l&apos; aide qu&apos; il a accompli à Cancún et tous le qui en vue d&apos; atteindre l&apos; objectif .\n",
            "0.7037037037037037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "krLyTOqcKyBU",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "35cfe15a-5830-431e-9db2-789b02714825"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(89,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  It is a mechanism that we have criticised here , in this Parliament , and that , I think , we continue to criticise .\n",
            "x_re:\n",
            "  It is a time that we have done here , in this House , and that , I think , we want to say .\n",
            "0.8\n",
            "y:\n",
            "  C&apos; est un mécanisme que nous avons critiqué ici , dans ce Parlement , et que , je pense , nous continuons de critiquer .\n",
            "y_re:\n",
            "  C&apos; est un mécanisme que nous avons été , , dans ce Parlement , et que , je pense que nous continuerons de défendre .\n",
            "0.84\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "aNMMBL_eKzOo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "5946b73f-222e-4312-ef92-8fdcfdf1ce7f"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(77,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  The issue of the euro is no small matter for our fellow citizens : it is , in their hands , one of the European Union &apos;s most valuable\n",
            "x_re:\n",
            "  The issue of the euro is no the way for the fellow citizens , it is , in our hands of it of the European Union &apos;s most complicated\n",
            "0.7931034482758621\n",
            "y:\n",
            "  L&apos; affaire de l&apos; euro n&apos; est pas une petite affaire pour nos concitoyens : c&apos; est , entre leurs mains , un des biens les plus précieux de\n",
            "y_re:\n",
            "  L&apos; importance de l&apos; euro n&apos; est pas une nouvelle fois de nos concitoyens , c&apos; est , . les intérêts d&apos; un des citoyens de plus économiques car\n",
            "0.6551724137931034\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "9EVkmUgbK0MX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "c78390e7-133a-439e-c9de-9ce065512c74"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(78,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  Since last year , however , since the opening of the debate on the accession of Lithuania , we have had the impression that it has become a debate\n",
            "x_re:\n",
            "  Since last year , however , since the opening of the debate on the accession of N , we have had the impression that it has been a job\n",
            "0.896551724137931\n",
            "y:\n",
            "  Pourtant , depuis l&apos; année dernière , depuis l&apos; ouverture du débat sur l&apos; adhésion de la Lituanie , nous avons l&apos; impression qu&apos; il est devenu un débat\n",
            "y_re:\n",
            "  Depuis , , l&apos; année dernière , dans l&apos; ouverture du débat sur l&apos; adhésion de la Commission , nous avons l&apos; impression qu&apos; il est devenu un Parlement\n",
            "0.8275862068965517\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDFnWZP5F3OC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ]
    },
    {
      "metadata": {
        "id": "E71tUu9EF3OC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "bbf4409b-b221-4306-c354-35dcfde5596a"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x and y ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 9\n",
        "t = 0\n",
        "decode_len = 30\n",
        "\n",
        "eos_id = en_word_to_id['eos']\n",
        "eos_prob = -float('Inf')\n",
        "\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "#########################################################\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "y_score = np.zeros((beam_size))\n",
        "\n",
        "#########################################################\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "5kY27EN-OZ9V",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "cdc8b73d-654f-4d02-ea4f-920ae5e12b04"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 64\n",
        "t = 0\n",
        "decode_len = 30\n",
        "\n",
        "eos_id = en_word_to_id['eos']\n",
        "eos_prob = -float('Inf')\n",
        "\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "\n",
        "y_in = np.random.randint(low=0, high=fr_vocab_size, size=(1, max_length))\n",
        "\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "\n",
        "#########################################################\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "        x_de = copy.deepcopy(x_de_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0826/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "Abw3fjuMQ337",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sen_len=30\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def output_sentence(idd,x_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(1, 0, 0, 0))\n",
        "  print(en_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NA62EOLVQ87H",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "c6311545-768e-4dda-b028-320d2fd42ba2"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(9,x_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I appeal for an in @-@ depth debate on this subject .\n",
            "x_re:\n",
            "  As , to the , the &apos;s , on this &apos; .\n",
            "0.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ge_5SLWZRYNp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "9cf93399-3848-4350-aefe-1f2900db7b2c"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(64,x_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I also wish to thank Mr Almunia for the assistance he has given Cyprus all this time in achieving this objective .\n",
            "x_re:\n",
            "  I , , to the the &apos; of the , , has on this and the report , the this area .\n",
            "0.3181818181818182\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6CKhxVAVMQQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Blue Score Functions"
      ]
    },
    {
      "metadata": {
        "id": "-flyQJg1YsX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_ngram(candidate, references, n):\n",
        "    clipped_count = 0\n",
        "    count = 0\n",
        "    r = 0\n",
        "    c = 0\n",
        "    for si in range(len(candidate)):\n",
        "        # Calculate precision for each sentence\n",
        "        ref_counts = []\n",
        "        ref_lengths = []\n",
        "        # Build dictionary of ngram counts\n",
        "        for reference in references:\n",
        "            ref_sentence = reference[si]\n",
        "            ngram_d = {}\n",
        "            words = ref_sentence.strip().split()\n",
        "            ref_lengths.append(len(words))\n",
        "            limits = len(words) - n + 1\n",
        "            # loop through the sentance consider the ngram length\n",
        "            for i in range(limits):\n",
        "                ngram = ' '.join(words[i:i+n]).lower()\n",
        "                if ngram in ngram_d.keys():\n",
        "                    ngram_d[ngram] += 1\n",
        "                else:\n",
        "                    ngram_d[ngram] = 1\n",
        "            ref_counts.append(ngram_d)\n",
        "        # candidate\n",
        "        cand_sentence = candidate[si]\n",
        "        cand_dict = {}\n",
        "        words = cand_sentence.strip().split()\n",
        "        limits = len(words) - n + 1\n",
        "        for i in range(0, limits):\n",
        "            ngram = ' '.join(words[i:i + n]).lower()\n",
        "            if ngram in cand_dict:\n",
        "                cand_dict[ngram] += 1\n",
        "            else:\n",
        "                cand_dict[ngram] = 1\n",
        "        clipped_count += clip_count(cand_dict, ref_counts)\n",
        "        count += limits\n",
        "        r += best_length_match(ref_lengths, len(words))\n",
        "        c += len(words)\n",
        "    if clipped_count == 0:\n",
        "        pr = 0\n",
        "    else:\n",
        "        pr = float(clipped_count) / count\n",
        "    bp = brevity_penalty(c, r)\n",
        "    return pr, bp\n",
        "\n",
        "\n",
        "def clip_count(cand_d, ref_ds):\n",
        "    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n",
        "    count = 0\n",
        "    for m in cand_d.keys():\n",
        "        m_w = cand_d[m]\n",
        "        m_max = 0\n",
        "        for ref in ref_ds:\n",
        "            if m in ref:\n",
        "                m_max = max(m_max, ref[m])\n",
        "        m_w = min(m_w, m_max)\n",
        "        count += m_w\n",
        "    return count\n",
        "\n",
        "\n",
        "def best_length_match(ref_l, cand_l):\n",
        "    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n",
        "    least_diff = abs(cand_l-ref_l[0])\n",
        "    best = ref_l[0]\n",
        "    for ref in ref_l:\n",
        "        if abs(cand_l-ref) < least_diff:\n",
        "            least_diff = abs(cand_l-ref)\n",
        "            best = ref\n",
        "    return best\n",
        "\n",
        "\n",
        "def brevity_penalty(c, r):\n",
        "    if c > r:\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.exp(1-(float(r)/c))\n",
        "    return bp\n",
        "\n",
        "\n",
        "def geometric_mean(precisions):\n",
        "    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n",
        "\n",
        "\n",
        "def sel_sentence_bleu(candidate, references):\n",
        "    precisions = []\n",
        "    for i in range(4):\n",
        "        pr, bp = count_ngram(candidate, references, i+1)\n",
        "        precisions.append(pr)\n",
        "    bleu = geometric_mean(precisions) * bp\n",
        "    return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcCYvSrNMpDp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate Sentence"
      ]
    },
    {
      "metadata": {
        "id": "ETRwssjH8a5M",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cal_kl_loss(mean,log_var,var ):\n",
        "  kl_loss = 1 + log_var - np.square(mean) - var                      # batch_size*max_length x latent_size\n",
        "  kl_loss = -0.5 * np.sum(kl_loss)                               # batch_size*max_length x 1\n",
        "#   kl_div_loss = np.reshape(kl_div_loss, (batch_size, max_length))        # batch_size x max_length\n",
        "#   kl_div_loss = np.sum(kl_div_loss, axis=1)\n",
        "  return kl_loss\n",
        "\n",
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sen_len=30\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave\n",
        "\n",
        "def find_next_word_beam_gene_first(logits_y, log_prob_y, log_prob_la, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] + (log_prob_y[l] + log_prob_la[l])[0]\n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave\n",
        "\n",
        "def output_sentence(idd,x_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(1, 0, 0, 0))\n",
        "  print(en_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n3RQ_VB9Xt7v",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "81170072-7d4d-4586-beaf-39e072980149"
      },
      "cell_type": "code",
      "source": [
        "########## \"English\" ##########\n",
        "idd = 144\n",
        "origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"x:\")\n",
        "print(or_sens_str)\n",
        "\n",
        "origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"y:\")\n",
        "print(or_sens_str)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I strongly urge those other businesses that have not as yet joined this initiative to do so , without delay .\n",
            "y:\n",
            "  J&apos; invite instamment les entreprises qui n&apos; ont pas encore rejoint cette initiative à le faire sans plus tarder .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gSp6CoBqJeDi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search translate x ###########\n",
        "\n",
        "beam_size = 50\n",
        "idd = 144\n",
        "decode_len = 30\n",
        "\n",
        "#### input y\n",
        "x_true = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "x_true_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "\n",
        "#### initialize x\n",
        "#x_in = np.concatenate((np.random.randint(low=0, high=en_vocab_size, size=(beam_size, x_len[0])), np.reshape(np.asarray(en_word_to_id['eos']),[1,1]), en_word_to_id['pad']*np.ones((1,max_length-x_len[0]-1),dtype=np.int32)), axis=1)\n",
        "#x_in = np.random.randint(low=0, high=en_vocab_size, size=(1,  max_length))\n",
        "x_in = en_word_to_id['<OOV>']*np.ones((1,  max_length),dtype=np.int32)\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#### define useful variables\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for h in range(15):\n",
        "      \n",
        "      x_score = np.zeros((beam_size))\n",
        "      \n",
        "      #### sample latent variables\n",
        "      gene_feed_dict = {input_placeholder: x_in, \n",
        "                        target_placeholder: y_in}                           \n",
        "            \n",
        "      mean, logvar, var, std = sess.run([la_mean, la_log_var, la_var, la_std], feed_dict=gene_feed_dict)                    # 1*max_len x fr_vocab_size\n",
        "      kl_div = cal_kl_loss(mean, logvar, var)\n",
        "      \n",
        "      la_vari = []\n",
        "      log_prob_la = []\n",
        "      for _ in range(latent_num):\n",
        "        eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "        la_var_sample = mean + std*eposida\n",
        "        la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))   # batch_size x max_length x latent_size\n",
        "        la_vari.append(la_var_sample)   \n",
        "        log_prob_la.append(np.sum(norm.logpdf(la_var_sample)))\n",
        "\n",
        "      la_vari = np.stack(la_vari,axis=0)                                                     # latent_num x batch_size x max_length x latent_size\n",
        "      log_prob_la = np.stack(log_prob_la,axis=0)                                           # latent_num x 1\n",
        "    \n",
        "      #### obtain log p(y|z)\n",
        "      gene_feed_dict = {latent_var_placeholder:la_vari,\n",
        "                        target_placeholder: y_in,\n",
        "                        input_placeholder: x_true,\n",
        "                        in_length_placeholder: x_true_len,\n",
        "                        out_length_placeholder: y_len}\n",
        "\n",
        "      log_prob_x, log_prob_y = sess.run([log_liki_x_to,log_liki_y_to], feed_dict=gene_feed_dict)                       # latent_num x 1\n",
        "      \n",
        "      ind = np.argsort(log_prob_y[:,0])[-10:]\n",
        "      new_la_var = copy.deepcopy(la_vari[ind])\n",
        "      for i in range(4):\n",
        "        new_la_var = np.concatenate((new_la_var,la_vari[ind]),axis=0)\n",
        "      \n",
        "      print(kl_div)\n",
        "      print(np.sum(log_prob_la)/latent_num)\n",
        "      print(np.sum(log_prob_x)/latent_num)\n",
        "      print(np.sum(log_prob_y)/latent_num)\n",
        "      \n",
        "      #### beam search for x\n",
        "    \n",
        "      for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          gene_feed_dict = {latent_var_placeholder:new_la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          if t==1:\n",
        "            x_prob_next_word[j] = find_next_word_beam_gene_first(logits_x, log_prob_y, log_prob_la, t)          \n",
        "            x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          else:  \n",
        "            x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "            x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)     \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "              \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        \n",
        "      #### at the end of beam search\n",
        "      or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "      or_sens_str = \" \"\n",
        "      for p in range(max_length):\n",
        "        or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "      print(\"x_tran:\")\n",
        "      print(or_sens_str)\n",
        "    \n",
        "      x_in[0] = copy.deepcopy(x_de[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8KpIQbsQX_fF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######### Beam Search translate x ###########\n",
        "\n",
        "beam_size = 50\n",
        "idd = 144\n",
        "decode_len = 30\n",
        "\n",
        "#### input x\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "y_true = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "y_true_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "#### initialize x\n",
        "y_in = fr_word_to_id['<OOV>']*np.ones((1,  max_length),dtype=np.int32)\n",
        "y_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#### define useful variables\n",
        "y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "y_score = np.zeros((beam_size))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for h in range(15):\n",
        "      \n",
        "      y_score = np.zeros((beam_size))      \n",
        "            \n",
        "      #### sample latent variables\n",
        "      gene_feed_dict = {input_placeholder: x_in, \n",
        "                        target_placeholder: y_in}                           \n",
        "            \n",
        "      mean, logvar, var, std = sess.run([la_mean, la_log_var, la_var, la_std], feed_dict=gene_feed_dict)                    # 1*max_len x fr_vocab_size\n",
        "      kl_div = cal_kl_loss(mean, logvar, var)\n",
        "      \n",
        "      la_vari = []\n",
        "      log_prob_la = []\n",
        "      for _ in range(latent_num):\n",
        "        eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "        la_var_sample = mean + std*eposida\n",
        "        la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))   # batch_size x max_length x latent_size\n",
        "        la_vari.append(la_var_sample)   \n",
        "        log_prob_la.append(np.sum(norm.logpdf(la_var_sample)))\n",
        "\n",
        "      la_vari = np.stack(la_vari,axis=0)                                                     # latent_num x batch_size x max_length x latent_size\n",
        "      log_prob_la = np.stack(log_prob_la,axis=0)                                           # latent_num x 1\n",
        "    \n",
        "      #### obtain log p(y|z)\n",
        "      gene_feed_dict = {latent_var_placeholder:la_vari,\n",
        "                        target_placeholder: y_true,\n",
        "                        input_placeholder: x_in,\n",
        "                        in_length_placeholder: x_len,\n",
        "                        out_length_placeholder: y_true_len}\n",
        "\n",
        "      log_prob_x, log_prob_y = sess.run([log_liki_x_to,log_liki_y_to], feed_dict=gene_feed_dict)                       # latent_num x 1\n",
        "      \n",
        "      ind = np.argsort(log_prob_x[:,0])[-10:]\n",
        "      new_la_var = copy.deepcopy(la_vari[ind])\n",
        "      for i in range(4):\n",
        "        new_la_var = np.concatenate((new_la_var,la_vari[ind]),axis=0)\n",
        "      \n",
        "#       new_la_var = la_vari\n",
        "    \n",
        "      print(kl_div)\n",
        "      print(np.sum(log_prob_la)/latent_num)\n",
        "      print(np.sum(log_prob_x)/latent_num)\n",
        "      print(np.sum(log_prob_y)/latent_num)\n",
        "      \n",
        "      #### beam search for y\n",
        "    \n",
        "      for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          gene_feed_dict = {latent_var_placeholder: new_la_var,\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          if t==1:\n",
        "            y_prob_next_word[j] = find_next_word_beam_gene_first(logits_y, log_prob_x, log_prob_la, t)          \n",
        "            y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "          else:  \n",
        "            y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "            y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "         \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)     \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "              \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        y_de = copy.deepcopy(y_de_new)\n",
        "        \n",
        "      #### at the end of beam search\n",
        "      or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, max_length)\n",
        "      or_sens_str = \" \"\n",
        "      for p in range(max_length):\n",
        "        or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "      print(\"y_tran:\")\n",
        "      print(or_sens_str)\n",
        "      \n",
        "      y_in[0] =  y_de[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}