{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Gene_translation_cnn_0812.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "E546I60IPRWK",
        "DqH5fygZc0yl",
        "qYw-PZxCWtMM",
        "7FRb2Ko3LoRL"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rMcrrnEBsjy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the drive path"
      ]
    },
    {
      "metadata": {
        "id": "66FgEFRN-AMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWRjvljv-GT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vn79dig8uT8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "944ee295-c8ee-46db-acd3-e9c1da5fedf7"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MTeUgeB-seY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "123a35bd-e85d-4cdf-9057-41bae60762a2"
      },
      "cell_type": "code",
      "source": [
        "cd drive/DCNN_residual_200"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/DCNN_residual_200\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B12HWS4xsx4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ]
    },
    {
      "metadata": {
        "id": "Ly79OANzc7RS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7Ngio7cmIGuI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "11382f7d-aae4-48a0-f848-aa32e08809f6"
      },
      "cell_type": "code",
      "source": [
        "sentence_bleu(references=[[\"I\", \"appeal\", \"for\", \"an\", \"in\", \"@-@\", \"depth\", \"debate\", \"on\", \"this\", \"subject\", \".\"]],\n",
        "              hypothesis = [\"I\", \"therefore\", \"call\", \"for\", \"a\", \"debate\", \"on\", \"the\", \"report\", \".\"],\n",
        "              weights=(0.25, 0.25, 0.25, 0.25))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3974870438631006"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "metadata": {
        "id": "842o8uvRtBnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "wjDy6-mfnnit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load vocab dict from txt file\n",
        "\n",
        "f = open(\"../dictionary 2/en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../dictionary 2/fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuETWPMbu2Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "959e9dbf-1328-41e9-c5b9-aa718b45e256"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "en_eos = en_word_to_id['eos']\n",
        "fr_eos = fr_word_to_id['eos']\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30772\n",
            "39578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9ifxSnpu6zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output\n",
        "\n",
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list\n",
        "\n",
        "\n",
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y57-AuNCvDSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length=30)\n",
        "\n",
        "    #out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    #fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehsowT7hwyjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juNYK867gw3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_oov_id = en_word_to_id['<OOV>']\n",
        "fr_oov_id = fr_word_to_id['<OOV>']\n",
        "\n",
        "def dropout_func(decode_input, dropout_prob, oov_id):\n",
        "  for i in range(decode_input.shape[0]):\n",
        "    for j in range(decode_input.shape[1]):\n",
        "        for k in range(1,decode_input.shape[2]):\n",
        "            if np.random.uniform() > dropout_prob:\n",
        "                decode_input[i,j,k] = oov_id        \n",
        "  return decode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq37O5wR21AC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "zcHfesh3uCDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define parameters ######################\n",
        "\n",
        "max_length = 30\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "embed_size = 300\n",
        "\n",
        "infer_hidden_size = 1000\n",
        "\n",
        "latent_size = 200\n",
        "\n",
        "latent_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FraHS_clcvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ###################### generate sentence #######################\n",
        "batch_size = 1\n",
        "\n",
        "latent_num = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJNVkwnhdg9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define placeholder ######################\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input')         # batch_size x max_length\n",
        "\n",
        "target_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target')       # batch_size x max_length\n",
        "\n",
        "in_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'in_len')              # batch_size x 1\n",
        "\n",
        "out_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'out_len')            # batch_size x 1\n",
        "\n",
        "discount_placeholder = tf.placeholder(tf.float32, name='discount')\n",
        "\n",
        "lr_placeholder = tf.placeholder(tf.float32, name='learn_rate')\n",
        "\n",
        "input_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input_drop')   # batch_size x max_length\n",
        "\n",
        "target_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target_drop') # batch_size x max_length\n",
        "\n",
        "if_gene_placeholder = tf.placeholder(tf.bool, name='if_gene')\n",
        "\n",
        "latent_var_placeholder = tf.placeholder(tf.float32, [latent_num, batch_size, max_length, latent_size], 'la_var')       # batch_size x max_length x latent_size\n",
        "\n",
        "xavier_initializer = tf.contrib.layers.xavier_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dqobGQGkHNg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### embedding look-up for input sentences ####################\n",
        "\n",
        "with tf.variable_scope('en_embedding'):\n",
        "    en_embedding = tf.get_variable('en_embeding',[en_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    inputs = tf.nn.embedding_lookup(en_embedding, input_placeholder)                      # batch_size x max_length x embed_size\n",
        "    inputs_drop = tf.nn.embedding_lookup(en_embedding, input_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    \n",
        "\n",
        "with tf.variable_scope('fr_embedding'):\n",
        "    fr_embedding = tf.get_variable('fr_embeding',[fr_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    targets = tf.nn.embedding_lookup(fr_embedding, target_placeholder)                      # batch_size x max_length x embed_size\n",
        "    targets_drop = tf.nn.embedding_lookup(fr_embedding, target_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmvdTgj5tSpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Inference Model - Encoder\n",
        "\n",
        "$q(z_1, z_2, ... , z_T|x,y)$\n",
        "\n",
        "Similar to the encoder of RNNSearch"
      ]
    },
    {
      "metadata": {
        "id": "DSPN85dailPC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### Inference model  #######################\n",
        "\n",
        "encode_inputs = tf.transpose(tf.concat([inputs, targets], axis=2), (1,0,2))\n",
        "\n",
        "with tf.variable_scope('encode'):\n",
        "    #basic_cell =tf.contrib.rnn.GRUCell(infer_hidden_size)\n",
        "    basic_cell = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state = basic_cell.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs, encode_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell, \n",
        "                                                                   cell_bw=basic_cell, \n",
        "                                                                   inputs=encode_inputs,                                                                \n",
        "                                                                   initial_state_fw=init_state,\n",
        "                                                                   initial_state_bw=init_state,\n",
        "                                                                   dtype=tf.float32,\n",
        "                                                                   time_major=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6h0aG1lX1ug4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo0Ycpq9invD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ##################### Inference model  #######################\n",
        "\n",
        "# ##################### bi-direction lstm of source sentence ######################\n",
        "\n",
        "# encode_inputs_x = tf.transpose(inputs, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_x'):\n",
        "#     basic_cell_x = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_x = basic_cell_x.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_x, encode_state_x = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_x, \n",
        "#                                                                        cell_bw=basic_cell_x, \n",
        "#                                                                        inputs=encode_inputs_x,\n",
        "#                                                                        sequence_length=in_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_x,\n",
        "#                                                                        initial_state_bw=init_state_x,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_x: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##################### bi-direction lstm of target sentence ######################\n",
        "\n",
        "# encode_inputs_y = tf.transpose(targets, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_y'):\n",
        "#     basic_cell_y = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_y = basic_cell_y.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_y, encode_state_y = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_y, \n",
        "#                                                                        cell_bw=basic_cell_y, \n",
        "#                                                                        inputs=encode_inputs_y,\n",
        "#                                                                        sequence_length=out_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_y,\n",
        "#                                                                        initial_state_bw=init_state_y,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_y: max_length x batch_size x infer_hidden_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYfWtHL8irsr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "# en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "# en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "# #en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size)) # batch_size*max_length x 2*infer_hidden_size\n",
        "\n",
        "# ##################### concatenate the state of encoder of x and y ######################\n",
        "\n",
        "# fw_bw_en_state_x = tf.concat((encode_state_x[0][1],encode_state_x[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state_y = tf.concat((encode_state_y[0][1],encode_state_y[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state = tf.concat((fw_bw_en_state_x, fw_bw_en_state_y), 1)             # en_max_length x  4*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state = tf.tile(tf.expand_dims(fw_bw_en_state, axis=1), (1,30,1))\n",
        "\n",
        "\n",
        "# fw_bw_en = tf.concat((fw_bw_en_state, en_outputs_tran), axis=2)\n",
        "\n",
        "# fw_bw_en = tf.reshape(fw_bw_en, (batch_size*max_length, 6*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Yq7w0u-iyl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('encode_projection'):\n",
        "    W_1 = tf.get_variable('W_1',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_1 = tf.get_variable('b_1',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_2 = tf.get_variable('W_2',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_2 = tf.get_variable('b_2',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gz4FQdg_i4b8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fw_bw_en_outputs_norm = tf.contrib.layers.batch_norm(fw_bw_en_outputs_resh, center=True, scale=True)\n",
        "\n",
        "la_mean = tf.matmul(en_outputs_resh, W_1) + b_1                              # batch_size*max_length x latent_size \n",
        "\n",
        "la_log_var = tf.matmul(en_outputs_resh, W_2) + b_2                           # batch_size*max_length x latent_size \n",
        "la_var = tf.exp(la_log_var)\n",
        "la_std = tf.sqrt(la_var)\n",
        "\n",
        "kl_div_loss = 1 + la_log_var - tf.square(la_mean) - la_var                      # batch_size*max_length x latent_size\n",
        "kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, axis=1)                         # batch_size*max_length x 1\n",
        "kl_div_loss = tf.reshape(kl_div_loss, (batch_size, max_length))                 # batch_size x max_length\n",
        "kl_div_loss = tf.reduce_sum(kl_div_loss, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2-aUlz0i7Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# latent_variables_v = []\n",
        "# for _ in range(latent_num):\n",
        "#   eposida = tf.random_normal(tf.shape(la_std), mean=0.0,stddev=1)\n",
        "#   latent_variables_sample = la_mean + la_std*eposida\n",
        "#   latent_variables_sample = tf.reshape(latent_variables_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "#   latent_variables_v.append(latent_variables_sample)\n",
        "\n",
        "# def if_true():\n",
        "#   latent_v = []\n",
        "#   for h in range(latent_num):\n",
        "#     latent_v.append(latent_var_placeholder[h])\n",
        "#   return latent_v\n",
        "\n",
        "# def if_false():\n",
        "#   return latent_variables_v\n",
        "\n",
        "# latent_variables = tf.cond(if_gene_placeholder, if_true, if_false)\n",
        "\n",
        "# if latent_num == 1:\n",
        "#   new_latent_variables = []\n",
        "#   new_latent_variables.append(latent_variables)\n",
        "#   latent_variables = new_latent_variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDVWXdubzXh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_v = []\n",
        "for h in range(latent_num):\n",
        "  latent_v.append(latent_var_placeholder[h])\n",
        "    \n",
        "latent_variables = latent_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on6adzC8518v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Generation Model - Decoder\n",
        "\n",
        "$p_\\theta(x|z_1, z_2, ... , z_T)$\n",
        "\n",
        "$p_\\theta(y|z_1, z_2, ... , z_T)$"
      ]
    },
    {
      "metadata": {
        "id": "tiEDu_jlXEtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filter_num = 150\n",
        "\n",
        "filter_size = 3\n",
        "\n",
        "filter_size_only_pre = 2\n",
        "\n",
        "filter_size_pad = filter_size - filter_size_only_pre\n",
        "\n",
        "filter_zero_pad = tf.zeros(shape=[filter_size_pad, embed_size+latent_size, filter_num], dtype=tf.float32)\n",
        "filter_zero_pad_2 = tf.zeros(shape=[1, filter_size_pad, filter_num, filter_num], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSBf8rYS9hX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Generation Model for source sentence $p_\\theta(x|z_1, z_2, ... , z_T)$\n"
      ]
    },
    {
      "metadata": {
        "id": "UNQlel3UksZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### concat beg token with input\n",
        "\n",
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "#x_input_cnn_1 = tf.concat([latent_variables_1,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "#x_input_cnn_2 = tf.concat([latent_variables_2,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "#x_input_cnn_4D = tf.expand_dims(x_input_cnn, axis=1)                                        # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "x_input_cnn = []\n",
        "for l in range(latent_num):\n",
        "  x_input_cnn.append(tf.concat([latent_variables[l],x_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4x0f5AYajCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "    f_x_1 = tf.get_variable(\"x_filter_1\", shape=[2, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_1_dia = tf.concat([f_x_1, \n",
        "                           tf.zeros((1,embed_size+latent_size,filter_num))], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((1,filter_num+latent_size, filter_num)), \n",
        "                           tf.reshape(f_x_2[1],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((1,filter_num+latent_size,filter_num)),\n",
        "                           tf.reshape(f_x_2[2],(1,filter_num+latent_size,filter_num)),\n",
        "                           tf.zeros((4,filter_num+latent_size,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((3,filter_num+latent_size,filter_num)), \n",
        "                           tf.reshape(f_x_3[1],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((3,filter_num+latent_size,filter_num)),\n",
        "                           tf.reshape(f_x_3[2],(1,filter_num+latent_size,filter_num)),\n",
        "                           tf.zeros((8,filter_num+latent_size,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((7,filter_num+latent_size,filter_num)), \n",
        "                           tf.reshape(f_x_4[1],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((7,filter_num+latent_size,filter_num)),\n",
        "                           tf.reshape(f_x_4[2],(1,filter_num+latent_size,filter_num)),\n",
        "                           tf.zeros((16,filter_num+latent_size,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_5 = tf.get_variable(\"x_filter_5\", shape=[2, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_5_dia = tf.concat([f_x_5, \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num))], axis=0)                                     \n",
        "#     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "\n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_x'):\n",
        "    proj_w_x = tf.get_variable('project_w_x', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### sequence weight of x\n",
        "squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQIFBsJnkJWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder(de_input, de_latent):\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "                                  f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(tf.concat((x_out_conv_dia_1, de_latent), axis=2),\n",
        "                                  f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(tf.concat((x_out_conv_dia_2, de_latent), axis=2), \n",
        "                                  f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(tf.concat((x_out_conv_dia_3, de_latent), axis=2), \n",
        "                                  f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "  x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "  target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "  logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "  prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_x\n",
        "\n",
        "log_liki_x_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_x_to.append(x_decoder(x_input_cnn[l],latent_variables[l]))\n",
        "log_liki_x_to = tf.stack(log_liki_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xDN_gSxDM8c2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder_gene(de_input, de_latent):\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "                                  f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(tf.concat((x_out_conv_dia_1, de_latent), axis=2),\n",
        "                                  f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(tf.concat((x_out_conv_dia_2, de_latent), axis=2), \n",
        "                                  f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(tf.concat((x_out_conv_dia_3, de_latent), axis=2), \n",
        "                                  f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "  x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "  return logits_x\n",
        "\n",
        "logits_gene_x_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l], latent_variables[l]))\n",
        "logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qX8_07vbQoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "    \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_gated_conv = x_out_conv_dia[:,:250]*tf.nn.sigmoid(x_out_conv_dia[:,250:])\n",
        "  \n",
        "#   x_out_project = tf.matmul(x_out_gated_conv, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0))) \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l], latent_variables[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4gCPfEbETNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJjC0NuO9oE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Generation Model for target sentence $p_\\theta(y|z_1, z_2, ... , z_T)$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-H6P3ob3kPlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### concat beg token with target\n",
        "\n",
        "#beg_token_y = tf.zeros((1,embed_size))\n",
        "beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "y_input_cnn = []\n",
        "for l in range(latent_num):\n",
        "  y_input_cnn.append(tf.concat([latent_variables[l],y_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW1zf5TWPQnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "    f_y_1 = tf.get_variable(\"y_filter_1\", shape=[2, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_1_dia = tf.concat([f_y_1, \n",
        "                           tf.zeros((1,embed_size+latent_size,filter_num))], axis=0)  \n",
        "                                    \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((1,filter_num+latent_size,filter_num)), \n",
        "                           tf.reshape(f_y_2[1],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((1,filter_num+latent_size,filter_num)),\n",
        "                           tf.reshape(f_y_2[2],(1,filter_num+latent_size,filter_num)),\n",
        "                           tf.zeros((4,filter_num+latent_size,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((3,filter_num+latent_size,filter_num)), \n",
        "                           tf.reshape(f_y_3[1],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((3,filter_num+latent_size,filter_num)),\n",
        "                           tf.reshape(f_y_3[2],(1,filter_num+latent_size,filter_num)),\n",
        "                           tf.zeros((8,filter_num+latent_size,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((7,filter_num+latent_size,filter_num)), \n",
        "                           tf.reshape(f_y_4[1],(1,filter_num+latent_size,filter_num)), \n",
        "                           tf.zeros((7,filter_num+latent_size,filter_num)),\n",
        "                           tf.reshape(f_y_4[2],(1,filter_num+latent_size,filter_num)),\n",
        "                           tf.zeros((16,filter_num+latent_size,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_y'):\n",
        "    proj_w_y = tf.get_variable('project_w_y', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "#### sequence weight of y\n",
        "squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpRsIIdCj5so",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder(de_input,de_latent):\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "                                  f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(tf.concat((y_out_conv_dia_1, de_latent), axis=2), \n",
        "                                  f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(tf.concat((y_out_conv_dia_2, de_latent), axis=2), \n",
        "                                  f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(tf.concat((y_out_conv_dia_3, de_latent), axis=2), \n",
        "                                  f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "  y_out_project = tf.matmul(y_out_conv_dia, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "\n",
        "  target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "  logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "  prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_y\n",
        "\n",
        "log_liki_y_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_y_to.append(y_decoder(y_input_cnn[l],latent_variables[l]))\n",
        "log_liki_y_to = tf.stack(log_liki_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "AlCzKBlFNjQy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder_gene(de_input, de_latent):\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "                                  f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(tf.concat((y_out_conv_dia_1, de_latent), axis=2), \n",
        "                                  f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(tf.concat((y_out_conv_dia_2, de_latent), axis=2), \n",
        "                                  f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(tf.concat((y_out_conv_dia_3, de_latent), axis=2), \n",
        "                                  f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "  y_out_project = tf.matmul(y_out_conv_dia, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "  return logits_y\n",
        "\n",
        "logits_gene_y_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_y_to.append(y_decoder_gene(y_input_cnn[l], latent_variables[l]))\n",
        "logits_gene_y_to = tf.stack(logits_gene_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jb2g_kS1cKxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The lower bound of log-joint-likelihood, to maximize"
      ]
    },
    {
      "metadata": {
        "id": "5wHFYwB9jkDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# nega_log_liki_x_y = 0\n",
        "\n",
        "# nega_elbo = 0\n",
        "\n",
        "# for l in range(latent_num):\n",
        "#   nega_log_liki_x_y = nega_log_liki_x_y + tf.reduce_mean(- log_liki_x_to[l] - log_liki_y_to[l])\n",
        "#   nega_elbo = nega_elbo - log_liki_x_to[l] - log_liki_y_to[l]\n",
        "  \n",
        "# nega_log_liki_x_y = nega_log_liki_x_y/latent_num\n",
        "# nega_elbo = nega_elbo/latent_num + discount_placeholder*kl_div_loss\n",
        "# objective = tf.reduce_mean(nega_elbo) \n",
        "# kl_div_loss_batch_mean = tf.reduce_mean(kl_div_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z8P6-XM38MV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# L2 reguralization for trainable variables\n",
        "#train_variables = tf.trainable_variables()\n",
        "#regularization_cost = tf.reduce_sum([tf.nn.l2_loss(variable) for variable in train_variables])\n",
        "#regular_rate = 0.00001\n",
        "#+ regular_rate*regularization_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwMB6m32Yzfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizer = tf.train.AdamOptimizer(lr_placeholder)\n",
        "\n",
        "# gvs, var = zip(*optimizer.compute_gradients(objective))\n",
        "\n",
        "# #checked_gvs = [tf.where(tf.is_nan(grad), tf.zeros_like(grad), grad) for grad in gvs]\n",
        "\n",
        "# cliped_gvs, _ = tf.clip_by_global_norm(gvs, 1)\n",
        "\n",
        "# opt = optimizer.apply_gradients(zip(cliped_gvs, var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4Glrs-GxHeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "#gvs = optimizer.compute_gradients(objective)\n",
        "#capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
        "#opt = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASMl3Fgnsyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### save the model\n",
        "def save_model(session, path):\n",
        "    if not os.path.exists(\"./result_0821/\"):\n",
        "        os.mkdir('./result_0821/')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(session, path)\n",
        "\n",
        "path1 = './result_0821/model_each_epch.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E546I60IPRWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "Bxq1rcyggfA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "\n",
        "\n",
        "def text_save(content,filename,mode='a'):\n",
        "    # Try to save a list variable in txt file.\n",
        "    file = open(filename,mode)\n",
        "    for i in range(len(content)):\n",
        "        file.write(str(content[i])+'\\n')\n",
        "    file.close()\n",
        "    \n",
        "def text_read(filename):\n",
        "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
        "    try:\n",
        "        file = open(filename,'r')\n",
        "    except IOError:\n",
        "        error = []\n",
        "        return error\n",
        "    content = file.readlines()\n",
        " \n",
        "    for i in range(len(content)):\n",
        "        content[i] = content[i][:len(content[i])-1]\n",
        " \n",
        "    file.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RovgeCM_9-i3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 3\n",
        "total_step = 0\n",
        "learning_rate = 0.001\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "        fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "        \n",
        "#         en_input_drop_batches = en_input_batches\n",
        "#         fr_output_drop_batches = fr_output_batches\n",
        "        \n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            discount_rate = 0.0002*total_step\n",
        "            if discount_rate >1:\n",
        "              discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "        \n",
        "text_save(elbo_results, './result_0821/elbo_results.txt')\n",
        "text_save(kl_results, './result_0821/kl_results.txt')\n",
        "text_save(likei_results, './result_0821/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCryB1PjBEYF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "learning_rate = 0.001\n",
        "total_step = 0\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc + 5\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "        fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "#             discount_rate = 0.0002*total_step\n",
        "#             if discount_rate >1:\n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "\n",
        "text_save(elbo_results, './result_0821/elbo_results.txt')\n",
        "text_save(kl_results, './result_0821/kl_results.txt')\n",
        "text_save(likei_results, './result_0821/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DqH5fygZc0yl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load test data and test the model"
      ]
    },
    {
      "metadata": {
        "id": "T3EsytHdPkpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 1"
      ]
    },
    {
      "metadata": {
        "id": "Is72zrRCKFOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "cd694813-6cb3-4e25-e431-64ed4a3d0cac"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_1 = []\n",
        "nage_likeli_test_1 = []\n",
        "objecti_test_1 = []\n",
        "llx_test_1 = []\n",
        "lly_test_1 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = epoc + 12\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "    fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_1.append(kl)\n",
        "            nage_likeli_test_1.append(nage_likeli)\n",
        "            objecti_test_1.append(objecti)\n",
        "            llx_test_1.append(llx_mean)\n",
        "            lly_test_1.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_1))\n",
        "print(np.mean(nage_likeli_test_1))\n",
        "print(np.mean(objecti_test_1))\n",
        "print(np.mean(llx_test_1))\n",
        "print(np.mean(lly_test_1))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0821/model_each_epch.ckpt\n",
            "28.879263\n",
            "156.4435\n",
            "185.32275\n",
            "77.76605\n",
            "78.677444\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r7mH1flzQMlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 2"
      ]
    },
    {
      "metadata": {
        "id": "BAKb79bddTJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "eed1560d-ce78-478e-a227-579cb6431bfb"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = epoc + 11\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "    fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_2.append(kl)\n",
        "            nage_likeli_test_2.append(nage_likeli)\n",
        "            objecti_test_2.append(objecti)\n",
        "            llx_test_2.append(llx_mean)\n",
        "            lly_test_2.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0821/model_each_epch.ckpt\n",
            "28.95707\n",
            "157.08453\n",
            "186.04161\n",
            "78.13848\n",
            "78.94606\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-HIuHoabk85N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 3"
      ]
    },
    {
      "metadata": {
        "id": "pNniNB7Qk5kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "ff1952d4-30f1-4387-cc1c-631b53bf1ca5"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 10\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "    fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_2.append(kl)\n",
        "            nage_likeli_test_2.append(nage_likeli)\n",
        "            objecti_test_2.append(objecti)\n",
        "            llx_test_2.append(llx_mean)\n",
        "            lly_test_2.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0821/model_each_epch.ckpt\n",
            "29.4092\n",
            "156.79362\n",
            "186.20282\n",
            "78.16045\n",
            "78.63317\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n31e9XAOQ5hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sub-Train Set"
      ]
    },
    {
      "metadata": {
        "id": "yn8UHiro-RTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "4aaa7366-b079-4109-cc5b-2158ec86eb79"
      },
      "cell_type": "code",
      "source": [
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_3 = []\n",
        "nage_likeli_test_3 = []\n",
        "objecti_test_3 = []\n",
        "llx_test_3 = []\n",
        "lly_test_3 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = epoc + 6\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "    fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_3.append(kl)\n",
        "            nage_likeli_test_3.append(nage_likeli)\n",
        "            objecti_test_3.append(objecti)\n",
        "            llx_test_3.append(llx_mean)\n",
        "            lly_test_3.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_3))\n",
        "print(np.mean(nage_likeli_test_3))\n",
        "print(np.mean(objecti_test_3))\n",
        "print(np.mean(llx_test_3))\n",
        "print(np.mean(lly_test_3))"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0821/model_each_epch.ckpt\n",
            "29.144653\n",
            "148.17519\n",
            "177.31984\n",
            "74.05465\n",
            "74.120544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qYw-PZxCWtMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numerical Results"
      ]
    },
    {
      "metadata": {
        "id": "YO6RbSRWWsxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "e2b1b7f0-36be-49a9-cea3-6b68e873b657"
      },
      "cell_type": "code",
      "source": [
        "elbo_read = text_read('./result_0821/elbo_results.txt')\n",
        "elbo_read = [-float(elbo) for elbo in elbo_read]\n",
        "\n",
        "kl_read = text_read('./result_0821/kl_results.txt')\n",
        "kl_read = [float(kl) for kl in kl_read]\n",
        "\n",
        "likei_read = text_read('./result_0821/likei_results.txt')\n",
        "likei_read = [-float(likei) for likei in likei_read]\n",
        "\n",
        "plt.plot(kl_read, color = 'C0')\n",
        "plt.plot(likei_read, color = 'C1')\n",
        "plt.plot(elbo_read, color = 'C2')\n",
        "plt.legend(['kl divergence','nage_log_like_x_n_y', 'nage_elbo'], fontsize=12)\n",
        "plt.title(\"Encoder_2_GDCNN_300_dropout_0.8\", fontsize=16)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Encoder_2_GDCNN_300_dropout_0.8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFcCAYAAAAH/v1SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nLYQQQgxgEqiFEEKIAez/A1fFLzC4XX4HAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f57a7713710>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5TrHF_jAS4dD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "|                                       |  Sub-train     | Test 1        |   Test 2    |  \n",
        "|-----------------------------------|--------------------|-------------------|----------------|\n",
        "|Reconstruction loss   | -213.30219 | -202.64314 |-203.7571  | \n",
        "| Nega_logp(x\\z)          |  -104.58628 |-102.27634 |-103.01279   |\n",
        "| Nega_logp(y\\z)          |  -108.71591 | -100.36681 |--100.744286 |\n",
        "|KL Divergence             | 15.922356   |14.511001    |14.691068  |\n",
        "|ELBO                             | -229.22456 | -217.15414   |-218.44814  |"
      ]
    },
    {
      "metadata": {
        "id": "d9ljOnBFieQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sentence"
      ]
    },
    {
      "metadata": {
        "id": "Cj3oqBvyDN2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  6\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fbx0ui5Ls9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "JwAPQJRDL0Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y) \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qJUmxgXGMYHo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sen_len = 30\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def output_sentence(idd,x_de,y_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=[1,0,0,0])\n",
        "  print(en_bleu)\n",
        "  \n",
        "  ########## \"French\" ##########\n",
        "  origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"y:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, 30)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"y_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  fr_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=[1,0,0,0])\n",
        "  print(fr_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FRb2Ko3LoRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Case 9 \\ 64 \\ 68 \\ 77 \\ 78 \\ 89"
      ]
    },
    {
      "metadata": {
        "id": "6ospTXb7Dnjq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "a3e31980-6924-4d7b-e4d7-2b2a240cc4d5"
      },
      "cell_type": "code",
      "source": [
        "idd = 24\n",
        "origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"x:\")\n",
        "print(or_sens_str)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  In order to avoid problems arising again in the future on procedural timetables , it is necessary to improve the method for consulting the European Parliament , by setting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xE56sEu7Tnzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "17a68872-aee3-4209-c15f-e0a918327b91"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(9,x_de,y_de)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I appeal for an in @-@ depth debate on this subject .\n",
            "x_re:\n",
            "  in writing . - ( DE ) Madam President , Commissioner , ladies and gentlemen , I would like to express my sincere thanks to my fellow Members .\n",
            "0.10344827586206899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Je demande qu&apos; un débat approfondi soit mené sur ce sujet .\n",
            "y_re:\n",
            "  J&apos; ai voté en faveur de ce rapport .\n",
            "0.15922918012750872\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x3Z8UXjowCYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0ef1958a-c1c9-44d0-f060-82800caee3a0"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(64,x_de,y_de)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I also wish to thank Mr Almunia for the assistance he has given Cyprus all this time in achieving this objective .\n",
            "x_re:\n",
            "  I would like to express my sincere thanks to my colleague , Mrs Kolarska @-@ Bobińska , Mrs Kolarska @-@ Bobińska for her excellent work she has done .\n",
            "0.1724137931034483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Je voudrais également remercier M . Almunia pour l&apos; aide qu&apos; il a apportée à Chypre pendant tout ce temps en vue d&apos; atteindre cet objectif .\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport car j&apos; ai voté en faveur de ce rapport .\n",
            "0.1476986154218617\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zH4EiusrqlhT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "874da012-3c8e-47f7-9063-42e750626573"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(89,x_de,y_de)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  It is a mechanism that we have criticised here , in this Parliament , and that , I think , we continue to criticise .\n",
            "x_re:\n",
            "  Therefore , I would like to express my sincere thanks to the Committee on Foreign Affairs , I would like to express my sincere thanks to your rapporteurs .\n",
            "0.1724137931034483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  C&apos; est un mécanisme que nous avons critiqué ici , dans ce Parlement , et que , je pense , nous continuons de critiquer .\n",
            "y_re:\n",
            "  C&apos; est pourquoi j&apos; ai voté en faveur de ce rapport .\n",
            "0.14102726046114258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nvg92hyOIMYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "0a7ea1b5-7b07-49b6-c4fa-ead1f5c22b7f"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(77,x_de,y_de)"
      ],
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  The issue of the euro is no small matter for our fellow citizens : it is , in their hands , one of the European Union &apos;s most valuable\n",
            "x_re:\n",
            "  on behalf of the S &amp; D Group . - ( DE ) Mr President , ladies and gentlemen , on behalf of the Committee on Civil Liberties ,\n",
            "0.20689655172413796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  L&apos; affaire de l&apos; euro n&apos; est pas une petite affaire pour nos concitoyens : c&apos; est , entre leurs mains , un des biens les plus précieux de\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport parce que j&apos; ai voté en faveur de l&apos; égalité entre les femmes et\n",
            "0.1724137931034483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hcsgFtuxcD2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "b246a8d3-380d-44c3-c892-519676209176"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(78,x_de,y_de)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  Since last year , however , since the opening of the debate on the accession of Lithuania , we have had the impression that it has become a debate\n",
            "x_re:\n",
            "  Secondly , I would like to draw your attention to the importance of the Committee on Economic and Monetary Affairs , on behalf of the Committee on Foreign Affairs\n",
            "0.27586206896551724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Pourtant , depuis l&apos; année dernière , depuis l&apos; ouverture du débat sur l&apos; adhésion de la Lituanie , nous avons l&apos; impression qu&apos; il est devenu un débat\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport .\n",
            "0.027734206880067492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "VuCyUgNGHM4f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "abc1897c-e38f-4719-db25-7083f6c5ad21"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(77,x_de,y_de)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  The issue of the euro is no small matter for our fellow citizens : it is , in their hands , one of the European Union &apos;s most valuable\n",
            "x_re:\n",
            "  in writing . - ( DE ) Mr President , ladies and gentlemen , I voted in favour of the report on the European Globalisation Adjustment Fund ( EGF )\n",
            "0.23333333333333334\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  L&apos; affaire de l&apos; euro n&apos; est pas une petite affaire pour nos concitoyens : c&apos; est , entre leurs mains , un des biens les plus précieux de\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport parce que j&apos; ai voté en faveur de l&apos; égalité entre les femmes et\n",
            "0.1724137931034483\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDFnWZP5F3OC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ]
    },
    {
      "metadata": {
        "id": "EP3HmItwtua6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idd = 4\n",
        "origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"x:\")\n",
        "print(or_sens_str)\n",
        "\n",
        "origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"y:\")\n",
        "print(or_sens_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E71tUu9EF3OC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "62e88044-c9ab-4371-f52f-811c1b772cfb"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 77\n",
        "t = 0\n",
        "decode_len = 30\n",
        "\n",
        "eos_id = en_word_to_id['eos']\n",
        "eos_prob = -float('Inf')\n",
        "\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "#########################################################\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "y_score = np.zeros((beam_size))\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0821/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dquc4MUZRcpO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a3eceb01-430c-4ec1-a42b-07f989a84e3e"
      },
      "cell_type": "code",
      "source": [
        "logits_y.shape"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(50, 30, 30772)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "metadata": {
        "id": "6CKhxVAVMQQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Blue Score Functions"
      ]
    },
    {
      "metadata": {
        "id": "-flyQJg1YsX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_ngram(candidate, references, n):\n",
        "    clipped_count = 0\n",
        "    count = 0\n",
        "    r = 0\n",
        "    c = 0\n",
        "    for si in range(len(candidate)):\n",
        "        # Calculate precision for each sentence\n",
        "        ref_counts = []\n",
        "        ref_lengths = []\n",
        "        # Build dictionary of ngram counts\n",
        "        for reference in references:\n",
        "            ref_sentence = reference[si]\n",
        "            ngram_d = {}\n",
        "            words = ref_sentence.strip().split()\n",
        "            ref_lengths.append(len(words))\n",
        "            limits = len(words) - n + 1\n",
        "            # loop through the sentance consider the ngram length\n",
        "            for i in range(limits):\n",
        "                ngram = ' '.join(words[i:i+n]).lower()\n",
        "                if ngram in ngram_d.keys():\n",
        "                    ngram_d[ngram] += 1\n",
        "                else:\n",
        "                    ngram_d[ngram] = 1\n",
        "            ref_counts.append(ngram_d)\n",
        "        # candidate\n",
        "        cand_sentence = candidate[si]\n",
        "        cand_dict = {}\n",
        "        words = cand_sentence.strip().split()\n",
        "        limits = len(words) - n + 1\n",
        "        for i in range(0, limits):\n",
        "            ngram = ' '.join(words[i:i + n]).lower()\n",
        "            if ngram in cand_dict:\n",
        "                cand_dict[ngram] += 1\n",
        "            else:\n",
        "                cand_dict[ngram] = 1\n",
        "        clipped_count += clip_count(cand_dict, ref_counts)\n",
        "        count += limits\n",
        "        r += best_length_match(ref_lengths, len(words))\n",
        "        c += len(words)\n",
        "    if clipped_count == 0:\n",
        "        pr = 0\n",
        "    else:\n",
        "        pr = float(clipped_count) / count\n",
        "    bp = brevity_penalty(c, r)\n",
        "    return pr, bp\n",
        "\n",
        "\n",
        "def clip_count(cand_d, ref_ds):\n",
        "    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n",
        "    count = 0\n",
        "    for m in cand_d.keys():\n",
        "        m_w = cand_d[m]\n",
        "        m_max = 0\n",
        "        for ref in ref_ds:\n",
        "            if m in ref:\n",
        "                m_max = max(m_max, ref[m])\n",
        "        m_w = min(m_w, m_max)\n",
        "        count += m_w\n",
        "    return count\n",
        "\n",
        "\n",
        "def best_length_match(ref_l, cand_l):\n",
        "    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n",
        "    least_diff = abs(cand_l-ref_l[0])\n",
        "    best = ref_l[0]\n",
        "    for ref in ref_l:\n",
        "        if abs(cand_l-ref) < least_diff:\n",
        "            least_diff = abs(cand_l-ref)\n",
        "            best = ref\n",
        "    return best\n",
        "\n",
        "\n",
        "def brevity_penalty(c, r):\n",
        "    if c > r:\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.exp(1-(float(r)/c))\n",
        "    return bp\n",
        "\n",
        "\n",
        "def geometric_mean(precisions):\n",
        "    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n",
        "\n",
        "\n",
        "def sel_sentence_bleu(candidate, references):\n",
        "    precisions = []\n",
        "    for i in range(4):\n",
        "        pr, bp = count_ngram(candidate, references, i+1)\n",
        "        precisions.append(pr)\n",
        "    bleu = geometric_mean(precisions) * bp\n",
        "    return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FSPt39hBls6Z",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate Sentence"
      ]
    },
    {
      "metadata": {
        "id": "GBQusgLVls6a",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "k6iOTIl6ls6b",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_gene(logits_y, log_proba_y, log_prob_la, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] + log_proba_y[l] + log_prob_la[l]\n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  lower_to_x = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to_x = lower_to_x + lower_ob[l]\n",
        "  lower_ave_x = lower_to_x/latent_num\n",
        "  \n",
        "  return lower_ave, lower_ave_x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "45xZFiOKCove",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 32\n",
        "t = 0\n",
        "\n",
        "#x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "x_in = np.random.randint(low=0, high=en_vocab_size, size=(1, max_length))\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "prob_next_x = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "score = np.zeros((beam_size)) \n",
        "\n",
        "decode_len = 30\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for h in range(5):\n",
        "      gene_feed_dict = {input_placeholder: x_in, \n",
        "                        target_placeholder: y_in,\n",
        "                        in_length_placeholder: x_len, \n",
        "                        out_length_placeholder: y_len}                           \n",
        "            \n",
        "      mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "      la_var = []\n",
        "      log_prob_la = []  # latent_num \n",
        "      \n",
        "      for _ in range(latent_num):\n",
        "        eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "        la_var_sample = mean + std*eposida\n",
        "        la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "        la_var.append(la_var_sample)\n",
        "        log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "      \n",
        "      log_prob_la = np.asarray(log_prob_la)\n",
        "    \n",
        "      gene_feed_dict = {if_gene_placeholder: True,\n",
        "                        latent_var_placeholder:la_var,\n",
        "                        input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                        target_placeholder: y_in,\n",
        "                        out_length_placeholder: y_len}\n",
        "      log_proba_y = sess.run(log_liki_y_to, feed_dict=gene_feed_dict)             # latent_num \n",
        "      \n",
        "      log_proba_y = np.reshape(log_proba_y, (latent_num))\n",
        "      \n",
        "      score_latent_y = np.sum(log_proba_y+log_prob_la)/latent_num\n",
        "      \n",
        "      print(score_latent_y)\n",
        "    \n",
        "      for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: y_in}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          prob_next_word[j], prob_next_x[j] = find_next_word_beam_gene(logits_x, log_proba_y, log_prob_la, t)          \n",
        "          \n",
        "          prob_next_word[j] = prob_next_word[j] + score[j]\n",
        "          \n",
        "          prob_next_x[j] = prob_next_x[j] + score[j]\n",
        "        \n",
        "        beam_id = np.argmax(prob_next_word, axis=0)\n",
        "        \n",
        "        prob_next_word_beam = np.max(prob_next_word, axis=0)\n",
        "        \n",
        "        next_word_id = np.argsort(prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          beam_id_j = beam_id[next_word_id[j]]\n",
        "          word_id_j = next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[beam_id_j])          \n",
        "          x_de_new[j,t] = copy.deepcopy(word_id_j)\n",
        "          \n",
        "          score[j] = copy.deepcopy(prob_next_x[beam_id_j,word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        \n",
        "      x_in = copy.deepcopy(x_de[-1])\n",
        "      x_in = np.reshape(x_in,(1,max_length))\n",
        "      print(score[-1])\n",
        "      print(score[-1]+score_latent_y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Xkkj7_RBls6f",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search translate ###########\n",
        "\n",
        "beam_size = 30\n",
        "\n",
        "x_in = np.reshape(np.copy(en_test[2]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_test[2]), (1, max_length))\n",
        "\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#########################################################\n",
        "prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "\n",
        "x_len = np.reshape(np.copy(en_test_len[2]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_test_len[2]), (1,))\n",
        "\n",
        "score = np.zeros((beam_size))\n",
        "latent_score = np.zeros((latent_num))\n",
        "current_prob = np.zeros((beam_size, en_vocab_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "    \n",
        "    gene_feed_dict = {latent_var_placeholder:la_var,\n",
        "                      target_placeholder: y_in,\n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    log_y = sess.run(log_liki_y_to, feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    \n",
        "    for i in range(latent_num):\n",
        "      latent_score[i] = log_prob_la[i] + log_y[i]\n",
        "    \n",
        "    for t in range(10):      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "            \n",
        "          prob_next_word[j], current_prob[j]  = find_next_word_beam_first(logits_x, latent_score, t)          \n",
        "          \n",
        "        \n",
        "        beam_max_id = np.argmax(prob_next_word, axis=0)\n",
        "        beam_max = np.max(prob_next_word, axis=0)\n",
        "        \n",
        "        next_beam_id = np.argsort(beam_max)[-beam_size:]\n",
        "        \n",
        "        score_new = np.zeros((beam_size))\n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          beam_id = beam_max_id[next_beam_id[j]]\n",
        "          x_de_new[j] = x_de[beam_id]\n",
        "          x_de_new[j,t] = next_beam_id[j]\n",
        "          score_new[j] =  latent_score[beam_id] + current_prob[beam_id, next_beam_id[j]]\n",
        "        \n",
        "        x_de = x_de_new\n",
        "        latent_score = score_new\n",
        "        \n",
        "    print(np.mean(latent_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}