{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of Copy of Gene_translation_cnn_0810.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "7FRb2Ko3LoRL",
        "yzo3aNrhY07T"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rMcrrnEBsjy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the drive path"
      ]
    },
    {
      "metadata": {
        "id": "66FgEFRN-AMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWRjvljv-GT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vn79dig8uT8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "07628af6-584e-448a-a616-040dcaadec94"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MTeUgeB-seY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "8077c0a4-6561-4ea7-90dc-2b6bf96bea98"
      },
      "cell_type": "code",
      "source": [
        "cd drive/DCNN_100_epoch_2"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/DCNN_100_epoch_2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B12HWS4xsx4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ]
    },
    {
      "metadata": {
        "id": "Ly79OANzc7RS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "842o8uvRtBnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "wjDy6-mfnnit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load vocab dict from txt file\n",
        "\n",
        "f = open(\"../dictionary/en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../dictionary/fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuETWPMbu2Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "eb1c88a5-b4e7-48d6-f3f7-1ad69d3dd3d4"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "en_eos = en_word_to_id['eos']\n",
        "fr_eos = fr_word_to_id['eos']\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30772\n",
            "39578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9ifxSnpu6zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output\n",
        "\n",
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list\n",
        "\n",
        "\n",
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y57-AuNCvDSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length=30)\n",
        "\n",
        "    #out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    #fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehsowT7hwyjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juNYK867gw3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# en_oov_id = en_word_to_id['<OOV>']\n",
        "# fr_oov_id = fr_word_to_id['<OOV>']\n",
        "\n",
        "# def dropout_func(decode_input, dropout_prob, oov_id):\n",
        "#   for i in range(decode_input.shape[0]):\n",
        "#     for j in range(decode_input.shape[1]):\n",
        "#         for k in range(1,decode_input.shape[2]):\n",
        "#             if np.random.uniform() > dropout_prob:\n",
        "#                 decode_input[i,j,k] = oov_id        \n",
        "#   return decode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq37O5wR21AC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "zcHfesh3uCDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define parameters ######################\n",
        "\n",
        "max_length = 30\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "embed_size = 300\n",
        "\n",
        "infer_hidden_size = 1000\n",
        "\n",
        "latent_size = 100\n",
        "\n",
        "latent_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FraHS_clcvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### generate sentence #######################\n",
        "batch_size = 1\n",
        "\n",
        "latent_num = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJNVkwnhdg9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define placeholder ######################\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input')         # batch_size x max_length\n",
        "\n",
        "target_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target')       # batch_size x max_length\n",
        "\n",
        "in_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'in_len')              # batch_size x 1\n",
        "\n",
        "out_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'out_len')            # batch_size x 1\n",
        "\n",
        "discount_placeholder = tf.placeholder(tf.float32, name='discount')\n",
        "\n",
        "lr_placeholder = tf.placeholder(tf.float32, name='learn_rate')\n",
        "\n",
        "#input_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input_drop')   # batch_size x max_length\n",
        "\n",
        "#target_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target_drop') # batch_size x max_length\n",
        "\n",
        "if_gene_placeholder = tf.placeholder(tf.bool, name='if_gene')\n",
        "\n",
        "latent_var_placeholder = tf.placeholder(tf.float32, [latent_num, batch_size, max_length, latent_size], 'la_var')       # batch_size x max_length x latent_size\n",
        "\n",
        "xavier_initializer = tf.contrib.layers.xavier_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dqobGQGkHNg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### embedding look-up for input sentences ####################\n",
        "\n",
        "with tf.variable_scope('en_embedding'):\n",
        "    en_embedding = tf.get_variable('en_embeding',[en_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    inputs = tf.nn.embedding_lookup(en_embedding, input_placeholder)                      # batch_size x max_length x embed_size\n",
        "    \n",
        "\n",
        "with tf.variable_scope('fr_embedding'):\n",
        "    fr_embedding = tf.get_variable('fr_embeding',[fr_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    targets = tf.nn.embedding_lookup(fr_embedding, target_placeholder)                      # batch_size x max_length x embed_size\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmvdTgj5tSpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Inference Model - Encoder\n",
        "\n",
        "$q(z_1, z_2, ... , z_T|x,y)$\n",
        "\n",
        "Similar to the encoder of RNNSearch"
      ]
    },
    {
      "metadata": {
        "id": "OXsPo82WXXdX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### Inference model  #######################\n",
        "\n",
        "encode_inputs = tf.transpose(tf.concat([inputs, targets], axis=2), (1,0,2))\n",
        "\n",
        "with tf.variable_scope('encode'):\n",
        "    #basic_cell =tf.contrib.rnn.GRUCell(infer_hidden_size)\n",
        "    basic_cell = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state = basic_cell.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs, encode_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell, \n",
        "                                                                   cell_bw=basic_cell, \n",
        "                                                                   inputs=encode_inputs,                                                                \n",
        "                                                                   initial_state_fw=init_state,\n",
        "                                                                   initial_state_bw=init_state,\n",
        "                                                                   dtype=tf.float32,\n",
        "                                                                   time_major=True)\n",
        "#### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size)) # batch_size*max_length x 2*infer_hidden_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ectGLNLWXfSe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ##################### Inference model  #######################\n",
        "\n",
        "# ##################### bi-direction lstm of source sentence ######################\n",
        "\n",
        "# encode_inputs_x = tf.transpose(inputs, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_x'):\n",
        "#     basic_cell_x = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_x = basic_cell_x.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_x, encode_state_x = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_x, \n",
        "#                                                                        cell_bw=basic_cell_x, \n",
        "#                                                                        inputs=encode_inputs_x,\n",
        "#                                                                        sequence_length=in_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_x,\n",
        "#                                                                        initial_state_bw=init_state_x,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_x: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##################### bi-direction lstm of target sentence ######################\n",
        "\n",
        "# encode_inputs_y = tf.transpose(targets, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_y'):\n",
        "#     basic_cell_y = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_y = basic_cell_y.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_y, encode_state_y = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_y, \n",
        "#                                                                        cell_bw=basic_cell_y, \n",
        "#                                                                        inputs=encode_inputs_y,\n",
        "#                                                                        sequence_length=out_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_y,\n",
        "#                                                                        initial_state_bw=init_state_y,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_y: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "\n",
        "# ##################### concatenate the output of encoder of x and y ######################\n",
        "\n",
        "# fw_bw_en_outputs_x = tf.concat((encode_outputs_x[0],encode_outputs_x[1]),2)     # en_max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_outputs_y = tf.concat((encode_outputs_y[0],encode_outputs_y[1]),2)     # en_max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_outputs = tf.concat((fw_bw_en_outputs_x, fw_bw_en_outputs_y), 2)       # en_max_length x batch_size x 4*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_outputs_tran = tf.transpose(fw_bw_en_outputs, (1,0,2))                 # batch_size x en_max_length x 4*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_outputs_resh = tf.reshape(fw_bw_en_outputs_tran, (batch_size*max_length, 4*infer_hidden_size)) # batch_size*en_max_length x 4*infer_hidden_size\n",
        "\n",
        "\n",
        "# # la_mean = tf.matmul(fw_bw_en_outputs_resh, W_1) + b_1                              # batch_size*max_length x latent_size \n",
        "\n",
        "# # la_log_var = tf.matmul(fw_bw_en_outputs_resh, W_2) + b_2                           # batch_size*max_length x latent_size \n",
        "# # la_var = tf.exp(la_log_var)\n",
        "# # la_std = tf.sqrt(la_var)\n",
        "\n",
        "# # kl_div_loss = 1 + la_log_var - tf.square(la_mean) - la_var                               # batch_size*max_length x latent_size\n",
        "# # kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, axis=1)                         # batch_size*max_length x 1\n",
        "# # kl_div_loss = tf.reshape(kl_div_loss, (batch_size, max_length))                 # batch_size x max_length\n",
        "# # kl_div_loss = tf.reduce_sum(kl_div_loss, axis=1)\n",
        "\n",
        "# # #### sample the latent variable z by reparameterization trick\n",
        "\n",
        "# # eposida = tf.random_normal(tf.shape(la_std), mean=0.0,stddev=1)\n",
        "# # latent_variables = la_mean + la_std*eposida\n",
        "# # latent_variables = tf.reshape(latent_variables, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XJwm5BECin44",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('encode_projection'):\n",
        "    W_1 = tf.get_variable('W_1',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_1 = tf.get_variable('b_1',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_2 = tf.get_variable('W_2',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_2 = tf.get_variable('b_2',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "\n",
        "\n",
        "#fw_bw_en_outputs_norm = tf.contrib.layers.batch_norm(fw_bw_en_outputs_resh, center=True, scale=True)\n",
        "\n",
        "la_mean = tf.matmul(en_outputs_resh, W_1) + b_1                              # batch_size*max_length x latent_size \n",
        "\n",
        "la_log_var = tf.matmul(en_outputs_resh, W_2) + b_2                           # batch_size*max_length x latent_size \n",
        "la_var = tf.exp(la_log_var)\n",
        "la_std = tf.sqrt(la_var)\n",
        "\n",
        "kl_div_loss = 1 + la_log_var - tf.square(la_mean) - la_var                               # batch_size*max_length x latent_size\n",
        "kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, axis=1)                         # batch_size*max_length x 1\n",
        "kl_div_loss = tf.reshape(kl_div_loss, (batch_size, max_length))                 # batch_size x max_length\n",
        "kl_div_loss = tf.reduce_sum(kl_div_loss, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJY332jnGK_e",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# latent_variables_v = []\n",
        "# for _ in range(latent_num):\n",
        "#   eposida = tf.random_normal(tf.shape(la_std), mean=0.0,stddev=1)\n",
        "#   latent_variables_sample = la_mean + la_std*eposida\n",
        "#   latent_variables_sample = tf.reshape(latent_variables_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "#   latent_variables_v.append(latent_variables_sample)\n",
        "\n",
        "# def if_true():\n",
        "#   latent_v = []\n",
        "#   for h in range(latent_num):\n",
        "#     latent_v.append(latent_var_placeholder[h])\n",
        "#   return latent_v\n",
        "\n",
        "# def if_false():\n",
        "#   return latent_variables_v\n",
        "\n",
        "# latent_variables = tf.cond(if_gene_placeholder, if_true, if_false)\n",
        "\n",
        "# if latent_num == 1:\n",
        "#   new_latent_variables = []\n",
        "#   new_latent_variables.append(latent_variables)\n",
        "#   latent_variables = new_latent_variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDVWXdubzXh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_v = []\n",
        "for h in range(latent_num):\n",
        "  latent_v.append(latent_var_placeholder[h])\n",
        "    \n",
        "latent_variables = latent_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on6adzC8518v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Generation Model - Decoder\n",
        "\n",
        "$p_\\theta(x|z_1, z_2, ... , z_T)$\n",
        "\n",
        "$p_\\theta(y|z_1, z_2, ... , z_T)$"
      ]
    },
    {
      "metadata": {
        "id": "tiEDu_jlXEtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filter_num = 150\n",
        "\n",
        "filter_size = 3\n",
        "\n",
        "filter_size_only_pre = 2\n",
        "\n",
        "filter_size_pad = filter_size - filter_size_only_pre\n",
        "\n",
        "filter_zero_pad = tf.zeros(shape=[filter_size_pad, embed_size+latent_size, filter_num], dtype=tf.float32)\n",
        "filter_zero_pad_2 = tf.zeros(shape=[1, filter_size_pad, filter_num, filter_num], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSBf8rYS9hX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Generation Model for source sentence $p_\\theta(x|z_1, z_2, ... , z_T)$\n"
      ]
    },
    {
      "metadata": {
        "id": "UNQlel3UksZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### concat beg token with input\n",
        "\n",
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "#x_input_cnn_1 = tf.concat([latent_variables_1,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "#x_input_cnn_2 = tf.concat([latent_variables_2,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "#x_input_cnn_4D = tf.expand_dims(x_input_cnn, axis=1)                                        # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "x_input_cnn = []\n",
        "for l in range(latent_num):\n",
        "  x_input_cnn.append(tf.concat([latent_variables[l],x_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4x0f5AYajCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "    f_x_1 = tf.get_variable(\"x_filter_1\", shape=[filter_size_only_pre, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_1_dia = tf.concat([f_x_1, filter_zero_pad], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_2[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_2[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((4,filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_3[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_3[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((6,filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_4[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_4[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((10,filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "    \n",
        "    f_x_5 = tf.get_variable(\"x_filter_5\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_5_dia = tf.concat([tf.reshape(f_x_5[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_5[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_5[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((18,filter_num,filter_num))], axis=0)\n",
        "    # 29 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_6 = tf.get_variable(\"x_filter_6\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_6_dia = tf.concat([tf.reshape(f_x_6[0],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((1,filter_num,filter_num)), \n",
        "#                            tf.reshape(f_x_6[1],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((1,filter_num,filter_num)),\n",
        "#                            tf.reshape(f_x_6[2],(1,filter_num,filter_num)),\n",
        "#                            tf.zeros((4,filter_num,filter_num))], axis=0)\n",
        "#     # 29 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_7 = tf.get_variable(\"x_filter_7\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_7_dia = tf.concat([tf.reshape(f_x_7[0],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((2,filter_num,filter_num)), \n",
        "#                            tf.reshape(f_x_7[1],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((2,filter_num,filter_num)),\n",
        "#                            tf.reshape(f_x_7[2],(1,filter_num,filter_num)),\n",
        "#                            tf.zeros((6,filter_num,filter_num))], axis=0)\n",
        "#     # 29 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_8 = tf.get_variable(\"x_filter_8\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_8_dia = tf.concat([tf.reshape(f_x_8[0],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((4,filter_num,filter_num)), \n",
        "#                            tf.reshape(f_x_8[1],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((4,filter_num,filter_num)),\n",
        "#                            tf.reshape(f_x_8[2],(1,filter_num,filter_num)),\n",
        "#                            tf.zeros((10,filter_num,filter_num))], axis=0)\n",
        "#     # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_x'):\n",
        "    proj_w_x = tf.get_variable('project_w_x', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### sequence weight of x\n",
        "squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQIFBsJnkJWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder(de_input, latent_var):\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_6 = tf.nn.conv1d(x_out_conv_dia_5, f_x_6_dia, stride=1, padding='SAME') \n",
        "#   x_out_conv_dia_7 = tf.nn.conv1d(x_out_conv_dia_6, f_x_7_dia, stride=1, padding='SAME') \n",
        "#   x_out_conv_dia_8 = tf.nn.conv1d(x_out_conv_dia_7, f_x_8_dia, stride=1, padding='SAME') \n",
        "\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "  target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "  logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "  prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_x\n",
        "\n",
        "log_liki_x_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_x_to.append(x_decoder(x_input_cnn[l], latent_variables[l]))\n",
        "log_liki_x_to = tf.stack(log_liki_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4gCPfEbETNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "  return logits_x\n",
        "\n",
        "logits_gene_x_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l], latent_variables[l]))\n",
        "logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJjC0NuO9oE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Generation Model for target sentence $p_\\theta(y|z_1, z_2, ... , z_T)$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-H6P3ob3kPlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### concat beg token with target\n",
        "\n",
        "#beg_token_y = tf.zeros((1,embed_size))\n",
        "beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "y_input_cnn = []\n",
        "for l in range(latent_num):\n",
        "  y_input_cnn.append(tf.concat([latent_variables[l],y_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW1zf5TWPQnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "    f_y_1 = tf.get_variable(\"y_filter_1\", shape=[filter_size_only_pre, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_1_dia = tf.concat([f_y_1, filter_zero_pad], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_2[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_2[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((4,filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_3[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_3[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((6,filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_4[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_4[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((10,filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "    \n",
        "    f_y_5 = tf.get_variable(\"y_filter_5\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_5_dia = tf.concat([tf.reshape(f_y_5[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_5[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_5[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((18,filter_num,filter_num))], axis=0)\n",
        "    # 37 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_6 = tf.get_variable(\"y_filter_6\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_6_dia = tf.concat([tf.reshape(f_y_6[0],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((1,filter_num,filter_num)), \n",
        "#                            tf.reshape(f_y_6[1],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((1,filter_num,filter_num)),\n",
        "#                            tf.reshape(f_y_6[2],(1,filter_num,filter_num)),\n",
        "#                            tf.zeros((4,filter_num,filter_num))], axis=0)\n",
        "#     # 29 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_7 = tf.get_variable(\"y_filter_7\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_7_dia = tf.concat([tf.reshape(f_y_7[0],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((2,filter_num,filter_num)), \n",
        "#                            tf.reshape(f_y_7[1],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((2,filter_num,filter_num)),\n",
        "#                            tf.reshape(f_y_7[2],(1,filter_num,filter_num)),\n",
        "#                            tf.zeros((6,filter_num,filter_num))], axis=0)\n",
        "#     # 29 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_8 = tf.get_variable(\"y_filter_8\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_8_dia = tf.concat([tf.reshape(f_y_8[0],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((4,filter_num,filter_num)), \n",
        "#                            tf.reshape(f_y_8[1],(1,filter_num,filter_num)), \n",
        "#                            tf.zeros((4,filter_num,filter_num)),\n",
        "#                            tf.reshape(f_y_8[2],(1,filter_num,filter_num)),\n",
        "#                            tf.zeros((10,filter_num,filter_num))], axis=0)\n",
        "#     # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_y'):\n",
        "    proj_w_y = tf.get_variable('project_w_y', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "#### sequence weight of y\n",
        "squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpRsIIdCj5so",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder(de_input, latent_var):\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(de_input, f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(y_out_conv_dia_1, f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(y_out_conv_dia_2, f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(y_out_conv_dia_3, f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_5 = tf.nn.conv1d(y_out_conv_dia_4, f_y_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_6 = tf.nn.conv1d(y_out_conv_dia_5, f_y_6_dia, stride=1, padding='SAME') \n",
        "#   y_out_conv_dia_7 = tf.nn.conv1d(y_out_conv_dia_6, f_y_7_dia, stride=1, padding='SAME') \n",
        "#   y_out_conv_dia_8 = tf.nn.conv1d(y_out_conv_dia_7, f_y_8_dia, stride=1, padding='SAME') \n",
        "\n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  y_out_project = tf.matmul(y_out_conv_dia, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "\n",
        "  target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "  logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "  prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_y\n",
        "\n",
        "log_liki_y_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_y_to.append(y_decoder(y_input_cnn[l], latent_variables[l]))\n",
        "log_liki_y_to = tf.stack(log_liki_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EsuEbnv_4HqY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(de_input, f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(y_out_conv_dia_1, f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(y_out_conv_dia_2, f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(y_out_conv_dia_3, f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_5 = tf.nn.conv1d(y_out_conv_dia_4, f_y_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "\n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  y_out_project = tf.matmul(y_out_conv_dia, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "  \n",
        "  return logits_y\n",
        "\n",
        "logits_gene_y_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_y_to.append(y_decoder_gene(y_input_cnn[l], latent_variables[l]))\n",
        "logits_gene_y_to = tf.stack(logits_gene_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jb2g_kS1cKxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The lower bound of log-joint-likelihood, to maximize"
      ]
    },
    {
      "metadata": {
        "id": "5wHFYwB9jkDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nega_log_liki_x_y = 0\n",
        "\n",
        "nega_elbo = 0\n",
        "\n",
        "for l in range(latent_num):\n",
        "  nega_log_liki_x_y = nega_log_liki_x_y + tf.reduce_mean(- log_liki_x_to[l] - log_liki_y_to[l])\n",
        "  nega_elbo = nega_elbo - log_liki_x_to[l] - log_liki_y_to[l]\n",
        "  \n",
        "nega_log_liki_x_y = nega_log_liki_x_y/latent_num\n",
        "nega_elbo = nega_elbo/latent_num + discount_placeholder*kl_div_loss\n",
        "objective = tf.reduce_mean(nega_elbo) \n",
        "kl_div_loss_batch_mean = tf.reduce_mean(kl_div_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z8P6-XM38MV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# L2 reguralization for trainable variables\n",
        "#train_variables = tf.trainable_variables()\n",
        "#regularization_cost = tf.reduce_sum([tf.nn.l2_loss(variable) for variable in train_variables])\n",
        "#regular_rate = 0.00001\n",
        "#+ regular_rate*regularization_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwMB6m32Yzfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(lr_placeholder)\n",
        "\n",
        "gvs, var = zip(*optimizer.compute_gradients(objective))\n",
        "\n",
        "#checked_gvs = [tf.where(tf.is_nan(grad), tf.zeros_like(grad), grad) for grad in gvs]\n",
        "\n",
        "cliped_gvs, _ = tf.clip_by_global_norm(gvs, 1)\n",
        "\n",
        "opt = optimizer.apply_gradients(zip(cliped_gvs, var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4Glrs-GxHeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "#gvs = optimizer.compute_gradients(objective)\n",
        "#capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
        "#opt = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASMl3Fgnsyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### save the model\n",
        "def save_model(session, path):\n",
        "    if not os.path.exists(\"./result_DCNN_100_epoch_2/\"):\n",
        "        os.mkdir('./result_DCNN_100_epoch_2/')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(session, path)\n",
        "\n",
        "path1 = './result_DCNN_100_epoch_2/model_each_epch.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E546I60IPRWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "Bxq1rcyggfA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "\n",
        "\n",
        "def text_save(content,filename,mode='a'):\n",
        "    # Try to save a list variable in txt file.\n",
        "    file = open(filename,mode)\n",
        "    for i in range(len(content)):\n",
        "        file.write(str(content[i])+'\\n')\n",
        "    file.close()\n",
        "    \n",
        "def text_read(filename):\n",
        "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
        "    try:\n",
        "        file = open(filename,'r')\n",
        "    except IOError:\n",
        "        error = []\n",
        "        return error\n",
        "    content = file.readlines()\n",
        " \n",
        "    for i in range(len(content)):\n",
        "        content[i] = content[i][:len(content[i])-1]\n",
        " \n",
        "    file.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RovgeCM_9-i3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8461
        },
        "outputId": "f06b2a3b-c222-45ea-ce74-5e112e261c41"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "total_step = 0\n",
        "learning_rate = 0.001\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            discount_rate = 0.0003*total_step\n",
        "            if discount_rate >1:\n",
        "              discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "        \n",
        "text_save(elbo_results,  './result_DCNN_100_epoch_2/elbo_results.txt')\n",
        "text_save(kl_results,    './result_DCNN_100_epoch_2/kl_results.txt')\n",
        "text_save(likei_results, './result_DCNN_100_epoch_2/likei_results.txt')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/0_en.txt\n",
            "../small_txt/0_fr.txt\n",
            "23.786654\n",
            "475.71637\n",
            "475.71637\n",
            "0.0\n",
            "232.2209\n",
            "243.49547\n",
            "158.9858\n",
            "281.38794\n",
            "286.1575\n",
            "0.03\n",
            "137.11212\n",
            "144.27582\n",
            "212.99796\n",
            "254.29584\n",
            "267.0757\n",
            "0.06\n",
            "122.8067\n",
            "131.4891\n",
            "175.37418\n",
            "215.4591\n",
            "231.2428\n",
            "0.09\n",
            "106.942566\n",
            "108.516556\n",
            "223.0526\n",
            "172.95213\n",
            "199.71844\n",
            "0.12\n",
            "83.89365\n",
            "89.05849\n",
            "243.51053\n",
            "145.06328\n",
            "181.58984\n",
            "0.15\n",
            "70.86809\n",
            "74.19517\n",
            "227.82227\n",
            "120.749626\n",
            "161.75763\n",
            "0.18\n",
            "58.24099\n",
            "62.508648\n",
            "217.16142\n",
            "105.26943\n",
            "150.87334\n",
            "0.21\n",
            "50.411488\n",
            "54.857944\n",
            "233.32892\n",
            "93.080215\n",
            "149.07916\n",
            "0.24\n",
            "43.67275\n",
            "49.407475\n",
            "229.59995\n",
            "84.8375\n",
            "146.82948\n",
            "0.26999999999999996\n",
            "40.538097\n",
            "44.2994\n",
            "218.09457\n",
            "89.51201\n",
            "154.94038\n",
            "0.3\n",
            "41.091854\n",
            "48.420166\n",
            "192.32661\n",
            "84.04453\n",
            "147.51231\n",
            "0.32999999999999996\n",
            "39.581226\n",
            "44.463306\n",
            "208.28088\n",
            "84.76678\n",
            "159.74791\n",
            "0.36\n",
            "38.73187\n",
            "46.0349\n",
            "202.16847\n",
            "86.52542\n",
            "165.37114\n",
            "0.38999999999999996\n",
            "40.850475\n",
            "45.67495\n",
            "183.99776\n",
            "84.592\n",
            "161.87105\n",
            "0.42\n",
            "39.337963\n",
            "45.254044\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/1_en.txt\n",
            "../small_txt/1_fr.txt\n",
            "193.75523\n",
            "89.29465\n",
            "176.42636\n",
            "0.44969999999999993\n",
            "41.02389\n",
            "48.270752\n",
            "187.97664\n",
            "87.5292\n",
            "177.7016\n",
            "0.47969999999999996\n",
            "40.484127\n",
            "47.045063\n",
            "175.3168\n",
            "87.3436\n",
            "176.70256\n",
            "0.5096999999999999\n",
            "40.9429\n",
            "46.4007\n",
            "167.88873\n",
            "99.611084\n",
            "190.22061\n",
            "0.5397\n",
            "49.279694\n",
            "50.331387\n",
            "156.7135\n",
            "97.533516\n",
            "186.8132\n",
            "0.5697\n",
            "46.56814\n",
            "50.96538\n",
            "144.51361\n",
            "97.840614\n",
            "184.50543\n",
            "0.5996999999999999\n",
            "46.172455\n",
            "51.668156\n",
            "155.72177\n",
            "113.153915\n",
            "211.2119\n",
            "0.6296999999999999\n",
            "54.646503\n",
            "58.50741\n",
            "138.01865\n",
            "104.98105\n",
            "196.03195\n",
            "0.6597\n",
            "52.43944\n",
            "52.54159\n",
            "120.92544\n",
            "107.43292\n",
            "190.83522\n",
            "0.6897\n",
            "52.509083\n",
            "54.923832\n",
            "122.04308\n",
            "115.36155\n",
            "203.19594\n",
            "0.7196999999999999\n",
            "56.60687\n",
            "58.754673\n",
            "117.671036\n",
            "126.663124\n",
            "214.88112\n",
            "0.7496999999999999\n",
            "62.247314\n",
            "64.415825\n",
            "97.85813\n",
            "118.787575\n",
            "195.08755\n",
            "0.7797\n",
            "60.814594\n",
            "57.972984\n",
            "91.5738\n",
            "132.0452\n",
            "206.19252\n",
            "0.8097\n",
            "65.098305\n",
            "66.94689\n",
            "87.9175\n",
            "139.11926\n",
            "212.94357\n",
            "0.8396999999999999\n",
            "69.46591\n",
            "69.65334\n",
            "77.487434\n",
            "139.71893\n",
            "207.10977\n",
            "0.8696999999999999\n",
            "69.83482\n",
            "69.88411\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/2_en.txt\n",
            "../small_txt/2_fr.txt\n",
            "73.033875\n",
            "165.04146\n",
            "230.72812\n",
            "0.8993999999999999\n",
            "83.709915\n",
            "81.33155\n",
            "58.527023\n",
            "158.25522\n",
            "212.65024\n",
            "0.9293999999999999\n",
            "80.31877\n",
            "77.93644\n",
            "46.94673\n",
            "168.29909\n",
            "213.33978\n",
            "0.9593999999999999\n",
            "85.22763\n",
            "83.071465\n",
            "35.64762\n",
            "185.60112\n",
            "220.87088\n",
            "0.9894\n",
            "94.254425\n",
            "91.34669\n",
            "23.55909\n",
            "182.77069\n",
            "206.32979\n",
            "1\n",
            "92.42622\n",
            "90.344475\n",
            "17.781078\n",
            "189.54727\n",
            "207.32835\n",
            "1\n",
            "96.87158\n",
            "92.675674\n",
            "15.665216\n",
            "205.40607\n",
            "221.07129\n",
            "1\n",
            "102.65103\n",
            "102.75503\n",
            "12.879403\n",
            "199.43933\n",
            "212.31876\n",
            "1\n",
            "100.51389\n",
            "98.92546\n",
            "11.262792\n",
            "205.83385\n",
            "217.09663\n",
            "1\n",
            "103.41905\n",
            "102.414795\n",
            "11.183332\n",
            "215.27437\n",
            "226.45772\n",
            "1\n",
            "110.20623\n",
            "105.068146\n",
            "8.894646\n",
            "203.88344\n",
            "212.77809\n",
            "1\n",
            "101.466705\n",
            "102.41672\n",
            "8.4035635\n",
            "195.99947\n",
            "204.40305\n",
            "1\n",
            "99.325676\n",
            "96.673805\n",
            "7.5792418\n",
            "197.45099\n",
            "205.03021\n",
            "1\n",
            "99.90605\n",
            "97.54493\n",
            "6.8417625\n",
            "194.30467\n",
            "201.14641\n",
            "1\n",
            "99.0857\n",
            "95.218956\n",
            "7.0272384\n",
            "206.56262\n",
            "213.58986\n",
            "1\n",
            "103.28542\n",
            "103.2772\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 3\n",
            "../small_txt/3_en.txt\n",
            "../small_txt/3_fr.txt\n",
            "7.029676\n",
            "247.08928\n",
            "254.11894\n",
            "1\n",
            "109.33576\n",
            "137.75352\n",
            "6.6033964\n",
            "185.62833\n",
            "192.23172\n",
            "1\n",
            "96.270226\n",
            "89.35809\n",
            "7.2519746\n",
            "205.16539\n",
            "212.41739\n",
            "1\n",
            "105.30427\n",
            "99.86112\n",
            "6.5001607\n",
            "191.44691\n",
            "197.94707\n",
            "1\n",
            "98.6236\n",
            "92.82332\n",
            "6.746663\n",
            "197.78862\n",
            "204.53528\n",
            "1\n",
            "101.082596\n",
            "96.70599\n",
            "7.313199\n",
            "195.39134\n",
            "202.70454\n",
            "1\n",
            "100.30546\n",
            "95.08588\n",
            "7.2238307\n",
            "201.06245\n",
            "208.28629\n",
            "1\n",
            "104.1793\n",
            "96.883156\n",
            "6.9022756\n",
            "191.25166\n",
            "198.15393\n",
            "1\n",
            "98.00523\n",
            "93.246414\n",
            "7.143356\n",
            "194.53737\n",
            "201.68073\n",
            "1\n",
            "101.02141\n",
            "93.515976\n",
            "6.8562255\n",
            "183.99239\n",
            "190.84862\n",
            "1\n",
            "93.970695\n",
            "90.0217\n",
            "6.868303\n",
            "183.80637\n",
            "190.67468\n",
            "1\n",
            "92.9617\n",
            "90.84469\n",
            "6.9497094\n",
            "184.53821\n",
            "191.48792\n",
            "1\n",
            "93.08964\n",
            "91.44856\n",
            "6.810981\n",
            "190.16806\n",
            "196.97902\n",
            "1\n",
            "98.60916\n",
            "91.558876\n",
            "7.0731964\n",
            "184.92236\n",
            "191.99556\n",
            "1\n",
            "94.611755\n",
            "90.31062\n",
            "7.167299\n",
            "191.36508\n",
            "198.53236\n",
            "1\n",
            "99.00474\n",
            "92.36032\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 4\n",
            "../small_txt/4_en.txt\n",
            "../small_txt/4_fr.txt\n",
            "7.2913756\n",
            "192.7535\n",
            "200.04486\n",
            "1\n",
            "101.18527\n",
            "91.56823\n",
            "7.04014\n",
            "181.7507\n",
            "188.79086\n",
            "1\n",
            "93.46279\n",
            "88.28792\n",
            "7.5799603\n",
            "192.12456\n",
            "199.70453\n",
            "1\n",
            "99.57942\n",
            "92.54514\n",
            "7.3064227\n",
            "180.86926\n",
            "188.17569\n",
            "1\n",
            "90.36053\n",
            "90.50873\n",
            "7.5677466\n",
            "191.3471\n",
            "198.91484\n",
            "1\n",
            "98.55744\n",
            "92.78965\n",
            "7.526212\n",
            "175.06091\n",
            "182.58713\n",
            "1\n",
            "91.213974\n",
            "83.846954\n",
            "7.962272\n",
            "188.49228\n",
            "196.45454\n",
            "1\n",
            "97.65854\n",
            "90.83374\n",
            "7.9308434\n",
            "199.2048\n",
            "207.13565\n",
            "1\n",
            "103.701096\n",
            "95.50373\n",
            "8.036775\n",
            "190.90967\n",
            "198.94644\n",
            "1\n",
            "98.609825\n",
            "92.29985\n",
            "7.973376\n",
            "178.91888\n",
            "186.89227\n",
            "1\n",
            "91.71181\n",
            "87.20708\n",
            "8.249044\n",
            "189.30734\n",
            "197.55643\n",
            "1\n",
            "96.32495\n",
            "92.98242\n",
            "7.939161\n",
            "188.23007\n",
            "196.16924\n",
            "1\n",
            "98.89987\n",
            "89.33021\n",
            "8.107633\n",
            "186.09557\n",
            "194.2032\n",
            "1\n",
            "96.45331\n",
            "89.64226\n",
            "8.520359\n",
            "181.43701\n",
            "189.95738\n",
            "1\n",
            "94.06508\n",
            "87.371956\n",
            "8.3884735\n",
            "184.46057\n",
            "192.84903\n",
            "1\n",
            "95.43155\n",
            "89.02901\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pJmFsVuh9u_6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8478
        },
        "outputId": "708cf234-d3c2-402c-853d-db43b74ac1ed"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "learning_rate = 0.001\n",
        "total_step = 0\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc + 5\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            #discount_rate = 0.0003*total_step\n",
        "            #if discount_rate >1:\n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "        \n",
        "        \n",
        "text_save(elbo_results,  './result_DCNN_100_epoch_2/elbo_results.txt')\n",
        "text_save(kl_results,    './result_DCNN_100_epoch_2/kl_results.txt')\n",
        "text_save(likei_results, './result_DCNN_100_epoch_2/likei_results.txt')"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_DCNN_100_epoch_2/model_each_epch.ckpt\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/5_en.txt\n",
            "../small_txt/5_fr.txt\n",
            "11.73754\n",
            "157.2738\n",
            "169.01135\n",
            "1\n",
            "79.95221\n",
            "77.32162\n",
            "12.176352\n",
            "165.48885\n",
            "177.66522\n",
            "1\n",
            "84.580734\n",
            "80.90812\n",
            "12.499333\n",
            "167.12051\n",
            "179.61984\n",
            "1\n",
            "85.69482\n",
            "81.42568\n",
            "13.140164\n",
            "176.17462\n",
            "189.31479\n",
            "1\n",
            "91.22341\n",
            "84.9512\n",
            "12.754413\n",
            "167.00182\n",
            "179.75623\n",
            "1\n",
            "86.82022\n",
            "80.181595\n",
            "12.220492\n",
            "161.17766\n",
            "173.39815\n",
            "1\n",
            "83.00285\n",
            "78.1748\n",
            "12.989642\n",
            "171.45119\n",
            "184.44084\n",
            "1\n",
            "87.56153\n",
            "83.88968\n",
            "12.040816\n",
            "157.27126\n",
            "169.31207\n",
            "1\n",
            "80.43272\n",
            "76.83856\n",
            "12.146176\n",
            "157.26631\n",
            "169.41248\n",
            "1\n",
            "80.58493\n",
            "76.68138\n",
            "12.801794\n",
            "170.03848\n",
            "182.84027\n",
            "1\n",
            "86.83835\n",
            "83.20013\n",
            "13.495182\n",
            "174.7434\n",
            "188.23856\n",
            "1\n",
            "90.27526\n",
            "84.46811\n",
            "13.177563\n",
            "172.1751\n",
            "185.35266\n",
            "1\n",
            "88.61036\n",
            "83.564735\n",
            "12.985726\n",
            "168.30286\n",
            "181.28859\n",
            "1\n",
            "87.388054\n",
            "80.9148\n",
            "12.36842\n",
            "159.74287\n",
            "172.11128\n",
            "1\n",
            "80.76654\n",
            "78.976326\n",
            "13.322602\n",
            "170.06691\n",
            "183.38953\n",
            "1\n",
            "86.957245\n",
            "83.10969\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/6_en.txt\n",
            "../small_txt/6_fr.txt\n",
            "13.205642\n",
            "166.31674\n",
            "179.52238\n",
            "1\n",
            "83.314\n",
            "83.00273\n",
            "13.648445\n",
            "158.34296\n",
            "171.99141\n",
            "1\n",
            "80.35276\n",
            "77.990204\n",
            "14.622235\n",
            "162.8002\n",
            "177.42244\n",
            "1\n",
            "84.09281\n",
            "78.70739\n",
            "13.957577\n",
            "168.55415\n",
            "182.51176\n",
            "1\n",
            "85.24742\n",
            "83.30675\n",
            "13.411359\n",
            "161.4402\n",
            "174.85156\n",
            "1\n",
            "81.54767\n",
            "79.89253\n",
            "14.506314\n",
            "165.44614\n",
            "179.95244\n",
            "1\n",
            "84.1402\n",
            "81.305954\n",
            "13.928899\n",
            "151.5058\n",
            "165.4347\n",
            "1\n",
            "76.5845\n",
            "74.92128\n",
            "12.576172\n",
            "153.1921\n",
            "165.76826\n",
            "1\n",
            "76.8787\n",
            "76.313385\n",
            "14.326943\n",
            "160.35854\n",
            "174.68549\n",
            "1\n",
            "81.42499\n",
            "78.933556\n",
            "14.5101595\n",
            "163.5187\n",
            "178.02887\n",
            "1\n",
            "83.02249\n",
            "80.496216\n",
            "14.352268\n",
            "165.39021\n",
            "179.74248\n",
            "1\n",
            "83.613014\n",
            "81.77719\n",
            "13.611449\n",
            "155.14818\n",
            "168.75963\n",
            "1\n",
            "76.85408\n",
            "78.2941\n",
            "14.432503\n",
            "164.31918\n",
            "178.7517\n",
            "1\n",
            "82.762024\n",
            "81.557175\n",
            "13.3885\n",
            "142.6266\n",
            "156.01509\n",
            "1\n",
            "72.204926\n",
            "70.42167\n",
            "14.627819\n",
            "160.19916\n",
            "174.827\n",
            "1\n",
            "81.441414\n",
            "78.75775\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/7_en.txt\n",
            "../small_txt/7_fr.txt\n",
            "14.504131\n",
            "160.05522\n",
            "174.55936\n",
            "1\n",
            "79.320915\n",
            "80.7343\n",
            "14.572474\n",
            "165.93643\n",
            "180.50888\n",
            "1\n",
            "85.431175\n",
            "80.505264\n",
            "13.355552\n",
            "151.086\n",
            "164.44156\n",
            "1\n",
            "76.09488\n",
            "74.99113\n",
            "14.953662\n",
            "170.68686\n",
            "185.64055\n",
            "1\n",
            "87.80125\n",
            "82.88563\n",
            "14.944839\n",
            "166.23318\n",
            "181.17802\n",
            "1\n",
            "85.20343\n",
            "81.02977\n",
            "14.216169\n",
            "156.69846\n",
            "170.91461\n",
            "1\n",
            "79.0723\n",
            "77.626144\n",
            "14.41644\n",
            "160.6606\n",
            "175.07703\n",
            "1\n",
            "80.56412\n",
            "80.096466\n",
            "15.030985\n",
            "165.7196\n",
            "180.75061\n",
            "1\n",
            "83.42087\n",
            "82.29875\n",
            "14.843497\n",
            "163.35703\n",
            "178.20053\n",
            "1\n",
            "84.08793\n",
            "79.2691\n",
            "14.194044\n",
            "163.78374\n",
            "177.9778\n",
            "1\n",
            "83.399345\n",
            "80.38441\n",
            "15.274215\n",
            "169.99467\n",
            "185.2689\n",
            "1\n",
            "85.26125\n",
            "84.73342\n",
            "13.922707\n",
            "157.71762\n",
            "171.64034\n",
            "1\n",
            "79.24787\n",
            "78.46976\n",
            "14.605277\n",
            "154.07033\n",
            "168.6756\n",
            "1\n",
            "77.307785\n",
            "76.76255\n",
            "14.014102\n",
            "164.17332\n",
            "178.18742\n",
            "1\n",
            "84.02735\n",
            "80.145966\n",
            "13.760506\n",
            "149.53513\n",
            "163.29561\n",
            "1\n",
            "75.67667\n",
            "73.85846\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 3\n",
            "../small_txt/8_en.txt\n",
            "../small_txt/8_fr.txt\n",
            "14.643473\n",
            "161.76602\n",
            "176.40948\n",
            "1\n",
            "82.346375\n",
            "79.41964\n",
            "14.69566\n",
            "157.4038\n",
            "172.09946\n",
            "1\n",
            "79.14349\n",
            "78.26031\n",
            "14.439359\n",
            "160.17531\n",
            "174.61469\n",
            "1\n",
            "81.13553\n",
            "79.03979\n",
            "14.997311\n",
            "155.33664\n",
            "170.33394\n",
            "1\n",
            "79.23883\n",
            "76.09782\n",
            "14.388367\n",
            "145.16982\n",
            "159.55818\n",
            "1\n",
            "73.15319\n",
            "72.016624\n",
            "15.097625\n",
            "160.19662\n",
            "175.29425\n",
            "1\n",
            "81.08384\n",
            "79.112785\n",
            "15.14995\n",
            "162.9692\n",
            "178.11914\n",
            "1\n",
            "81.88263\n",
            "81.08658\n",
            "14.815932\n",
            "162.8258\n",
            "177.64174\n",
            "1\n",
            "81.46579\n",
            "81.36002\n",
            "15.151202\n",
            "169.1379\n",
            "184.28908\n",
            "1\n",
            "86.022484\n",
            "83.11542\n",
            "14.562808\n",
            "157.30646\n",
            "171.86926\n",
            "1\n",
            "79.85416\n",
            "77.4523\n",
            "14.874277\n",
            "151.4801\n",
            "166.3544\n",
            "1\n",
            "77.6155\n",
            "73.8646\n",
            "13.980945\n",
            "150.42384\n",
            "164.4048\n",
            "1\n",
            "75.66073\n",
            "74.76312\n",
            "15.3587\n",
            "164.71707\n",
            "180.07576\n",
            "1\n",
            "83.43662\n",
            "81.28045\n",
            "15.791582\n",
            "157.11319\n",
            "172.90477\n",
            "1\n",
            "79.17824\n",
            "77.93493\n",
            "15.390603\n",
            "161.95937\n",
            "177.34996\n",
            "1\n",
            "82.946396\n",
            "79.012955\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 4\n",
            "../small_txt/9_en.txt\n",
            "../small_txt/9_fr.txt\n",
            "15.064618\n",
            "155.78706\n",
            "170.8517\n",
            "1\n",
            "78.8254\n",
            "76.96167\n",
            "14.86524\n",
            "157.9384\n",
            "172.80365\n",
            "1\n",
            "79.10835\n",
            "78.830055\n",
            "15.273862\n",
            "168.94305\n",
            "184.21692\n",
            "1\n",
            "85.800674\n",
            "83.142365\n",
            "14.65701\n",
            "153.34677\n",
            "168.00378\n",
            "1\n",
            "78.02703\n",
            "75.31974\n",
            "14.96317\n",
            "156.20943\n",
            "171.17258\n",
            "1\n",
            "79.24584\n",
            "76.96358\n",
            "14.215096\n",
            "154.56233\n",
            "168.77744\n",
            "1\n",
            "78.141525\n",
            "76.42082\n",
            "14.419311\n",
            "146.77266\n",
            "161.19197\n",
            "1\n",
            "73.89628\n",
            "72.87637\n",
            "14.954296\n",
            "153.96036\n",
            "168.91464\n",
            "1\n",
            "78.61718\n",
            "75.34317\n",
            "15.059552\n",
            "158.33734\n",
            "173.3969\n",
            "1\n",
            "79.7009\n",
            "78.63644\n",
            "14.343219\n",
            "150.7049\n",
            "165.04813\n",
            "1\n",
            "77.15974\n",
            "73.54517\n",
            "15.132442\n",
            "160.15466\n",
            "175.28711\n",
            "1\n",
            "82.412346\n",
            "77.742325\n",
            "14.323181\n",
            "145.36763\n",
            "159.69083\n",
            "1\n",
            "72.954666\n",
            "72.41297\n",
            "15.419302\n",
            "157.43001\n",
            "172.84932\n",
            "1\n",
            "80.49333\n",
            "76.936676\n",
            "14.253354\n",
            "150.71857\n",
            "164.97194\n",
            "1\n",
            "75.958534\n",
            "74.76004\n",
            "15.309286\n",
            "158.58965\n",
            "173.89893\n",
            "1\n",
            "79.60491\n",
            "78.98473\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Q8PCfIMaiEKY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "7de01773-ba0b-4be0-b9ea-6090a29e6dca"
      },
      "cell_type": "code",
      "source": [
        "elbo_read = text_read('./result_DCNN_100_epoch_2/elbo_results.txt')\n",
        "elbo_read = [float(elbo) for elbo in elbo_read]\n",
        "\n",
        "kl_read = text_read('./result_DCNN_100_epoch_2/kl_results.txt')\n",
        "kl_read = [float(kl) for kl in kl_read]\n",
        "\n",
        "likei_read = text_read('./result_DCNN_100_epoch_2/likei_results.txt')\n",
        "likei_read = [float(likei) for likei in likei_read]\n",
        "\n",
        "plt.plot(kl_read, color = 'C0')\n",
        "plt.plot(likei_read, color = 'C1')\n",
        "plt.plot(elbo_read, color = 'C2')\n",
        "plt.legend(['kl divergence','nage_log_like_x_n_y', 'nage_elbo'], fontsize=12)\n",
        "plt.title(\"DCNN_500\", fontsize=16)"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'DCNN_500')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFcCAYAAAAZN83hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f3a443a4a90>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "DqH5fygZc0yl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load test data and test the model"
      ]
    },
    {
      "metadata": {
        "id": "T3EsytHdPkpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 1"
      ]
    },
    {
      "metadata": {
        "id": "3EnIt8iWjMFA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "6a2c4356-ca04-4afb-f050-0b2120e18b16"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "\n",
        "ind_small_txt = 12\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "print(en_file)\n",
        "print(fr_file)\n",
        "        \n",
        "en_test, en_test_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_test, fr_test_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_test_batches, en_test_len_batches = batch_producer(en_test, en_test_len, batch_size) \n",
        "fr_test_batches, fr_test_len_batches = batch_producer(fr_test, fr_test_len, batch_size)\n",
        "                       \n",
        "################################################################################\n",
        "############################### testing ########################################\n",
        "################################################################################\n",
        "\n",
        "kl_test_1 = []\n",
        "nage_likeli_test_1 = []\n",
        "objecti_test_1 = []\n",
        "llx_test_1 = []\n",
        "lly_test_1 = []\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    print(\"wa\")\n",
        "    \n",
        "    ########### testing ###########\n",
        "    for i in range(en_test_batches.shape[0]):\n",
        "          \n",
        "        discount_rate = 1\n",
        "        \n",
        "        test_feed_dict = {input_placeholder: en_test_batches[i], \n",
        "                     target_placeholder: fr_test_batches[i],\n",
        "                     in_length_placeholder: en_test_len_batches[i], \n",
        "                     out_length_placeholder: fr_test_len_batches[i],\n",
        "                     discount_placeholder: discount_rate,\n",
        "                     if_gene_placeholder: False,\n",
        "                     latent_var_placeholder: zero_latent}\n",
        "        \n",
        "        kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=test_feed_dict)       \n",
        "                                   \n",
        "        kl_test_1.append(kl)\n",
        "        nage_likeli_test_1.append(nage_likeli)\n",
        "        objecti_test_1.append(objecti)\n",
        "        llx_test_1.append(llx)\n",
        "        lly_test_1.append(lly)\n",
        "\n",
        "print(np.mean(kl_test_1))\n",
        "print(np.mean(nage_likeli_test_1))\n",
        "print(np.mean(objecti_test_1))\n",
        "print(np.mean(llx_test_1))\n",
        "print(np.mean(lly_test_1))"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../small_txt/12_en.txt\n",
            "../small_txt/12_fr.txt\n",
            "INFO:tensorflow:Restoring parameters from ./result_DCNN_100_epoch_2/model_each_epch.ckpt\n",
            "wa\n",
            "14.088301\n",
            "176.21794\n",
            "190.30624\n",
            "-88.3593\n",
            "-87.85864\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r7mH1flzQMlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 2"
      ]
    },
    {
      "metadata": {
        "id": "BAKb79bddTJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "6da65b6e-75d9-4ba3-84f6-772ca0165a2e"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "ind_small_txt = 11\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "print(en_file)\n",
        "print(fr_file)\n",
        "        \n",
        "en_test, en_test_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_test, fr_test_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_test_batches, en_test_len_batches = batch_producer(en_test, en_test_len, batch_size) \n",
        "fr_test_batches, fr_test_len_batches = batch_producer(fr_test, fr_test_len, batch_size)\n",
        "                       \n",
        "######## test set ########\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    ########### training ###########\n",
        "    for i in range(en_test_batches.shape[0]):\n",
        "          \n",
        "        discount_rate = 1\n",
        "        \n",
        "        test_feed_dict = {input_placeholder: en_test_batches[i], \n",
        "                          target_placeholder: fr_test_batches[i],\n",
        "                          in_length_placeholder: en_test_len_batches[i], \n",
        "                          out_length_placeholder: fr_test_len_batches[i],\n",
        "                          discount_placeholder: discount_rate,\n",
        "                          lr_placeholder: learning_rate,\n",
        "                          if_gene_placeholder: False,\n",
        "                          latent_var_placeholder: zero_latent}\n",
        "             \n",
        "        kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=test_feed_dict)       \n",
        "                                   \n",
        "        kl_test_2.append(kl)\n",
        "        nage_likeli_test_2.append(nage_likeli)\n",
        "        objecti_test_2.append(objecti)\n",
        "        llx_test_2.append(llx)\n",
        "        lly_test_2.append(lly)\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../small_txt/11_en.txt\n",
            "../small_txt/11_fr.txt\n",
            "INFO:tensorflow:Restoring parameters from ./result_DCNN_100_epoch_2/model_each_epch.ckpt\n",
            "14.171515\n",
            "177.78476\n",
            "191.95627\n",
            "-89.53014\n",
            "-88.25461\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n31e9XAOQ5hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sub-Train Set"
      ]
    },
    {
      "metadata": {
        "id": "uE7gOKrYmJMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "e6607324-2a38-42ae-e473-331ac90ea46a"
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  6\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "\n",
        "######## test set ########\n",
        "kl_test_3 = []\n",
        "nage_likeli_test_3 = []\n",
        "objecti_test_3 = []\n",
        "llx_test_3 = []\n",
        "lly_test_3 = []\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    print(en_file)\n",
        "    print(fr_file)\n",
        "    \n",
        "    ########### training ###########\n",
        "    for i in range(500):\n",
        "          \n",
        "        discount_rate = 1\n",
        "        \n",
        "        subtrain_feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                     target_placeholder: fr_output_batches[i],\n",
        "                     in_length_placeholder: en_input_len_batches[i], \n",
        "                     out_length_placeholder: fr_output_len_batches[i],\n",
        "                     discount_placeholder: discount_rate,\n",
        "                     if_gene_placeholder: False,\n",
        "                     latent_var_placeholder: zero_latent}\n",
        "       \n",
        "        kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=subtrain_feed_dict)       \n",
        "                                   \n",
        "        kl_test_3.append(kl)\n",
        "        nage_likeli_test_3.append(nage_likeli)\n",
        "        objecti_test_3.append(objecti)\n",
        "        llx_test_3.append(llx)\n",
        "        lly_test_3.append(lly)\n",
        "\n",
        "print(np.mean(kl_test_3))\n",
        "print(np.mean(nage_likeli_test_3))\n",
        "print(np.mean(objecti_test_3))\n",
        "print(np.mean(llx_test_3))\n",
        "print(np.mean(lly_test_3))"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_DCNN_100_epoch_2/model_each_epch.ckpt\n",
            "../small_txt/6_en.txt\n",
            "../small_txt/6_fr.txt\n",
            "14.650325\n",
            "161.88243\n",
            "176.53276\n",
            "-82.05018\n",
            "-79.83227\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5TrHF_jAS4dD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "|                                       |  Sub-train     | Test 1        |   Test 2    |  Test    |  \n",
        "|-----------------------------------|--------------------|-------------------|----------------|---------------|\n",
        "|Reconstruction loss   | -161.87569 | -177.78476 |-176.21794  | -177.00135|\n",
        "| Nega_logp(x\\z)          |  -82.03589   | -89.53014  | -88.3593  |-88.94472 |\n",
        "| Nega_logp(y\\z)          |  -79.8398  |  -88.25461     | -87.85864 |-88.056625| \n",
        "|KL Divergence             | 14.650325 | 14.171515   |  14.0883 |14.1299|\n",
        "|ELBO                             | -176.52602 |-191.95627   |-190.30624  |-191.13125|"
      ]
    },
    {
      "metadata": {
        "id": "d9ljOnBFieQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sentence"
      ]
    },
    {
      "metadata": {
        "id": "Cj3oqBvyDN2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  6\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fbx0ui5Ls9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "JwAPQJRDL0Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y) \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ywNKxdMLNMI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def output_sentence(idd,x_de,y_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len], weights=[1,0,0,0])\n",
        "  print(en_bleu)\n",
        "  \n",
        "  ########## \"French\" ##########\n",
        "  origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"y:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, 30)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"y_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  fr_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=[1,0,0,0])\n",
        "  print(fr_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYs3_536bI6L",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Case 9 \\ 64 \\ 68 \\ 77 \\ 78 \\ 89"
      ]
    },
    {
      "metadata": {
        "id": "FKSns0ylDA23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "6796b4a6-d6e8-49b6-831c-e1aebcf33ce9"
      },
      "cell_type": "code",
      "source": [
        "idd = 24\n",
        "origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"x:\")\n",
        "print(or_sens_str)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  In order to avoid problems arising again in the future on procedural timetables , it is necessary to improve the method for consulting the European Parliament , by setting\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "2EBLalUbMwsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "cc8c75e7-ec97-4e02-a8e9-b9aa6f9c578c"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(9,x_de,y_de)"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I appeal for an in @-@ depth debate on this subject .\n",
            "x_re:\n",
            "  I would like to thank the rapporteur for his excellent work .\n",
            "0.25\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Je demande qu&apos; un débat approfondi soit mené sur ce sujet .\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport .\n",
            "0.12500000000000003\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "erRZVNvSPVvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "136bd64b-e7f1-49c9-d1aa-ce456da63f39"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(64,x_de,y_de)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I also wish to thank Mr Almunia for the assistance he has given Cyprus all this time in achieving this objective .\n",
            "x_re:\n",
            "  I would like to thank Mr President , ladies and gentlemen , on behalf of the Committee on Civil Liberties , Justice and Home Affairs , on behalf of\n",
            "0.1724137931034483\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Je voudrais également remercier M . Almunia pour l&apos; aide qu&apos; il a apportée à Chypre pendant tout ce temps en vue d&apos; atteindre cet objectif .\n",
            "y_re:\n",
            "  J&apos; ai voté en faveur de la mobilisation du Fonds européen d&apos; ajustement à la mondialisation ( FEM ) en faveur de la mobilisation du Fonds européen d&apos; ajustement\n",
            "0.10344827586206899\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "vuY8c1UuTe7x",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "46a46bc0-70b1-492c-e996-3bbacf5fc8f8"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(89,x_de,y_de)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  It is a mechanism that we have criticised here , in this Parliament , and that , I think , we continue to criticise .\n",
            "x_re:\n",
            "  It is an important step towards the freedom of expression of the European Union .\n",
            "0.1026834238065184\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  C&apos; est un mécanisme que nous avons critiqué ici , dans ce Parlement , et que , je pense , nous continuons de critiquer .\n",
            "y_re:\n",
            "  C&apos; est pourquoi j&apos; ai voté en faveur de ce rapport .\n",
            "0.14102726046114258\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "J8S_KlDZTuxl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "5489d1b9-7f8a-4acd-ec01-09559992a1d2"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(77,x_de,y_de)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  The issue of the euro is no small matter for our fellow citizens : it is , in their hands , one of the European Union &apos;s most valuable\n",
            "x_re:\n",
            "  on behalf of the ECR Group . - Mr President , I would like to thank my fellow Members .\n",
            "0.12752563032435468\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  L&apos; affaire de l&apos; euro n&apos; est pas une petite affaire pour nos concitoyens : c&apos; est , entre leurs mains , un des biens les plus précieux de\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport .\n",
            "0.027734206880067492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "xR5obarBbsch",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "outputId": "3c37d4ae-f48d-4cc2-c373-504fbe863cdf"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(78,x_de,y_de)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  Since last year , however , since the opening of the debate on the accession of Lithuania , we have had the impression that it has become a debate\n",
            "x_re:\n",
            "  in writing . - I voted in favour of this motion for a resolution on the mobilisation of the European Globalisation Adjustment Fund ( EGF ) in favour of\n",
            "0.20689655172413796\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Pourtant , depuis l&apos; année dernière , depuis l&apos; ouverture du débat sur l&apos; adhésion de la Lituanie , nous avons l&apos; impression qu&apos; il est devenu un débat\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport sur la mobilisation du Fonds européen d&apos; ajustement à la mondialisation ( FEM )\n",
            "0.13793103448275862\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "sI5YrWqGHPfS",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 285
        },
        "outputId": "be659c64-d4d7-4f3c-d9b0-6e9bbd411c7f"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(24,x_de,y_de)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  In order to avoid problems arising again in the future on procedural timetables , it is necessary to improve the method for consulting the European Parliament , by setting\n",
            "x_re:\n",
            "  on behalf of the S &amp; D Group . - Mr President , I would like to thank the rapporteur for his excellent work .\n",
            "0.20451450935189075\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Afin d&apos; éviter que des problèmes surviennent encore à l&apos; avenir concernant le calendrier de procédure , il y a lieu d&apos; améliorer la méthode de consultation du Parlement\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport parce que j&apos; ai voté en faveur de ce rapport parce que je pense\n",
            "0.10344827586206899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c4ozFg9Yt1cf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "outputId": "395abbae-cf49-434c-da85-470c0416feee"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(77,x_de,y_de)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  The issue of the euro is no small matter for our fellow citizens : it is , in their hands , one of the European Union &apos;s most valuable\n",
            "x_re:\n",
            "  on behalf of the Group of the European People &apos;s Party ( Christian Democrats ) .\n",
            "0.16640524128040496\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  L&apos; affaire de l&apos; euro n&apos; est pas une petite affaire pour nos concitoyens : c&apos; est , entre leurs mains , un des biens les plus précieux de\n",
            "y_re:\n",
            "  par écrit . - ( EN ) J&apos; ai voté en faveur de ce rapport .\n",
            "0.027734206880067492\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "mTAHkcxVT1rJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ]
    },
    {
      "metadata": {
        "id": "FjKtJhDStxCw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "idd = 4\n",
        "origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"x:\")\n",
        "print(or_sens_str)\n",
        "\n",
        "origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"y:\")\n",
        "print(or_sens_str)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3AFeQ3XwLbKj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "d0309d5d-3dcf-449b-cee9-4d6f3a2d1c1a"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 77\n",
        "t = 0\n",
        "decode_len = 30\n",
        "\n",
        "eos_id = en_word_to_id['eos']\n",
        "eos_prob = -float('Inf')\n",
        "\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "#########################################################\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "y_score = np.zeros((beam_size))\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_DCNN_100_epoch_2/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6CKhxVAVMQQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Blue Score Functions"
      ]
    },
    {
      "metadata": {
        "id": "-flyQJg1YsX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_ngram(candidate, references, n):\n",
        "    clipped_count = 0\n",
        "    count = 0\n",
        "    r = 0\n",
        "    c = 0\n",
        "    for si in range(len(candidate)):\n",
        "        # Calculate precision for each sentence\n",
        "        ref_counts = []\n",
        "        ref_lengths = []\n",
        "        # Build dictionary of ngram counts\n",
        "        for reference in references:\n",
        "            ref_sentence = reference[si]\n",
        "            ngram_d = {}\n",
        "            words = ref_sentence.strip().split()\n",
        "            ref_lengths.append(len(words))\n",
        "            limits = len(words) - n + 1\n",
        "            # loop through the sentance consider the ngram length\n",
        "            for i in range(limits):\n",
        "                ngram = ' '.join(words[i:i+n]).lower()\n",
        "                if ngram in ngram_d.keys():\n",
        "                    ngram_d[ngram] += 1\n",
        "                else:\n",
        "                    ngram_d[ngram] = 1\n",
        "            ref_counts.append(ngram_d)\n",
        "        # candidate\n",
        "        cand_sentence = candidate[si]\n",
        "        cand_dict = {}\n",
        "        words = cand_sentence.strip().split()\n",
        "        limits = len(words) - n + 1\n",
        "        for i in range(0, limits):\n",
        "            ngram = ' '.join(words[i:i + n]).lower()\n",
        "            if ngram in cand_dict:\n",
        "                cand_dict[ngram] += 1\n",
        "            else:\n",
        "                cand_dict[ngram] = 1\n",
        "        clipped_count += clip_count(cand_dict, ref_counts)\n",
        "        count += limits\n",
        "        r += best_length_match(ref_lengths, len(words))\n",
        "        c += len(words)\n",
        "    if clipped_count == 0:\n",
        "        pr = 0\n",
        "    else:\n",
        "        pr = float(clipped_count) / count\n",
        "    bp = brevity_penalty(c, r)\n",
        "    return pr, bp\n",
        "\n",
        "\n",
        "def clip_count(cand_d, ref_ds):\n",
        "    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n",
        "    count = 0\n",
        "    for m in cand_d.keys():\n",
        "        m_w = cand_d[m]\n",
        "        m_max = 0\n",
        "        for ref in ref_ds:\n",
        "            if m in ref:\n",
        "                m_max = max(m_max, ref[m])\n",
        "        m_w = min(m_w, m_max)\n",
        "        count += m_w\n",
        "    return count\n",
        "\n",
        "\n",
        "def best_length_match(ref_l, cand_l):\n",
        "    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n",
        "    least_diff = abs(cand_l-ref_l[0])\n",
        "    best = ref_l[0]\n",
        "    for ref in ref_l:\n",
        "        if abs(cand_l-ref) < least_diff:\n",
        "            least_diff = abs(cand_l-ref)\n",
        "            best = ref\n",
        "    return best\n",
        "\n",
        "\n",
        "def brevity_penalty(c, r):\n",
        "    if c > r:\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.exp(1-(float(r)/c))\n",
        "    return bp\n",
        "\n",
        "\n",
        "def geometric_mean(precisions):\n",
        "    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n",
        "\n",
        "\n",
        "def sel_sentence_bleu(candidate, references):\n",
        "    precisions = []\n",
        "    for i in range(4):\n",
        "        pr, bp = count_ngram(candidate, references, i+1)\n",
        "        precisions.append(pr)\n",
        "    bleu = geometric_mean(precisions) * bp\n",
        "    return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcCYvSrNMpDp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate Sentence"
      ]
    },
    {
      "metadata": {
        "id": "vQ36FC_AM1Mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "OlZ6FVjzMrDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_tran(logits_y, latent_score, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    #y_max = np.max(logits_y[l,t])\n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l]  + latent_score[l]\n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave,  lower_ob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZV6l0TCMxnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search translate ###########\n",
        "\n",
        "beam_size = 30\n",
        "\n",
        "x_in = np.reshape(np.copy(en_test[2]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_test[2]), (1, max_length))\n",
        "\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#########################################################\n",
        "prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "\n",
        "x_len = np.reshape(np.copy(en_test_len[2]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_test_len[2]), (1,))\n",
        "\n",
        "score = np.zeros((beam_size))\n",
        "latent_score = np.zeros((latent_num))\n",
        "current_prob = np.zeros((beam_size, en_vocab_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "    \n",
        "    gene_feed_dict = {latent_var_placeholder:la_var,\n",
        "                      target_placeholder: y_in,\n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    log_y = sess.run(log_liki_y_to, feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    \n",
        "    for i in range(latent_num):\n",
        "      latent_score[i] = log_prob_la[i] + log_y[i]\n",
        "    \n",
        "    for t in range(10):      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "            \n",
        "          prob_next_word[j], current_prob[j]  = find_next_word_beam_first(logits_x, latent_score, t)          \n",
        "          \n",
        "        \n",
        "        beam_max_id = np.argmax(prob_next_word, axis=0)\n",
        "        beam_max = np.max(prob_next_word, axis=0)\n",
        "        \n",
        "        next_beam_id = np.argsort(beam_max)[-beam_size:]\n",
        "        \n",
        "        score_new = np.zeros((beam_size))\n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          beam_id = beam_max_id[next_beam_id[j]]\n",
        "          x_de_new[j] = x_de[beam_id]\n",
        "          x_de_new[j,t] = next_beam_id[j]\n",
        "          score_new[j] =  latent_score[beam_id] + current_prob[beam_id, next_beam_id[j]]\n",
        "        \n",
        "        x_de = x_de_new\n",
        "        latent_score = score_new\n",
        "        \n",
        "    print(np.mean(latent_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}