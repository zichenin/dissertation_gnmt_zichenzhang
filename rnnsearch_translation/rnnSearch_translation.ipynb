{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "rnnSearch_translation_v4.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rMcrrnEBsjy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the drive path"
      ]
    },
    {
      "metadata": {
        "id": "66FgEFRN-AMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWRjvljv-GT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OSojfZFw-rrO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "14b88839-ae60-41f1-c3cb-af0de4931a92"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mdrive\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MTeUgeB-seY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 33
        },
        "outputId": "7a419ab3-722c-4410-a357-11203e3ac930"
      },
      "cell_type": "code",
      "source": [
        "cd drive/RNNSearch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/RNNSearch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B12HWS4xsx4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ]
    },
    {
      "metadata": {
        "id": "Ly79OANzc7RS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "842o8uvRtBnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "wjDy6-mfnnit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load vocab dict from txt file\n",
        "\n",
        "f = open(\"../dictionary/en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../dictionary/fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuETWPMbu2Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        },
        "outputId": "6b686747-acaf-4d27-cff0-d45cd80f9158"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "\n",
        "fr_word_to_id['<beg>'] = len(fr_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30772\n",
            "39579\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9ifxSnpu6zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output\n",
        "\n",
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list\n",
        "\n",
        "\n",
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)\n",
        "              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)\n",
        "            \n",
        "    return data, each_sen_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y57-AuNCvDSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length)\n",
        "\n",
        "    out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehsowT7hwyjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq37O5wR21AC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "ygcXeO285H37",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define parameters ######################\n",
        "\n",
        "max_length = 40     # max sentence length\n",
        "\n",
        "batch_size = 50\n",
        "\n",
        "embed_size = 300\n",
        "\n",
        "hidden_size = 500\n",
        "\n",
        "align_size = 500\n",
        "\n",
        "maxout_size = 500\n",
        "\n",
        "learning_rate = 0.001\n",
        "\n",
        "regular_rate = 0.00001"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "58gMURKN21T9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define placeholder ######################\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.int32, [None, max_length], 'input')                   # batch_size x en_max_length\n",
        "\n",
        "target_placeholder = tf.placeholder(tf.int32, [None, max_length+1], 'target')                 # batch_size x fr_max_length\n",
        "\n",
        "in_length_placeholder = tf.placeholder(tf.int32, [None, ], 'in_len')                         # batch_size x 1\n",
        "\n",
        "out_length_placeholder = tf.placeholder(tf.int32, [None, ], 'out_len')                       # batch_size x 1\n",
        "\n",
        "xavier_initializer = tf.contrib.layers.xavier_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5g-_TLMB4Nox",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### embedding look-up for input sentences ######################\n",
        "\n",
        "with tf.variable_scope('en_embedding'):\n",
        "    en_embedding = tf.get_variable('en_embeding',[en_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    inputs = tf.nn.embedding_lookup(en_embedding, input_placeholder)                                      # batch_size x en_max_length x embed_size\n",
        "    \n",
        "\n",
        "with tf.variable_scope('fr_embedding'):\n",
        "    fr_embedding = tf.get_variable('fr_embeding',[fr_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    targets = tf.nn.embedding_lookup(fr_embedding, target_placeholder)                                    # batch_size x fr_max_length x embed_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_Tbx5tdGcP4j",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 135
        },
        "outputId": "a44755cd-3d35-4e69-d5ab-8639354e7d6a"
      },
      "cell_type": "code",
      "source": [
        "##################### encoder ######################\n",
        "##################### bi-direction lstm ######################\n",
        "\n",
        "encode_inputs = tf.transpose(inputs, (1,0,2))                                   # en_max_length x batch_size x embed_size\n",
        "  \n",
        "with tf.variable_scope('encode'):\n",
        "    basic_cell = tf.contrib.rnn.BasicLSTMCell(hidden_size, forget_bias=0.0, state_is_tuple=True)\n",
        "    init_state = basic_cell.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs, encode_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell, cell_bw=basic_cell, inputs=encode_inputs,\n",
        "                                                                   sequence_length=in_length_placeholder,\n",
        "                                                                   initial_state_fw=init_state,\n",
        "                                                                   initial_state_bw=init_state,\n",
        "                                                                   dtype=tf.float32,\n",
        "                                                                   time_major=True)"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:417: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "seq_dim is deprecated, use seq_axis instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:432: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "batch_dim is deprecated, use batch_axis instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "j9QZ1eBncWAh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### handle the final state of encoder ######################\n",
        "\n",
        "fw_bw_en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)           # en_max_length x batch_size x 2*hidden_size\n",
        "fw_bw_en_outputs_list = tf.split(fw_bw_en_outputs, max_length, axis=0)       # en_max_length x batch_size x 2*hidden_size\n",
        "\n",
        "annotations = tf.transpose(fw_bw_en_outputs,(1,0,2))                                      # batch_size x en_max_length x 2*hidden_size\n",
        "annotations_reshaped = tf.reshape(annotations, (batch_size*max_length, 2*hidden_size)) # batch_size*en_max_length x 2*hidden_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6EknYY939VFe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### parameters ######################\n",
        "\n",
        "with tf.variable_scope('alignment'):\n",
        "    W_a = tf.get_variable('W_a',[hidden_size, align_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    U_a = tf.get_variable('U_a',[2*hidden_size, align_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    v_a = tf.get_variable('v_a',[align_size,1], dtype=tf.float32, initializer=xavier_initializer)\n",
        "      \n",
        "\n",
        "with tf.variable_scope('decoder'):\n",
        "    W_s_init = tf.get_variable('W_s_init',[hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    W_z = tf.get_variable('W_z',[embed_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    U_z = tf.get_variable('U_z',[hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_z = tf.get_variable('C_z',[2*hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    W_r = tf.get_variable('W_r',[embed_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    U_r = tf.get_variable('U_r',[hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_r = tf.get_variable('C_r',[2*hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    W_ss = tf.get_variable('W_ss',[embed_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    U_ss = tf.get_variable('U_ss',[hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_ss = tf.get_variable('C_ss',[2*hidden_size, hidden_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "with tf.variable_scope('probability'):\n",
        "    W_o = tf.get_variable('W_o',[maxout_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    U_o = tf.get_variable('U_o',[hidden_size, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_o = tf.get_variable('V_o',[embed_size,  2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_o = tf.get_variable('C_o',[2*hidden_size, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "v4OUiArTUJHt",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "UaHj = tf.matmul(annotations_reshaped, U_a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a-sd1PMpJq4s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### decoder ######################\n",
        "\n",
        "#### define the inital hidden state for decoder\n",
        "s_0 = tf.nn.tanh(tf.matmul(encode_state[1].h, W_s_init))                        # batch_size x hidden_size\n",
        "prob_norm = []\n",
        "prob_constant = []\n",
        "\n",
        "#### decoder loop ####\n",
        "\n",
        "for i in range(max_length):\n",
        "    \n",
        "    #### alignment model\n",
        "    WaS0 = tf.matmul(s_0, W_a)                                                  # batch_size x align_size\n",
        "    WaS0_allsteps = tf.tile(WaS0, [max_length, 1])                        # batch_size*en_max_length x align_size\n",
        "    \n",
        "    e_1 = tf.matmul(tf.nn.tanh(WaS0_allsteps + UaHj), v_a)                      # batch_size*en_max_length x 1\n",
        "    e_1j = tf.reshape(e_1, [batch_size,max_length])                        # batch_size x en_max_length\n",
        "    \n",
        "    alpha_1 = tf.nn.softmax(e_1j)                                               # batch_size x en_max_length\n",
        "    alpha_1_reshaped = tf.reshape(alpha_1,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "    \n",
        "    \n",
        "    #### context vector\n",
        "    c_1j = tf.reshape(annotations_reshaped*alpha_1_reshaped, (batch_size, max_length, 2*hidden_size))  # batch_size x en_max_length x 2*hidden_size\n",
        "    c_1 = tf.reduce_sum(c_1j, axis=1)                                                                     # batch_size x 2*hidden_size\n",
        "    \n",
        "    \n",
        "    #### decoder hidden variables\n",
        "    r_1 = tf.nn.sigmoid(tf.matmul(targets[:,i,:], W_r) + tf.matmul(s_0, U_r) + tf.matmul(c_1, C_r))               # batch_size x hidden_size\n",
        "\n",
        "    z_1 = tf.nn.sigmoid(tf.matmul(targets[:,i,:], W_z) + tf.matmul(s_0, U_z) + tf.matmul(c_1, C_z))               # batch_size x hidden_size\n",
        "\n",
        "    ss_1 = tf.nn.tanh(tf.matmul(targets[:,i,:], W_ss) + tf.matmul(r_1*s_0, U_ss) + tf.matmul(c_1, C_ss))          # batch_size x hidden_size\n",
        "    \n",
        "    s_1 = (1-z_1)*s_0 + z_1*ss_1                                                                                  # batch_size x hidden_size\n",
        "    \n",
        "    \n",
        "    #### probability of the target word\n",
        "    tt_1 = tf.matmul(s_0, U_o) + tf.matmul(targets[:,i,:], V_o) + tf.matmul(c_1, C_o)       # batch_size x 2*maxout_size\n",
        "    t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                # batch_size x maxout_size\n",
        "    \n",
        "    WoT1 = tf.matmul(t_1, W_o)\n",
        "    prob_unnorm_1 = tf.exp(tf.reduce_sum(targets[:,i+1,:]*WoT1, axis=1))                      # batch_size x 1\n",
        "    \n",
        "    prob_constant_1 = tf.exp(tf.matmul(WoT1, tf.transpose(fr_embedding,(1,0))))               # batch_size x fr_vocab_size\n",
        "    \n",
        "    prob_norm_1 = prob_unnorm_1/tf.reduce_sum(prob_constant_1, axis=1)                            # batch_size x 1\n",
        "    \n",
        "    prob_constant.append(prob_constant_1)\n",
        "\n",
        "    prob_norm.append(prob_norm_1)\n",
        "    \n",
        "    \n",
        "    #### update the hidden state\n",
        "    s_0 = s_1\n",
        " \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_gpWU1asqf9h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "log_prob_norm = tf.log(tf.stack(prob_norm, axis=1))\n",
        "\n",
        "out_seq_weight = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)\n",
        "\n",
        "nll = - tf.reduce_sum(log_prob_norm*out_seq_weight, axis=1)\n",
        "\n",
        "nll_average = tf.reduce_mean(nll)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JaU0t-oftVYk",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train_variables = tf.trainable_variables()\n",
        "\n",
        "regularization_cost = tf.reduce_sum([tf.nn.l2_loss(variable) for variable in train_variables])\n",
        "\n",
        "objective = nll_average + regular_rate*regularization_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vq4yC_KTRPM-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizer = tf.train.AdadeltaOptimizer(learning_rate=0.001, rho=0.95, epsilon=1e-6)\n",
        "# train_op = optimizer.minimize(nll_average)\n",
        "\n",
        "opt = tf.train.AdamOptimizer(learning_rate).minimize(objective)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASMl3Fgnsyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### save the model\n",
        "def save_model(session, path):\n",
        "    if not os.path.exists(\"./result_4/\"):\n",
        "        os.mkdir('./result_4/')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(session, path)\n",
        "\n",
        "path1 = './result_4/model_each_epch.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E546I60IPRWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "hRuhu8YXQ_lB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "file_en_test = \"../translation_test_data/newstest2012.en\"\n",
        "\n",
        "file_fr_test = \"../translation_test_data/newstest2012.fr\"\n",
        "\n",
        "en_test, en_test_len = generate_input_en(file_en_test, en_word_to_id, max_length)\n",
        "\n",
        "fr_test, fr_test_len = generate_output_fr(file_fr_test, fr_word_to_id, max_length)\n",
        "\n",
        "file_en_test = \"../translation_test_data/newstest2013.en\"\n",
        "\n",
        "file_fr_test = \"../translation_test_data/newstest2013.fr\"\n",
        "\n",
        "en_test_2, en_test_len_2 = generate_input_en(file_en_test, en_word_to_id, max_length)\n",
        "\n",
        "fr_test_2, fr_test_len_2 = generate_output_fr(file_fr_test, fr_word_to_id, max_length)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "l16rmpp9ULuS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_test = np.concatenate((en_test,en_test_2),axis=0)\n",
        "en_test_len = np.concatenate((en_test_len,en_test_len_2),axis=0)\n",
        "\n",
        "fr_test = np.concatenate((fr_test,fr_test_2),axis=0)\n",
        "fr_test_len = np.concatenate((fr_test_len,fr_test_len_2),axis=0)\n",
        "\n",
        "en_test_batches, en_test_len_batches = batch_producer(en_test, en_test_len, batch_size) \n",
        "fr_test_batches, fr_test_len_batches = batch_producer(fr_test, fr_test_len, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uwvBKRZ-74vJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 1\n",
        "\n",
        "nll_aver_results=[]\n",
        "nll_aver_dep_results = []\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "      \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "        \n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i]}\n",
        "      \n",
        "            nll_aver, regu_cost, _ = sess.run([nll_average,regularization_cost,opt], feed_dict=feed_dict)\n",
        "      \n",
        "            if i%100 == 0:\n",
        "                nll_aver_results.append(nll_aver)\n",
        "                print(nll_aver)\n",
        "                #print(regu_cost)\n",
        "        \n",
        "            if i%1000 == 0:\n",
        "                save_model(sess, path1)\n",
        "              \n",
        "                test_nll = []\n",
        "                for h in range(en_test_batches.shape[0]):\n",
        "                  \n",
        "                  test_feed_dict = {input_placeholder: en_test_batches[h], \n",
        "                                    target_placeholder: fr_test_batches[h],\n",
        "                                    in_length_placeholder: en_test_len_batches[h], \n",
        "                                    out_length_placeholder: fr_test_len_batches[h]}\n",
        "                  \n",
        "                  test_nll_aver = sess.run(nll_average, feed_dict=test_feed_dict)\n",
        "                  test_nll.append(test_nll_aver)\n",
        "                \n",
        "                print(\"tesing result: updated every 1000 updates\")\n",
        "                print(np.mean(test_nll))\n",
        "                nll_aver_dep_results.append(np.mean(test_nll))\n",
        "                  \n",
        "              \n",
        "                "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVeNvu_bFZmj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1360
        },
        "outputId": "80501cc0-9841-49c9-935b-e2fc4f298afa"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 2\n",
        "nll_aver_results = []\n",
        "nll_aver_dep_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc + 1\n",
        "        \n",
        "        en_file = \"./small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"./small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "        \n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i]}\n",
        "      \n",
        "            nll_aver, _ = sess.run([nll_average,opt], feed_dict=feed_dict)\n",
        "      \n",
        "            if i%100 == 0:\n",
        "                nll_aver_results.append(nll_aver)\n",
        "                print(nll_aver)\n",
        "            \n",
        "            if i%1000 == 0:\n",
        "                save_model(sess, path1)\n",
        "              \n",
        "                test_nll = []\n",
        "                for h in range(en_test_batches.shape[0]):\n",
        "                  \n",
        "                  test_feed_dict = {input_placeholder: en_test_batches[h], \n",
        "                                    target_placeholder: fr_test_batches[h],\n",
        "                                    in_length_placeholder: en_test_len_batches[h], \n",
        "                                    out_length_placeholder: fr_test_len_batches[h]}\n",
        "                  \n",
        "                  test_nll_aver = sess.run(nll_average, feed_dict=test_feed_dict)\n",
        "                  test_nll.append(test_nll_aver)\n",
        "                \n",
        "                print(\"tesing result: updated every 1000 updates\")\n",
        "                print(np.mean(test_nll))\n",
        "                nll_aver_dep_results.append(np.mean(test_nll))\n",
        "            "
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_4/model_each_epch.ckpt\n",
            "Epoch 0\n",
            "./small_txt/1_en.txt\n",
            "./small_txt/1_fr.txt\n",
            "109.3119\n",
            "tesing result: updated every 1000 updates\n",
            "123.16238\n",
            "103.4042\n",
            "102.48878\n",
            "108.78156\n",
            "84.672424\n",
            "100.86406\n",
            "100.90387\n",
            "97.66413\n",
            "80.823746\n",
            "92.261345\n",
            "91.72873\n",
            "tesing result: updated every 1000 updates\n",
            "112.958984\n",
            "92.79151\n",
            "87.88254\n",
            "88.835175\n",
            "91.120316\n",
            "91.90158\n",
            "93.43369\n",
            "100.87776\n",
            "81.57022\n",
            "93.91861\n",
            "89.25955\n",
            "tesing result: updated every 1000 updates\n",
            "106.317345\n",
            "75.979355\n",
            "74.57379\n",
            "91.66147\n",
            "94.33628\n",
            "87.79422\n",
            "64.76533\n",
            "87.8458\n",
            "67.85983\n",
            "97.79245\n",
            "Epoch 1\n",
            "./small_txt/2_en.txt\n",
            "./small_txt/2_fr.txt\n",
            "89.917755\n",
            "tesing result: updated every 1000 updates\n",
            "103.07902\n",
            "85.867256\n",
            "78.4876\n",
            "73.660675\n",
            "84.77483\n",
            "84.61196\n",
            "63.287315\n",
            "86.49585\n",
            "83.71025\n",
            "72.51001\n",
            "82.783005\n",
            "tesing result: updated every 1000 updates\n",
            "97.94598\n",
            "76.75386\n",
            "79.65358\n",
            "74.85089\n",
            "70.279274\n",
            "79.21473\n",
            "72.214226\n",
            "81.79712\n",
            "73.43077\n",
            "72.368866\n",
            "80.73326\n",
            "tesing result: updated every 1000 updates\n",
            "96.30542\n",
            "77.83921\n",
            "72.02315\n",
            "69.0442\n",
            "59.855392\n",
            "71.98671\n",
            "72.45532\n",
            "73.70856\n",
            "55.199867\n",
            "81.51866\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "31YBnMxa-es_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5372df1b-71f9-4720-f047-31d6ab6d4096"
      },
      "cell_type": "code",
      "source": [
        "len(nll_aver_results)"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "metadata": {
        "id": "VqWdRISVaUBi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def text_save(content,filename,mode='a'):\n",
        "    # Try to save a list variable in txt file.\n",
        "    file = open(filename,mode)\n",
        "    for i in range(len(content)):\n",
        "        file.write(str(content[i])+'\\n')\n",
        "    file.close()\n",
        "    \n",
        "def text_read(filename):\n",
        "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
        "    try:\n",
        "        file = open(filename,'r')\n",
        "    except IOError:\n",
        "        error = []\n",
        "        return error\n",
        "    content = file.readlines()\n",
        " \n",
        "    for i in range(len(content)):\n",
        "        content[i] = content[i][:len(content[i])-1]\n",
        " \n",
        "    file.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vS-QFk2duXGo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "text_save(nll_aver_results,'./nll_aver_result_v4_epoch_1.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oatuIvhpP2yY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "read_results = text_read('./nll_aver_result_v4_epoch_1.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "R-U8Vn7bP9OP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "92fe7bb6-3a70-4756-917d-92dd0f6097c2"
      },
      "cell_type": "code",
      "source": [
        "len(read_results) #012"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "90"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "metadata": {
        "id": "_qecq972bm0y",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "af8618ad-4079-4268-9a3d-18c227adb582"
      },
      "cell_type": "code",
      "source": [
        "######## test set ########\n",
        "\n",
        "nll_aver_test_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for i in range(en_test_batches.shape[0]):\n",
        "      \n",
        "      test_feed_dict = {input_placeholder: en_test_batches[i], \n",
        "                        target_placeholder: fr_test_batches[i],\n",
        "                        in_length_placeholder: en_test_len_batches[i], \n",
        "                        out_length_placeholder: fr_test_len_batches[i]}\n",
        "      \n",
        "      nll_aver_test = sess.run(nll_average, feed_dict=test_feed_dict)\n",
        "      \n",
        "      nll_aver_test_results.append(nll_aver_test)\n",
        "    \n",
        "    print(np.mean(nll_aver_test_results))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_2/model_each_epch.ckpt\n",
            "76.7958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "CIAbQZtXcuof",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "fb09fa67-81eb-44aa-e1da-8c994a0b1167"
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt = 12\n",
        "        \n",
        "en_file = \"./small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"./small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "print(en_file)\n",
        "print(fr_file)\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "batch_len = en_input_batches.shape[0]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "./small_txt/12_en.txt\n",
            "./small_txt/12_fr.txt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "nHVgmOT6a-UY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "e533d448-a2bc-451b-9480-4a4615b93b76"
      },
      "cell_type": "code",
      "source": [
        "######## train set (the last txt) ########\n",
        "\n",
        "nll_aver_subtrain_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for i in range(batch_len):\n",
        " \n",
        "      train_feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i]}\n",
        "      \n",
        "      nll_aver_train = sess.run(nll_average, feed_dict=train_feed_dict)\n",
        "   \n",
        "      nll_aver_subtrain_results.append(nll_aver_train)\n",
        "    \n",
        "    print(np.mean(nll_aver_subtrain_results))\n",
        "      "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_2/model_each_epch.ckpt\n",
            "61.65899\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sb3phRzybDwl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translation Samples \n",
        "\n",
        "### Select the next target word with highest probability (Greedy Search)"
      ]
    },
    {
      "metadata": {
        "id": "HRxhfFccbBiF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 32
        },
        "outputId": "fe2cb404-0da1-482f-e065-89dfd472eda1"
      },
      "cell_type": "code",
      "source": [
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    fr_test_translated = np.copy(fr_test_batches)\n",
        "    \n",
        "    for k in range(6):      \n",
        "      for i in range(max_length):\n",
        "        prob_next_word = np.zeros((batch_size, fr_vocab_size))\n",
        "      \n",
        "        test_feed_dict = {input_placeholder: en_test_batches[k], \n",
        "                          target_placeholder: fr_test_translated[k],\n",
        "                          in_length_placeholder: en_test_len_batches[k], \n",
        "                          out_length_placeholder: fr_test_len_batches[k]}\n",
        "      \n",
        "      \n",
        "        prob_all_words = sess.run(prob_constant, feed_dict=test_feed_dict)\n",
        "      \n",
        "        for j in range(batch_size):\n",
        "          prob_next_word[j] = prob_all_words[i][j]/np.sum(prob_all_words[i][j])\n",
        "          fr_test_translated[k, j,i+1] = np.argmax(prob_next_word[j])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_2/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BAJ90CNObF8J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length+1):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "  return sens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eVrc43TSbIKt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98
        },
        "outputId": "64072e9d-ce92-4293-be2c-04cb68243b63"
      },
      "cell_type": "code",
      "source": [
        "total_scores = []\n",
        "for k in range(3):\n",
        "  for i in range(batch_size):  \n",
        "    gene_sen = id_to_word(fr_test_translated[k,i], fr_word_to_id,max_length)\n",
        "    orig_sen = id_to_word(fr_test_batches[k,i], fr_word_to_id,max_length)\n",
        "\n",
        "    score = sentence_bleu(orig_sen, gene_sen)  #(reference, candidates)\n",
        "    total_scores.append(score)\n",
        "    \n",
        "print(np.mean(total_scores))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "0.3134934988955799\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PzDOOXh6bhX5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 168
        },
        "outputId": "ba715d31-8207-4f0a-d773-8f2264921234"
      },
      "cell_type": "code",
      "source": [
        "id_sen = 9\n",
        "\n",
        "en_sens_str = \"\"\n",
        "\n",
        "en_sens = id_to_word(en_test_batches[0,id_sen], en_word_to_id, max_length-1)\n",
        "for p in range(len(en_sens)):\n",
        "        en_sens_str = en_sens_str + \" \" + en_sens[p]\n",
        "    \n",
        "print(en_sens_str)\n",
        "\n",
        "\n",
        "fr_sens_str = \"\"\n",
        "\n",
        "fr_sens = id_to_word(fr_test_batches[0,id_sen], fr_word_to_id, max_length)\n",
        "for p in range(len(fr_sens)):\n",
        "        fr_sens_str = fr_sens_str + \" \" + fr_sens[p]\n",
        "    \n",
        "print(fr_sens_str)\n",
        "\n",
        "tr_sens_str = \"\"\n",
        "\n",
        "tr_sens = id_to_word(fr_test_translated[0,id_sen], fr_word_to_id, max_length)\n",
        "for p in range(len(tr_sens)):\n",
        "        tr_sens_str = tr_sens_str + \" \" + tr_sens[p]\n",
        "    \n",
        "print(tr_sens_str)\n",
        "\n",
        "print(sentence_bleu(fr_sens_str, tr_sens_str))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " The Croatians continued in fine form after half @-@ time . eos <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            " <beg> Même après la pause , les Croates n&apos; ont rien laissé brûler . eos <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
            " <beg> Les Croates ont continué à la valeur de la durée de la période de longue période . eos mois de janvier , la Commission a un autre point de vue\n",
            "0.6127146389227656\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}