{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gene_translation_cnn_0812.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "lq37O5wR21AC",
        "BSBf8rYS9hX9",
        "gJjC0NuO9oE4"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rMcrrnEBsjy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the drive path"
      ]
    },
    {
      "metadata": {
        "id": "66FgEFRN-AMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWRjvljv-GT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vn79dig8uT8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a9150b16-8f06-4b76-d259-c16fc2c3eead"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mdatalab\u001b[0m/  \u001b[01;34mdrive\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MTeUgeB-seY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ce0fa746-34a9-46e8-b4fb-9fa200c83737"
      },
      "cell_type": "code",
      "source": [
        "cd drive/encoder_2_GDCNN_300_dropout_08"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/encoder_2_GDCNN_300_dropout_08\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B12HWS4xsx4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ]
    },
    {
      "metadata": {
        "id": "Ly79OANzc7RS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "842o8uvRtBnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "wjDy6-mfnnit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load vocab dict from txt file\n",
        "\n",
        "f = open(\"../dictionary 2/en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../dictionary 2/fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuETWPMbu2Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "9e5f4878-4867-486e-e240-121bcecfb268"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "en_eos = en_word_to_id['eos']\n",
        "fr_eos = fr_word_to_id['eos']\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30772\n",
            "39578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9ifxSnpu6zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output\n",
        "\n",
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list\n",
        "\n",
        "\n",
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y57-AuNCvDSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length=30)\n",
        "\n",
        "    #out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    #fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehsowT7hwyjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juNYK867gw3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_oov_id = en_word_to_id['<OOV>']\n",
        "fr_oov_id = fr_word_to_id['<OOV>']\n",
        "\n",
        "def dropout_func(decode_input, dropout_prob, oov_id):\n",
        "  for i in range(decode_input.shape[0]):\n",
        "    for j in range(decode_input.shape[1]):\n",
        "        for k in range(1,decode_input.shape[2]):\n",
        "            if np.random.uniform() > dropout_prob:\n",
        "                decode_input[i,j,k] = oov_id        \n",
        "  return decode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq37O5wR21AC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "zcHfesh3uCDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define parameters ######################\n",
        "\n",
        "max_length = 30\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "embed_size = 300\n",
        "\n",
        "infer_hidden_size = 800\n",
        "\n",
        "latent_size = 300\n",
        "\n",
        "latent_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FraHS_clcvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ###################### generate sentence #######################\n",
        "batch_size = 1\n",
        "\n",
        "latent_num = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJNVkwnhdg9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define placeholder ######################\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input')         # batch_size x max_length\n",
        "\n",
        "target_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target')       # batch_size x max_length\n",
        "\n",
        "in_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'in_len')              # batch_size x 1\n",
        "\n",
        "out_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'out_len')            # batch_size x 1\n",
        "\n",
        "discount_placeholder = tf.placeholder(tf.float32, name='discount')\n",
        "\n",
        "lr_placeholder = tf.placeholder(tf.float32, name='learn_rate')\n",
        "\n",
        "input_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input_drop')   # batch_size x max_length\n",
        "\n",
        "target_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target_drop') # batch_size x max_length\n",
        "\n",
        "if_gene_placeholder = tf.placeholder(tf.bool, name='if_gene')\n",
        "\n",
        "latent_var_placeholder = tf.placeholder(tf.float32, [latent_num, batch_size, max_length, latent_size], 'la_var')       # batch_size x max_length x latent_size\n",
        "\n",
        "xavier_initializer = tf.contrib.layers.xavier_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dqobGQGkHNg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### embedding look-up for input sentences ####################\n",
        "\n",
        "with tf.variable_scope('en_embedding'):\n",
        "    en_embedding = tf.get_variable('en_embeding',[en_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    inputs = tf.nn.embedding_lookup(en_embedding, input_placeholder)                      # batch_size x max_length x embed_size\n",
        "    inputs_drop = tf.nn.embedding_lookup(en_embedding, input_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    \n",
        "\n",
        "with tf.variable_scope('fr_embedding'):\n",
        "    fr_embedding = tf.get_variable('fr_embeding',[fr_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    targets = tf.nn.embedding_lookup(fr_embedding, target_placeholder)                      # batch_size x max_length x embed_size\n",
        "    targets_drop = tf.nn.embedding_lookup(fr_embedding, target_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmvdTgj5tSpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Inference Model - Encoder\n",
        "\n",
        "$q(z_1, z_2, ... , z_T|x,y)$\n",
        "\n",
        "Similar to the encoder of RNNSearch"
      ]
    },
    {
      "metadata": {
        "id": "DSPN85dailPC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### Inference model  #######################\n",
        "\n",
        "encode_inputs = tf.transpose(tf.concat([inputs, targets], axis=2), (1,0,2))\n",
        "\n",
        "with tf.variable_scope('encode'):\n",
        "    #basic_cell =tf.contrib.rnn.GRUCell(infer_hidden_size)\n",
        "    basic_cell = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state = basic_cell.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs, encode_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell, \n",
        "                                                                   cell_bw=basic_cell, \n",
        "                                                                   inputs=encode_inputs,                                                                \n",
        "                                                                   initial_state_fw=init_state,\n",
        "                                                                   initial_state_bw=init_state,\n",
        "                                                                   dtype=tf.float32,\n",
        "                                                                   time_major=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo0Ycpq9invD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 143
        },
        "outputId": "a2cc0b82-8161-4dca-e5d7-3e709df2d981"
      },
      "cell_type": "code",
      "source": [
        "##################### Inference model  #######################\n",
        "\n",
        "##################### bi-direction lstm of source sentence ######################\n",
        "\n",
        "encode_inputs_x = tf.transpose(inputs, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "with tf.variable_scope('encode_x'):\n",
        "    basic_cell_x = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state_x = basic_cell_x.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs_x, encode_state_x = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_x, \n",
        "                                                                       cell_bw=basic_cell_x, \n",
        "                                                                       inputs=encode_inputs_x,\n",
        "                                                                       sequence_length=in_length_placeholder,\n",
        "                                                                       initial_state_fw=init_state_x,\n",
        "                                                                       initial_state_bw=init_state_x,\n",
        "                                                                       dtype=tf.float32,\n",
        "                                                                       time_major=True)\n",
        "#### encode_outputs_x: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##################### bi-direction lstm of target sentence ######################\n",
        "\n",
        "encode_inputs_y = tf.transpose(targets, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "with tf.variable_scope('encode_y'):\n",
        "    basic_cell_y = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state_y = basic_cell_y.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs_y, encode_state_y = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_y, \n",
        "                                                                       cell_bw=basic_cell_y, \n",
        "                                                                       inputs=encode_inputs_y,\n",
        "                                                                       sequence_length=out_length_placeholder,\n",
        "                                                                       initial_state_fw=init_state_y,\n",
        "                                                                       initial_state_bw=init_state_y,\n",
        "                                                                       dtype=tf.float32,\n",
        "                                                                       time_major=True)\n",
        "#### encode_outputs_y: max_length x batch_size x infer_hidden_size"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn.py:430: calling reverse_sequence (from tensorflow.python.ops.array_ops) with seq_dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "seq_dim is deprecated, use seq_axis instead\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/util/deprecation.py:454: calling reverse_sequence (from tensorflow.python.ops.array_ops) with batch_dim is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "batch_dim is deprecated, use batch_axis instead\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RYfWtHL8irsr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "#en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size)) # batch_size*max_length x 2*infer_hidden_size\n",
        "\n",
        "##################### concatenate the state of encoder of x and y ######################\n",
        "\n",
        "fw_bw_en_state_x = tf.concat((encode_state_x[0][1],encode_state_x[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "fw_bw_en_state_y = tf.concat((encode_state_y[0][1],encode_state_y[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "fw_bw_en_state = tf.concat((fw_bw_en_state_x, fw_bw_en_state_y), 1)             # en_max_length x  4*infer_hidden_size\n",
        "\n",
        "fw_bw_en_state = tf.tile(tf.expand_dims(fw_bw_en_state, axis=1), (1,30,1))\n",
        "\n",
        "\n",
        "fw_bw_en = tf.concat((fw_bw_en_state, en_outputs_tran), axis=2)\n",
        "\n",
        "fw_bw_en = tf.reshape(fw_bw_en, (batch_size*max_length, 6*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Yq7w0u-iyl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('encode_projection'):\n",
        "    W_1 = tf.get_variable('W_1',[6*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_1 = tf.get_variable('b_1',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_2 = tf.get_variable('W_2',[6*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_2 = tf.get_variable('b_2',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gz4FQdg_i4b8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fw_bw_en_outputs_norm = tf.contrib.layers.batch_norm(fw_bw_en_outputs_resh, center=True, scale=True)\n",
        "\n",
        "la_mean = tf.matmul(fw_bw_en, W_1) + b_1                              # batch_size*max_length x latent_size \n",
        "\n",
        "la_log_var = tf.matmul(fw_bw_en, W_2) + b_2                           # batch_size*max_length x latent_size \n",
        "la_var = tf.exp(la_log_var)\n",
        "la_std = tf.sqrt(la_var)\n",
        "\n",
        "kl_div_loss = 1 + la_log_var - tf.square(la_mean) - la_var                               # batch_size*max_length x latent_size\n",
        "kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, axis=1)                         # batch_size*max_length x 1\n",
        "kl_div_loss = tf.reshape(kl_div_loss, (batch_size, max_length))                 # batch_size x max_length\n",
        "kl_div_loss = tf.reduce_sum(kl_div_loss, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2-aUlz0i7Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# latent_variables_v = []\n",
        "# for _ in range(latent_num):\n",
        "#   eposida = tf.random_normal(tf.shape(la_std), mean=0.0,stddev=1)\n",
        "#   latent_variables_sample = la_mean + la_std*eposida\n",
        "#   latent_variables_sample = tf.reshape(latent_variables_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "#   latent_variables_v.append(latent_variables_sample)\n",
        "\n",
        "# def if_true():\n",
        "#   latent_v = []\n",
        "#   for h in range(latent_num):\n",
        "#     latent_v.append(latent_var_placeholder[h])\n",
        "#   return latent_v\n",
        "\n",
        "# def if_false():\n",
        "#   return latent_variables_v\n",
        "\n",
        "# latent_variables = tf.cond(if_gene_placeholder, if_true, if_false)\n",
        "\n",
        "# if latent_num == 1:\n",
        "#   new_latent_variables = []\n",
        "#   new_latent_variables.append(latent_variables)\n",
        "#   latent_variables = new_latent_variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDVWXdubzXh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_v = []\n",
        "for h in range(latent_num):\n",
        "  latent_v.append(latent_var_placeholder[h])\n",
        "    \n",
        "latent_variables = latent_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on6adzC8518v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Generation Model - Decoder\n",
        "\n",
        "$p_\\theta(x|z_1, z_2, ... , z_T)$\n",
        "\n",
        "$p_\\theta(y|z_1, z_2, ... , z_T)$"
      ]
    },
    {
      "metadata": {
        "id": "tiEDu_jlXEtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filter_num = 500\n",
        "\n",
        "filter_size = 3\n",
        "\n",
        "filter_size_only_pre = 2\n",
        "\n",
        "filter_size_pad = filter_size - filter_size_only_pre\n",
        "\n",
        "filter_zero_pad = tf.zeros(shape=[filter_size_pad, embed_size+latent_size, filter_num], dtype=tf.float32)\n",
        "filter_zero_pad_2 = tf.zeros(shape=[1, filter_size_pad, filter_num, filter_num], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSBf8rYS9hX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Generation Model for source sentence $p_\\theta(x|z_1, z_2, ... , z_T)$\n"
      ]
    },
    {
      "metadata": {
        "id": "UNQlel3UksZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### concat beg token with input\n",
        "\n",
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "#x_input_cnn_1 = tf.concat([latent_variables_1,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "#x_input_cnn_2 = tf.concat([latent_variables_2,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "#x_input_cnn_4D = tf.expand_dims(x_input_cnn, axis=1)                                        # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "x_input_cnn = []\n",
        "for l in range(latent_num):\n",
        "  x_input_cnn.append(tf.concat([latent_variables[l],x_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4x0f5AYajCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "    f_x_1 = tf.get_variable(\"x_filter_1\", shape=[filter_size_only_pre, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_1_dia = tf.concat([f_x_1, filter_zero_pad], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_2[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_2[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((4,filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_3[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_3[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((6,filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_4[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_4[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((10,filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "    \n",
        "    f_x_5 = tf.get_variable(\"x_filter_5\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_5_dia = tf.concat([tf.reshape(f_x_5[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_5[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_5[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((18,filter_num,filter_num))], axis=0)\n",
        "    # 29 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_x'):\n",
        "    proj_w_x = tf.get_variable('project_w_x', [int(filter_num/2),embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### sequence weight of x\n",
        "squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQIFBsJnkJWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder(de_input):\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  x_out_gated_conv = x_out_conv_dia[:,:250]*tf.nn.sigmoid(x_out_conv_dia[:,250:])\n",
        "  \n",
        "  x_out_project = tf.matmul(x_out_gated_conv, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "  target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "  logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "  prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_x\n",
        "\n",
        "log_liki_x_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_x_to.append(x_decoder(x_input_cnn[l]))\n",
        "log_liki_x_to = tf.stack(log_liki_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qX8_07vbQoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "    \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  x_out_gated_conv = x_out_conv_dia[:,:250]*tf.nn.sigmoid(x_out_conv_dia[:,250:])\n",
        "  \n",
        "  x_out_project = tf.matmul(x_out_gated_conv, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0))) \n",
        "  return logits_x\n",
        "\n",
        "logits_gene_x_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l], latent_variables[l]))\n",
        "logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4gCPfEbETNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJjC0NuO9oE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Generation Model for target sentence $p_\\theta(y|z_1, z_2, ... , z_T)$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-H6P3ob3kPlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### concat beg token with target\n",
        "\n",
        "#beg_token_y = tf.zeros((1,embed_size))\n",
        "beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "y_input_cnn = []\n",
        "for l in range(latent_num):\n",
        "  y_input_cnn.append(tf.concat([latent_variables[l],y_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW1zf5TWPQnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "    f_y_1 = tf.get_variable(\"y_filter_1\", shape=[filter_size_only_pre, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_1_dia = tf.concat([f_y_1, filter_zero_pad], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_2[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_2[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((4,filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_3[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((2,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_3[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((6,filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_4[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((4,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_4[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((10,filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "    \n",
        "    f_y_5 = tf.get_variable(\"y_filter_5\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_5_dia = tf.concat([tf.reshape(f_y_5[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_5[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((8,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_5[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((18,filter_num,filter_num))], axis=0)\n",
        "    # 37 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_y'):\n",
        "    proj_w_y = tf.get_variable('project_w_y', [int(filter_num/2),embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "#### sequence weight of y\n",
        "squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpRsIIdCj5so",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder(de_input):\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(de_input, f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(y_out_conv_dia_1, f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(y_out_conv_dia_2, f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(y_out_conv_dia_3, f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_5 = tf.nn.conv1d(y_out_conv_dia_4, f_y_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "\n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "  y_out_gated_conv = y_out_conv_dia[:,:250]*tf.nn.sigmoid(y_out_conv_dia[:,250:])\n",
        "  \n",
        "  y_out_project = tf.matmul(y_out_gated_conv, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "\n",
        "  target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "  logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "  prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_y\n",
        "\n",
        "log_liki_y_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_y_to.append(y_decoder(y_input_cnn[l]))\n",
        "log_liki_y_to = tf.stack(log_liki_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jb2g_kS1cKxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The lower bound of log-joint-likelihood, to maximize"
      ]
    },
    {
      "metadata": {
        "id": "5wHFYwB9jkDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nega_log_liki_x_y = 0\n",
        "\n",
        "nega_elbo = 0\n",
        "\n",
        "for l in range(latent_num):\n",
        "  nega_log_liki_x_y = nega_log_liki_x_y + tf.reduce_mean(- log_liki_x_to[l] - log_liki_y_to[l])\n",
        "  nega_elbo = nega_elbo - log_liki_x_to[l] - log_liki_y_to[l]\n",
        "  \n",
        "nega_log_liki_x_y = nega_log_liki_x_y/latent_num\n",
        "nega_elbo = nega_elbo/latent_num + discount_placeholder*kl_div_loss\n",
        "objective = tf.reduce_mean(nega_elbo) \n",
        "kl_div_loss_batch_mean = tf.reduce_mean(kl_div_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z8P6-XM38MV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# L2 reguralization for trainable variables\n",
        "#train_variables = tf.trainable_variables()\n",
        "#regularization_cost = tf.reduce_sum([tf.nn.l2_loss(variable) for variable in train_variables])\n",
        "#regular_rate = 0.00001\n",
        "#+ regular_rate*regularization_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwMB6m32Yzfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer(lr_placeholder)\n",
        "\n",
        "gvs, var = zip(*optimizer.compute_gradients(objective))\n",
        "\n",
        "#checked_gvs = [tf.where(tf.is_nan(grad), tf.zeros_like(grad), grad) for grad in gvs]\n",
        "\n",
        "cliped_gvs, _ = tf.clip_by_global_norm(gvs, 1)\n",
        "\n",
        "opt = optimizer.apply_gradients(zip(cliped_gvs, var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4Glrs-GxHeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "#gvs = optimizer.compute_gradients(objective)\n",
        "#capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
        "#opt = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASMl3Fgnsyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### save the model\n",
        "def save_model(session, path):\n",
        "    if not os.path.exists(\"./result_0812_encoder_2_GDCNN_300_dropout_08/\"):\n",
        "        os.mkdir('./result_0812_encoder_2_GDCNN_300_dropout_08/')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(session, path)\n",
        "\n",
        "path1 = './result_0812_encoder_2_GDCNN_300_dropout_08/model_each_epch.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E546I60IPRWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "Bxq1rcyggfA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "\n",
        "\n",
        "def text_save(content,filename,mode='a'):\n",
        "    # Try to save a list variable in txt file.\n",
        "    file = open(filename,mode)\n",
        "    for i in range(len(content)):\n",
        "        file.write(str(content[i])+'\\n')\n",
        "    file.close()\n",
        "    \n",
        "def text_read(filename):\n",
        "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
        "    try:\n",
        "        file = open(filename,'r')\n",
        "    except IOError:\n",
        "        error = []\n",
        "        return error\n",
        "    content = file.readlines()\n",
        " \n",
        "    for i in range(len(content)):\n",
        "        content[i] = content[i][:len(content[i])-1]\n",
        " \n",
        "    file.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RovgeCM_9-i3",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5083
        },
        "outputId": "ec54ae18-cd1d-486c-9304-5cdd31551314"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 3\n",
        "total_step = 0\n",
        "learning_rate = 0.001\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "        fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            discount_rate = 0.0002*total_step\n",
        "            if discount_rate >1:\n",
        "              discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "        \n",
        "text_save(elbo_results, './result_0810/elbo_results.txt')\n",
        "text_save(kl_results, './result_0810/kl_results.txt')\n",
        "text_save(likei_results, './result_0810/likei_results.txt')"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/0_en.txt\n",
            "../small_txt/0_fr.txt\n",
            "23.04999\n",
            "475.55557\n",
            "475.55557\n",
            "0.0\n",
            "232.01318\n",
            "243.54243\n",
            "30.47138\n",
            "288.45398\n",
            "289.0634\n",
            "0.02\n",
            "145.79733\n",
            "142.65662\n",
            "101.986404\n",
            "232.82616\n",
            "236.90558\n",
            "0.04\n",
            "119.45289\n",
            "113.37326\n",
            "205.23076\n",
            "188.69429\n",
            "201.00815\n",
            "0.060000000000000005\n",
            "94.1749\n",
            "94.51939\n",
            "186.97002\n",
            "174.90686\n",
            "189.86446\n",
            "0.08\n",
            "91.80454\n",
            "83.1023\n",
            "174.18504\n",
            "169.22072\n",
            "186.63922\n",
            "0.1\n",
            "92.7825\n",
            "76.438225\n",
            "147.15439\n",
            "144.3828\n",
            "162.04134\n",
            "0.12000000000000001\n",
            "76.77362\n",
            "67.609184\n",
            "141.6337\n",
            "133.00255\n",
            "152.83125\n",
            "0.14\n",
            "68.36474\n",
            "64.637825\n",
            "146.50801\n",
            "127.526505\n",
            "150.96779\n",
            "0.16\n",
            "65.88376\n",
            "61.642746\n",
            "144.15602\n",
            "128.3748\n",
            "154.3229\n",
            "0.18000000000000002\n",
            "64.16492\n",
            "64.2099\n",
            "144.20518\n",
            "136.93567\n",
            "165.7767\n",
            "0.2\n",
            "67.03601\n",
            "69.89965\n",
            "128.1747\n",
            "116.973946\n",
            "145.1724\n",
            "0.22\n",
            "58.95244\n",
            "58.021515\n",
            "136.19878\n",
            "132.82542\n",
            "165.51312\n",
            "0.24000000000000002\n",
            "67.761284\n",
            "65.06415\n",
            "136.57065\n",
            "131.0238\n",
            "166.53217\n",
            "0.26\n",
            "64.98239\n",
            "66.04141\n",
            "123.47973\n",
            "120.97024\n",
            "155.54456\n",
            "0.28\n",
            "59.47936\n",
            "61.49088\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/1_en.txt\n",
            "../small_txt/1_fr.txt\n",
            "128.32901\n",
            "125.53065\n",
            "164.00368\n",
            "0.2998\n",
            "62.748257\n",
            "62.7824\n",
            "130.42589\n",
            "128.29814\n",
            "170.00835\n",
            "0.31980000000000003\n",
            "63.72522\n",
            "64.57292\n",
            "125.22261\n",
            "123.81424\n",
            "166.36488\n",
            "0.3398\n",
            "61.850204\n",
            "61.964027\n",
            "127.30077\n",
            "125.675095\n",
            "171.4779\n",
            "0.3598\n",
            "60.260292\n",
            "65.414795\n",
            "119.53091\n",
            "117.754364\n",
            "163.1522\n",
            "0.3798\n",
            "57.111134\n",
            "60.643238\n",
            "115.73689\n",
            "114.184235\n",
            "160.45583\n",
            "0.39980000000000004\n",
            "54.749428\n",
            "59.4348\n",
            "123.27647\n",
            "127.578575\n",
            "179.33002\n",
            "0.4198\n",
            "61.68132\n",
            "65.89725\n",
            "116.72611\n",
            "116.6624\n",
            "167.99855\n",
            "0.4398\n",
            "58.087627\n",
            "58.57478\n",
            "105.30382\n",
            "112.626076\n",
            "161.04477\n",
            "0.45980000000000004\n",
            "55.83472\n",
            "56.79135\n",
            "111.60804\n",
            "121.143135\n",
            "174.69266\n",
            "0.4798\n",
            "58.790752\n",
            "62.35238\n",
            "114.75252\n",
            "127.085686\n",
            "184.43901\n",
            "0.4998\n",
            "64.313866\n",
            "62.77183\n",
            "101.48955\n",
            "113.50906\n",
            "166.26332\n",
            "0.5198\n",
            "57.114063\n",
            "56.395004\n",
            "100.54055\n",
            "121.27006\n",
            "175.54184\n",
            "0.5398000000000001\n",
            "61.23136\n",
            "60.038696\n",
            "102.08272\n",
            "126.66057\n",
            "183.80646\n",
            "0.5598000000000001\n",
            "63.010254\n",
            "63.650307\n",
            "96.854744\n",
            "118.426125\n",
            "174.58253\n",
            "0.5798\n",
            "58.940582\n",
            "59.485542\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/2_en.txt\n",
            "../small_txt/2_fr.txt\n",
            "98.84239\n",
            "133.38545\n",
            "192.65132\n",
            "0.5996\n",
            "67.9959\n",
            "65.389534\n",
            "99.908516\n",
            "116.931175\n",
            "178.83449\n",
            "0.6196\n",
            "60.340378\n",
            "56.59081\n",
            "99.39494\n",
            "114.131874\n",
            "177.70488\n",
            "0.6396000000000001\n",
            "57.37292\n",
            "56.758957\n",
            "100.55085\n",
            "124.53949\n",
            "190.86281\n",
            "0.6596000000000001\n",
            "63.25825\n",
            "61.281235\n",
            "91.2115\n",
            "114.113525\n",
            "176.10088\n",
            "0.6796\n",
            "57.26925\n",
            "56.844284\n",
            "90.9042\n",
            "117.011986\n",
            "180.60855\n",
            "0.6996\n",
            "60.98671\n",
            "56.025272\n",
            "94.91686\n",
            "126.20003\n",
            "194.50221\n",
            "0.7196\n",
            "62.417816\n",
            "63.78221\n",
            "87.233116\n",
            "127.61335\n",
            "192.13095\n",
            "0.7396\n",
            "64.88184\n",
            "62.73151\n",
            "83.996864\n",
            "131.88391\n",
            "195.68793\n",
            "0.7596\n",
            "66.747955\n",
            "65.135956\n",
            "84.03656\n",
            "141.44426\n",
            "206.95917\n",
            "0.7796000000000001\n",
            "72.35025\n",
            "69.094\n",
            "76.78527\n",
            "133.58792\n",
            "194.9854\n",
            "0.7996000000000001\n",
            "67.82082\n",
            "65.76709\n",
            "72.384674\n",
            "134.84097\n",
            "194.16743\n",
            "0.8196\n",
            "68.68754\n",
            "66.153435\n",
            "64.727585\n",
            "136.57388\n",
            "190.91917\n",
            "0.8396\n",
            "69.75829\n",
            "66.8156\n",
            "60.353912\n",
            "138.89873\n",
            "190.77896\n",
            "0.8596\n",
            "71.2216\n",
            "67.677124\n",
            "58.703384\n",
            "153.98038\n",
            "205.6159\n",
            "0.8796\n",
            "77.62701\n",
            "76.35338\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TCryB1PjBEYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5830
        },
        "outputId": "99a98d0b-8f62-4b7f-82aa-b46b93559740"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "total_step=0\n",
        "learning_rate = 0.001\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "        fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            #discount_rate = 0.0002*total_step\n",
        "            #if discount_rate >1:\n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "\n",
        "text_save(elbo_results, './result_0812_encoder_2_GDCNN_300_dropout_08/elbo_results.txt')\n",
        "text_save(kl_results, './result_0812_encoder_2_GDCNN_300_dropout_08/kl_results.txt')\n",
        "text_save(likei_results, './result_0812_encoder_2_GDCNN_300_dropout_08/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0812_encoder_2_GDCNN_300_dropout_08/model_each_epch.ckpt\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/0_en.txt\n",
            "../small_txt/0_fr.txt\n",
            "16.517498\n",
            "166.39432\n",
            "182.91182\n",
            "1\n",
            "84.50939\n",
            "81.88491\n",
            "17.565102\n",
            "164.00378\n",
            "181.56891\n",
            "1\n",
            "82.83785\n",
            "81.165924\n",
            "17.749779\n",
            "177.16579\n",
            "194.91554\n",
            "1\n",
            "88.48266\n",
            "88.683105\n",
            "17.76081\n",
            "170.61005\n",
            "188.37083\n",
            "1\n",
            "83.95923\n",
            "86.65081\n",
            "17.93645\n",
            "169.8761\n",
            "187.81255\n",
            "1\n",
            "85.93489\n",
            "83.941185\n",
            "18.178839\n",
            "172.33752\n",
            "190.51634\n",
            "1\n",
            "86.76008\n",
            "85.57745\n",
            "17.77395\n",
            "162.01727\n",
            "179.79123\n",
            "1\n",
            "81.90662\n",
            "80.110634\n",
            "17.110699\n",
            "161.94246\n",
            "179.05316\n",
            "1\n",
            "81.549164\n",
            "80.39331\n",
            "16.714462\n",
            "163.16846\n",
            "179.8829\n",
            "1\n",
            "82.87703\n",
            "80.29141\n",
            "17.251467\n",
            "164.88219\n",
            "182.13365\n",
            "1\n",
            "81.397285\n",
            "83.48491\n",
            "16.978806\n",
            "168.83392\n",
            "185.81273\n",
            "1\n",
            "85.70664\n",
            "83.12729\n",
            "16.283514\n",
            "151.64848\n",
            "167.93199\n",
            "1\n",
            "76.8197\n",
            "74.82878\n",
            "17.88399\n",
            "163.25494\n",
            "181.13895\n",
            "1\n",
            "80.91819\n",
            "82.33677\n",
            "17.334345\n",
            "170.90399\n",
            "188.23833\n",
            "1\n",
            "87.14083\n",
            "83.76315\n",
            "16.279818\n",
            "158.15\n",
            "174.42981\n",
            "1\n",
            "78.52012\n",
            "79.62987\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/1_en.txt\n",
            "../small_txt/1_fr.txt\n",
            "17.192965\n",
            "168.35347\n",
            "185.54646\n",
            "1\n",
            "87.92864\n",
            "80.424835\n",
            "17.59557\n",
            "170.00435\n",
            "187.5999\n",
            "1\n",
            "85.46292\n",
            "84.54143\n",
            "17.150545\n",
            "163.1967\n",
            "180.34724\n",
            "1\n",
            "83.10298\n",
            "80.09371\n",
            "17.72184\n",
            "167.71535\n",
            "185.4372\n",
            "1\n",
            "84.77416\n",
            "82.94119\n",
            "16.571564\n",
            "165.04066\n",
            "181.61224\n",
            "1\n",
            "83.73131\n",
            "81.30936\n",
            "16.411268\n",
            "158.523\n",
            "174.93427\n",
            "1\n",
            "79.20697\n",
            "79.31602\n",
            "17.892214\n",
            "177.18472\n",
            "195.07695\n",
            "1\n",
            "88.55047\n",
            "88.63425\n",
            "17.389717\n",
            "166.7051\n",
            "184.09482\n",
            "1\n",
            "85.01562\n",
            "81.689476\n",
            "15.65083\n",
            "160.35114\n",
            "176.00195\n",
            "1\n",
            "81.085175\n",
            "79.26597\n",
            "16.631414\n",
            "166.59256\n",
            "183.22397\n",
            "1\n",
            "82.93495\n",
            "83.657616\n",
            "18.47973\n",
            "176.65765\n",
            "195.13739\n",
            "1\n",
            "89.03069\n",
            "87.62696\n",
            "16.669128\n",
            "156.74603\n",
            "173.41516\n",
            "1\n",
            "81.96744\n",
            "74.778595\n",
            "16.420015\n",
            "163.8176\n",
            "180.23761\n",
            "1\n",
            "83.126076\n",
            "80.69152\n",
            "17.20367\n",
            "169.2783\n",
            "186.48195\n",
            "1\n",
            "84.69706\n",
            "84.58125\n",
            "16.522089\n",
            "160.03503\n",
            "176.55713\n",
            "1\n",
            "80.29485\n",
            "79.74017\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/2_en.txt\n",
            "../small_txt/2_fr.txt\n",
            "17.941216\n",
            "177.96936\n",
            "195.91057\n",
            "1\n",
            "90.911514\n",
            "87.05785\n",
            "17.427294\n",
            "163.31781\n",
            "180.7451\n",
            "1\n",
            "83.16049\n",
            "80.15732\n",
            "16.62202\n",
            "166.00258\n",
            "182.6246\n",
            "1\n",
            "83.81328\n",
            "82.1893\n",
            "17.8243\n",
            "169.12807\n",
            "186.95236\n",
            "1\n",
            "85.064095\n",
            "84.063965\n",
            "16.87727\n",
            "161.04301\n",
            "177.92026\n",
            "1\n",
            "80.63938\n",
            "80.4036\n",
            "17.273846\n",
            "162.55663\n",
            "179.83047\n",
            "1\n",
            "82.062775\n",
            "80.493866\n",
            "17.352348\n",
            "173.9016\n",
            "191.25395\n",
            "1\n",
            "87.5119\n",
            "86.389694\n",
            "17.2654\n",
            "169.89803\n",
            "187.16344\n",
            "1\n",
            "84.69884\n",
            "85.199196\n",
            "17.894075\n",
            "171.37422\n",
            "189.2683\n",
            "1\n",
            "88.47248\n",
            "82.901726\n",
            "18.154074\n",
            "181.01176\n",
            "199.16582\n",
            "1\n",
            "92.68467\n",
            "88.32706\n",
            "16.739374\n",
            "173.41713\n",
            "190.15651\n",
            "1\n",
            "85.8164\n",
            "87.60072\n",
            "16.726082\n",
            "167.00053\n",
            "183.72662\n",
            "1\n",
            "83.78835\n",
            "83.21218\n",
            "17.015242\n",
            "165.93529\n",
            "182.95055\n",
            "1\n",
            "81.24188\n",
            "84.693405\n",
            "16.623045\n",
            "161.04541\n",
            "177.66846\n",
            "1\n",
            "82.44291\n",
            "78.60251\n",
            "17.735994\n",
            "177.21884\n",
            "194.95482\n",
            "1\n",
            "89.588005\n",
            "87.63084\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 3\n",
            "../small_txt/3_en.txt\n",
            "../small_txt/3_fr.txt\n",
            "19.493805\n",
            "187.17969\n",
            "206.6735\n",
            "1\n",
            "87.26287\n",
            "99.916824\n",
            "18.472368\n",
            "158.37292\n",
            "176.84528\n",
            "1\n",
            "81.05507\n",
            "77.31783\n",
            "18.43073\n",
            "173.39536\n",
            "191.82608\n",
            "1\n",
            "88.35869\n",
            "85.03667\n",
            "17.471146\n",
            "165.00058\n",
            "182.47173\n",
            "1\n",
            "83.88536\n",
            "81.115204\n",
            "17.451797\n",
            "169.2044\n",
            "186.6562\n",
            "1\n",
            "84.144066\n",
            "85.06035\n",
            "18.425175\n",
            "170.42476\n",
            "188.84995\n",
            "1\n",
            "86.61543\n",
            "83.80936\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DqH5fygZc0yl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load test data and test the model"
      ]
    },
    {
      "metadata": {
        "id": "T3EsytHdPkpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 1"
      ]
    },
    {
      "metadata": {
        "id": "Is72zrRCKFOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "1b9f7ca6-bff2-44bc-d356-55e1ddac4058"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "ind_small_txt = 12\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "print(en_file)\n",
        "print(fr_file)\n",
        "        \n",
        "en_test, en_test_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_test, fr_test_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_test_batches, en_test_len_batches = batch_producer(en_test, en_test_len, batch_size) \n",
        "fr_test_batches, fr_test_len_batches = batch_producer(fr_test, fr_test_len, batch_size)\n",
        "                       \n",
        "batch_len = en_test_batches.shape[0]\n",
        "        \n",
        "\n",
        "######## test set ########\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    ########### training ###########\n",
        "    for i in range(en_test_batches.shape[0]):\n",
        "          \n",
        "        discount_rate = 1\n",
        "        \n",
        "        test_feed_dict = {input_placeholder: en_test_batches[i], \n",
        "                          target_placeholder: fr_test_batches[i],\n",
        "                          in_length_placeholder: en_test_len_batches[i], \n",
        "                          out_length_placeholder: fr_test_len_batches[i],\n",
        "                          discount_placeholder: discount_rate,\n",
        "                          if_gene_placeholder: False,\n",
        "                          latent_var_placeholder: zero_latent}\n",
        "             \n",
        "        kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=test_feed_dict)       \n",
        "                                   \n",
        "        kl_test_2.append(kl)\n",
        "        nage_likeli_test_2.append(nage_likeli)\n",
        "        objecti_test_2.append(objecti)\n",
        "        llx_test_2.append(llx)\n",
        "        lly_test_2.append(lly)\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../small_txt/12_en.txt\n",
            "../small_txt/12_fr.txt\n",
            "INFO:tensorflow:Restoring parameters from ./result_0812_encoder_2_GDCNN_300_dropout_08/model_each_epch.ckpt\n",
            "14.511001\n",
            "202.64314\n",
            "217.15414\n",
            "-102.27634\n",
            "-100.36681\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r7mH1flzQMlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 2"
      ]
    },
    {
      "metadata": {
        "id": "BAKb79bddTJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "d0ef108e-e7ab-4476-c6b2-39885464c989"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "ind_small_txt = 11\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "print(en_file)\n",
        "print(fr_file)\n",
        "        \n",
        "en_test, en_test_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_test, fr_test_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_test_batches, en_test_len_batches = batch_producer(en_test, en_test_len, batch_size) \n",
        "fr_test_batches, fr_test_len_batches = batch_producer(fr_test, fr_test_len, batch_size)\n",
        "                       \n",
        "batch_len = en_test_batches.shape[0]\n",
        "        \n",
        "\n",
        "######## test set ########\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    ########### training ###########\n",
        "    for i in range(en_test_batches.shape[0]):\n",
        "          \n",
        "        discount_rate = 1\n",
        "        \n",
        "        test_feed_dict = {input_placeholder: en_test_batches[i], \n",
        "                          target_placeholder: fr_test_batches[i],\n",
        "                          in_length_placeholder: en_test_len_batches[i], \n",
        "                          out_length_placeholder: fr_test_len_batches[i],\n",
        "                          discount_placeholder: discount_rate,\n",
        "                          if_gene_placeholder: False,\n",
        "                          latent_var_placeholder: zero_latent}\n",
        "             \n",
        "        kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=test_feed_dict)       \n",
        "                                   \n",
        "        kl_test_2.append(kl)\n",
        "        nage_likeli_test_2.append(nage_likeli)\n",
        "        objecti_test_2.append(objecti)\n",
        "        llx_test_2.append(llx)\n",
        "        lly_test_2.append(lly)\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "../small_txt/11_en.txt\n",
            "../small_txt/11_fr.txt\n",
            "INFO:tensorflow:Restoring parameters from ./result_0812_encoder_2_GDCNN_300_dropout_08/model_each_epch.ckpt\n",
            "14.691068\n",
            "203.7571\n",
            "218.44814\n",
            "-103.01279\n",
            "-100.744286\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n31e9XAOQ5hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sub-Train Set"
      ]
    },
    {
      "metadata": {
        "id": "uE7gOKrYmJMz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "outputId": "966e580c-10bd-475c-ba33-fda50d791087"
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  5\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "\n",
        "######## test set ########\n",
        "kl_test_3 = []\n",
        "nage_likeli_test_3 = []\n",
        "objecti_test_3 = []\n",
        "llx_test_3 = []\n",
        "lly_test_3 = []\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    print(en_file)\n",
        "    print(fr_file)\n",
        "    \n",
        "    ########### training ###########\n",
        "    for i in range(500):\n",
        "          \n",
        "        discount_rate = 1\n",
        "        \n",
        "        feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                     target_placeholder: fr_output_batches[i],\n",
        "                     in_length_placeholder: en_input_len_batches[i], \n",
        "                     out_length_placeholder: fr_output_len_batches[i],\n",
        "                     discount_placeholder: discount_rate,\n",
        "                     if_gene_placeholder: False,\n",
        "                     latent_var_placeholder: zero_latent}\n",
        "       \n",
        "        kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "                                   \n",
        "        kl_test_3.append(kl)\n",
        "        nage_likeli_test_3.append(nage_likeli)\n",
        "        objecti_test_3.append(objecti)\n",
        "        llx_test_3.append(llx)\n",
        "        lly_test_3.append(lly)\n",
        "\n",
        "print(np.mean(kl_test_3))\n",
        "print(np.mean(nage_likeli_test_3))\n",
        "print(np.mean(objecti_test_3))\n",
        "print(np.mean(llx_test_3))\n",
        "print(np.mean(lly_test_3))"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0812_encoder_2_GDCNN_300_dropout_08/model_each_epch.ckpt\n",
            "../small_txt/5_en.txt\n",
            "../small_txt/5_fr.txt\n",
            "15.922356\n",
            "213.30219\n",
            "229.22456\n",
            "-104.58628\n",
            "-108.71591\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qYw-PZxCWtMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numerical Results"
      ]
    },
    {
      "metadata": {
        "id": "YO6RbSRWWsxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "c631f219-2330-4082-b7e3-41ed434348b7"
      },
      "cell_type": "code",
      "source": [
        "elbo_read = text_read('./result_0812_encoder_2_GDCNN_300_dropout_08/elbo_results.txt')\n",
        "elbo_read = [float(elbo) for elbo in elbo_read]\n",
        "\n",
        "kl_read = text_read('./result_0812_encoder_2_GDCNN_300_dropout_08/kl_results.txt')\n",
        "kl_read = [float(kl) for kl in kl_read]\n",
        "\n",
        "likei_read = text_read('./result_0812_encoder_2_GDCNN_300_dropout_08/likei_results.txt')\n",
        "likei_read = [float(likei) for likei in likei_read]\n",
        "\n",
        "plt.plot(kl_read, color = 'C0')\n",
        "plt.plot(likei_read, color = 'C1')\n",
        "plt.plot(elbo_read, color = 'C2')\n",
        "plt.legend(['kl divergence','nage_log_like_x_n_y', 'nage_elbo'], fontsize=12)\n",
        "plt.title(\"Encoder_2_GDCNN_300_dropout_0.8\", fontsize=16)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Encoder_2_GDCNN_300_dropout_0.8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeQAAAFcCAYAAAAZN83hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f9eb4473400>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "5TrHF_jAS4dD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "|                                       |  Sub-train     | Test 1        |   Test 2    |  \n",
        "|-----------------------------------|--------------------|-------------------|----------------|\n",
        "|Reconstruction loss   | -213.30219 | -202.64314 |-203.7571  | \n",
        "| Nega_logp(x\\z)          |  -104.58628 |-102.27634 |-103.01279   |\n",
        "| Nega_logp(y\\z)          |  -108.71591 | -100.36681 |--100.744286 |\n",
        "|KL Divergence             | 15.922356   |14.511001    |14.691068  |\n",
        "|ELBO                             | -229.22456 | -217.15414   |-218.44814  |"
      ]
    },
    {
      "metadata": {
        "id": "d9ljOnBFieQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sentence"
      ]
    },
    {
      "metadata": {
        "id": "Cj3oqBvyDN2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  6\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fbx0ui5Ls9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "T1zhq_u2Ti_4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "  return sens"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JwAPQJRDL0Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FRb2Ko3LoRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Case 1"
      ]
    },
    {
      "metadata": {
        "id": "N6TBj8g9L1XJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "5cf44be4-beaa-4a70-f9ed-17f27c0ab06e"
      },
      "cell_type": "code",
      "source": [
        "sentence_bleu([origin_sens], or_sens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.19352631821247096"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 123
        }
      ]
    },
    {
      "metadata": {
        "id": "s_Ej9M04LhcX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "cc781f14-4525-48e0-b1ef-735505bf26f2"
      },
      "cell_type": "code",
      "source": [
        "origin_sens = id_to_word(en_input[9], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(max_length):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(or_sens_str)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  I appeal for an in @-@ depth debate on this subject . eos <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "xE56sEu7Tnzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "outputId": "bf7dfb66-0eaf-4a25-eb50-daff4af293db"
      },
      "cell_type": "code",
      "source": [
        "for i in range(beam_size):\n",
        "  or_sens = id_to_word(x_de[i], en_word_to_id, x_len[0])\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(x_len[0]):\n",
        "     or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(or_sens_str)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> .\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n",
            "  I would like to point out that <OOV> <OOV> <OOV> <OOV> <OOV> <OOV>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "yzo3aNrhY07T",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Case 2"
      ]
    },
    {
      "metadata": {
        "id": "LXiDCxqgah8e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a42bc2a7-ad0d-4897-ea43-f62ab210743d"
      },
      "cell_type": "code",
      "source": [
        "sentence_bleu([origin_sens], or_sens)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.4352598446478626"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "metadata": {
        "id": "B25WCSMYY0l2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "47edbed6-2ba3-444e-99ef-c2c619ef8815"
      },
      "cell_type": "code",
      "source": [
        "origin_sens = id_to_word(en_input[10], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(max_length):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(or_sens_str)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  on behalf of the ALDE Group . - ( IT ) Mr President , ladies and gentlemen , first of all I would like to thank Mr Langen , eos\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "BTRBVlN_ZLl6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 570
        },
        "outputId": "8b7cabd7-28cb-469c-db6c-13cd4b0990e2"
      },
      "cell_type": "code",
      "source": [
        "for i in range(beam_size):\n",
        "  or_sens = id_to_word(x_de[i], en_word_to_id, x_len[0])\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(x_len[0]):\n",
        "     or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(or_sens_str)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  . ( PT ) Mr President @-@ in @-@ Office of the Council , I interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) for interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) behalf interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) H interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office of the Council , I am interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office , I would like to start interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office of the Council , Mr President interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) which interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office , I would like to express interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) &#93; interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office of the Committee on Women &apos;s interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office of the Council , I would interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Commissioner , ladies and gentlemen , I would like to express my interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Mr President @-@ in @-@ Office , I would like to begin interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) . interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) Rural interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) ( interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) and interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) N interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) the interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) Mrs interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) , interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Mr President , Commissioner , ladies and gentlemen , I would like to begin by interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) of interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) Mr interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) <OOV> interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) on interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) ) interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) eos interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n",
            "  Question No N by Mara Izquierdo Rojo ( H @-@ N / N ) : interfered Bossi Elements North Filipino chance ERASMUS coding stationary Kovalev lvarez SURE affiliations setaside energetically\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "wOvuPlyHb5xB",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Case 3"
      ]
    },
    {
      "metadata": {
        "id": "VCK5eYLSb7pK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "da08286e-f62f-47aa-98c9-c57416b940aa"
      },
      "cell_type": "code",
      "source": [
        "origin_sens = id_to_word(en_input[90], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(max_length):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(or_sens_str)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  We think also that the functioning of the Eurogroup is not quite satisfactory and that we must do even better . eos <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hcsgFtuxcD2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "69a20f2c-1bdd-470d-d280-e4333e4faecf"
      },
      "cell_type": "code",
      "source": [
        "for i in range(beam_size):\n",
        "  or_sens = id_to_word(x_de[i], en_word_to_id, x_len[0])\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(x_len[0]):\n",
        "     or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(or_sens_str)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  We hope that the European Union will be able to <OOV> the European consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> our consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> @-@ consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to reach an agreement consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> as consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to reach an end consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> the same consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to ensure that we consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> for consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to ensure that they consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> this consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to make it clear consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We must therefore , however , <OOV> <OOV> the <OOV> . eos is consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to make it easier consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> on consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> with consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> to be consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to ensure that it consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> a consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to make it possible consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> and consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> . consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to do so that consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will take place . eos . eos consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> , consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> of consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> the consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to <OOV> <OOV> <OOV> consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n",
            "  We hope that the European Union will be able to be able to consolidating express Spinelli Ideas fanatic arrangements irrigated Unity \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mTAHkcxVT1rJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ]
    },
    {
      "metadata": {
        "id": "PdeGDsTbMEXG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1004
        },
        "outputId": "a7cdd293-a248-448a-a424-f779b6ee5e68"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 90\n",
        "t = 0\n",
        "\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#########################################################\n",
        "prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "\n",
        "score = np.zeros((beam_size))\n",
        "decode_len = 15\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          prob_next_word[j] = prob_next_word[j] + score[j]\n",
        "        \n",
        "        \n",
        "        beam_id = np.argmax(prob_next_word, axis=0)\n",
        "        \n",
        "        prob_next_word_beam = np.max(prob_next_word, axis=0)\n",
        "        \n",
        "        next_word_id = np.argsort(prob_next_word_beam)[-beam_size:]\n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          beam_id_j = beam_id[next_word_id[j]]\n",
        "          word_id_j = next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[beam_id_j])          \n",
        "          x_de_new[j,t] = copy.deepcopy(word_id_j)\n",
        "          \n",
        "          score[j] = copy.deepcopy(prob_next_word_beam[word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        "
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0812_encoder_2_GDCNN_300_dropout_08/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-47-ced11e8c8837>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     51\u001b[0m                             input_placeholder: np.reshape(x_de[j], (1,max_length))}\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m           \u001b[0mlogits_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_gene_x_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgene_feed_dict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m           \u001b[0mprob_next_word\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_next_word_beam_gene\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogits_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 877\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    878\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1098\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1099\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1100\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1101\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1271\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1272\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1273\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1274\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1276\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1278\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1279\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1280\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1261\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1262\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1263\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1265\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1348\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1349\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "6CKhxVAVMQQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Blue Score Functions"
      ]
    },
    {
      "metadata": {
        "id": "-flyQJg1YsX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_ngram(candidate, references, n):\n",
        "    clipped_count = 0\n",
        "    count = 0\n",
        "    r = 0\n",
        "    c = 0\n",
        "    for si in range(len(candidate)):\n",
        "        # Calculate precision for each sentence\n",
        "        ref_counts = []\n",
        "        ref_lengths = []\n",
        "        # Build dictionary of ngram counts\n",
        "        for reference in references:\n",
        "            ref_sentence = reference[si]\n",
        "            ngram_d = {}\n",
        "            words = ref_sentence.strip().split()\n",
        "            ref_lengths.append(len(words))\n",
        "            limits = len(words) - n + 1\n",
        "            # loop through the sentance consider the ngram length\n",
        "            for i in range(limits):\n",
        "                ngram = ' '.join(words[i:i+n]).lower()\n",
        "                if ngram in ngram_d.keys():\n",
        "                    ngram_d[ngram] += 1\n",
        "                else:\n",
        "                    ngram_d[ngram] = 1\n",
        "            ref_counts.append(ngram_d)\n",
        "        # candidate\n",
        "        cand_sentence = candidate[si]\n",
        "        cand_dict = {}\n",
        "        words = cand_sentence.strip().split()\n",
        "        limits = len(words) - n + 1\n",
        "        for i in range(0, limits):\n",
        "            ngram = ' '.join(words[i:i + n]).lower()\n",
        "            if ngram in cand_dict:\n",
        "                cand_dict[ngram] += 1\n",
        "            else:\n",
        "                cand_dict[ngram] = 1\n",
        "        clipped_count += clip_count(cand_dict, ref_counts)\n",
        "        count += limits\n",
        "        r += best_length_match(ref_lengths, len(words))\n",
        "        c += len(words)\n",
        "    if clipped_count == 0:\n",
        "        pr = 0\n",
        "    else:\n",
        "        pr = float(clipped_count) / count\n",
        "    bp = brevity_penalty(c, r)\n",
        "    return pr, bp\n",
        "\n",
        "\n",
        "def clip_count(cand_d, ref_ds):\n",
        "    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n",
        "    count = 0\n",
        "    for m in cand_d.keys():\n",
        "        m_w = cand_d[m]\n",
        "        m_max = 0\n",
        "        for ref in ref_ds:\n",
        "            if m in ref:\n",
        "                m_max = max(m_max, ref[m])\n",
        "        m_w = min(m_w, m_max)\n",
        "        count += m_w\n",
        "    return count\n",
        "\n",
        "\n",
        "def best_length_match(ref_l, cand_l):\n",
        "    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n",
        "    least_diff = abs(cand_l-ref_l[0])\n",
        "    best = ref_l[0]\n",
        "    for ref in ref_l:\n",
        "        if abs(cand_l-ref) < least_diff:\n",
        "            least_diff = abs(cand_l-ref)\n",
        "            best = ref\n",
        "    return best\n",
        "\n",
        "\n",
        "def brevity_penalty(c, r):\n",
        "    if c > r:\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.exp(1-(float(r)/c))\n",
        "    return bp\n",
        "\n",
        "\n",
        "def geometric_mean(precisions):\n",
        "    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n",
        "\n",
        "\n",
        "def sel_sentence_bleu(candidate, references):\n",
        "    precisions = []\n",
        "    for i in range(4):\n",
        "        pr, bp = count_ngram(candidate, references, i+1)\n",
        "        precisions.append(pr)\n",
        "    bleu = geometric_mean(precisions) * bp\n",
        "    return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcCYvSrNMpDp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate Sentence"
      ]
    },
    {
      "metadata": {
        "id": "vQ36FC_AM1Mw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "OlZ6FVjzMrDr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_tran(logits_y, latent_score, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    #y_max = np.max(logits_y[l,t])\n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l]  + latent_score[l]\n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave,  lower_ob"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wZV6l0TCMxnO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search translate ###########\n",
        "\n",
        "beam_size = 30\n",
        "\n",
        "x_in = np.reshape(np.copy(en_test[2]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_test[2]), (1, max_length))\n",
        "\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#########################################################\n",
        "prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "\n",
        "x_len = np.reshape(np.copy(en_test_len[2]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_test_len[2]), (1,))\n",
        "\n",
        "score = np.zeros((beam_size))\n",
        "latent_score = np.zeros((latent_num))\n",
        "current_prob = np.zeros((beam_size, en_vocab_size))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "    \n",
        "    gene_feed_dict = {latent_var_placeholder:la_var,\n",
        "                      target_placeholder: y_in,\n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    log_y = sess.run(log_liki_y_to, feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    \n",
        "    for i in range(latent_num):\n",
        "      latent_score[i] = log_prob_la[i] + log_y[i]\n",
        "    \n",
        "    for t in range(10):      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "            \n",
        "          prob_next_word[j], current_prob[j]  = find_next_word_beam_first(logits_x, latent_score, t)          \n",
        "          \n",
        "        \n",
        "        beam_max_id = np.argmax(prob_next_word, axis=0)\n",
        "        beam_max = np.max(prob_next_word, axis=0)\n",
        "        \n",
        "        next_beam_id = np.argsort(beam_max)[-beam_size:]\n",
        "        \n",
        "        score_new = np.zeros((beam_size))\n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          beam_id = beam_max_id[next_beam_id[j]]\n",
        "          x_de_new[j] = x_de[beam_id]\n",
        "          x_de_new[j,t] = next_beam_id[j]\n",
        "          score_new[j] =  latent_score[beam_id] + current_prob[beam_id, next_beam_id[j]]\n",
        "        \n",
        "        x_de = x_de_new\n",
        "        latent_score = score_new\n",
        "        \n",
        "    print(np.mean(latent_score))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}