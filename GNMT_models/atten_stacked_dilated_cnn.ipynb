{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy_of_Copy_of_Gene_translation_cnn_0812.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "BSBf8rYS9hX9",
        "gJjC0NuO9oE4",
        "E546I60IPRWK",
        "DqH5fygZc0yl",
        "qYw-PZxCWtMM",
        "7fbx0ui5Ls9v",
        "7FRb2Ko3LoRL",
        "mDFnWZP5F3OC",
        "6CKhxVAVMQQb"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "rMcrrnEBsjy3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Set up the drive path"
      ]
    },
    {
      "metadata": {
        "id": "66FgEFRN-AMY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "# !add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "# !apt-get update -qq 2>&1 > /dev/null\n",
        "# !apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "# from google.colab import auth\n",
        "# auth.authenticate_user()\n",
        "# from oauth2client.client import GoogleCredentials\n",
        "# creds = GoogleCredentials.get_application_default()\n",
        "# import getpass\n",
        "# !google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "# vcode = getpass.getpass()\n",
        "# !echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SWRjvljv-GT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# !mkdir -p drive\n",
        "# !google-drive-ocamlfuse drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Vn79dig8uT8B",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "04be8fbc-09a0-4378-f0cd-7f10312de8ae"
      },
      "cell_type": "code",
      "source": [
        "ls"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "adc.json  \u001b[0m\u001b[01;34mdrive\u001b[0m/  \u001b[01;34msample_data\u001b[0m/\r\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "3MTeUgeB-seY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "a0db1df2-1380-4136-a9ab-bc9d2a044635"
      },
      "cell_type": "code",
      "source": [
        "cd drive/atten_maxout"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/atten_maxout\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "B12HWS4xsx4b",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Import Library"
      ]
    },
    {
      "metadata": {
        "id": "Ly79OANzc7RS",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import collections\n",
        "import os\n",
        "import sys\n",
        "\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "from tensorflow.contrib.seq2seq import sequence_loss\n",
        "\n",
        "import math\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import pickle\n",
        "\n",
        "!pip install -q mosestokenizer\n",
        "from mosestokenizer import *\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu\n",
        "\n",
        "\n",
        "from scipy.stats import multivariate_normal\n",
        "from scipy.stats import norm\n",
        "import copy"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "842o8uvRtBnF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "metadata": {
        "id": "wjDy6-mfnnit",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "## load vocab dict from txt file\n",
        "\n",
        "f = open(\"../dictionary 2/en_word_to_id.txt\", \"rb\")\n",
        "en_word_to_id = pickle.load(f)\n",
        "f.close()\n",
        "\n",
        "f = open(\"../dictionary 2/fr_word_to_id.txt\", \"rb\")\n",
        "fr_word_to_id = pickle.load(f)\n",
        "f.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TuETWPMbu2Yj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "d2e1377e-cdf9-4272-ee11-02f604e371db"
      },
      "cell_type": "code",
      "source": [
        "en_vocab_size = len(en_word_to_id)\n",
        "fr_vocab_size = len(fr_word_to_id)\n",
        "\n",
        "en_eos = en_word_to_id['eos']\n",
        "fr_eos = fr_word_to_id['eos']\n",
        "\n",
        "print(en_vocab_size)\n",
        "print(fr_vocab_size)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "30772\n",
            "39578\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "c9ifxSnpu6zh",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def _read_words(filename):\n",
        "  with tf.gfile.GFile(filename, \"r\") as f: \n",
        "    output = f.read().replace(\"\\n\", \" eos \").replace(\".\", \" .\")\n",
        "    output = re.sub('[0-9]+', 'N', output)\n",
        "    return output\n",
        "\n",
        "def _file_to_word_ids(data, word_to_id):\n",
        "  \n",
        "  id_list = []\n",
        "  \n",
        "  for word in data:\n",
        "    if word in word_to_id:\n",
        "      id_list.append(word_to_id[word])\n",
        "    else:\n",
        "      id_list.append(1)\n",
        "          \n",
        "  return id_list\n",
        "\n",
        "\n",
        "def preprocess_train_data(pre_data, word_to_id, max_length):\n",
        "    pre_data_array = np.asarray(pre_data)\n",
        "    last_start = 0\n",
        "    data = []\n",
        "    each_sen_len = []\n",
        "    \n",
        "    for i in range(len(pre_data_array)):\n",
        "        if pre_data_array[i]==word_to_id['eos']:\n",
        "            if max_length >= len(pre_data_array[last_start:(i+1)]):                \n",
        "              data.append(pre_data_array[last_start:(i+1)])\n",
        "              each_sen_len.append(i+1-last_start)              \n",
        "            else:\n",
        "              shorten_sentences = pre_data_array[last_start:(last_start+max_length-1)]\n",
        "              shorten_sentences = np.concatenate((shorten_sentences, np.asarray([word_to_id['eos']])), axis=0)\n",
        "              data.append(shorten_sentences)\n",
        "              each_sen_len.append(max_length) \n",
        "            \n",
        "            last_start = i+1\n",
        "            \n",
        "    out_sentences = np.full([len(data), max_length], word_to_id['<PAD>'], dtype=np.int32)\n",
        "    for i in range(len(data)):\n",
        "        out_sentences[i,:len(data[i])] = data[i]    \n",
        "    return out_sentences, np.asarray(each_sen_len)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y57-AuNCvDSE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def generate_input_en(en_file, en_word_to_id, max_length):\n",
        "  \n",
        "    en_data = _read_words(en_file)\n",
        "\n",
        "    en_tokenize = MosesTokenizer('en')\n",
        "\n",
        "    en_data = en_tokenize(en_data)\n",
        "\n",
        "    en_data_id = _file_to_word_ids(en_data, en_word_to_id)\n",
        "\n",
        "    en_input, en_input_len = preprocess_train_data(en_data_id, en_word_to_id, max_length)\n",
        "    \n",
        "    return en_input, en_input_len\n",
        "  \n",
        "  \n",
        "  \n",
        "def generate_output_fr(fr_file, fr_word_to_id, max_length):\n",
        "    \n",
        "    fr_data = _read_words(fr_file)\n",
        "\n",
        "    fr_tokenize = MosesTokenizer('fr')\n",
        "\n",
        "    fr_data = fr_tokenize(fr_data)\n",
        "\n",
        "    fr_data_id = _file_to_word_ids(fr_data, fr_word_to_id)\n",
        "\n",
        "    fr_output, fr_output_len = preprocess_train_data(fr_data_id, fr_word_to_id,max_length=30)\n",
        "\n",
        "    #out_beg_token = fr_word_to_id['<beg>']*np.ones((fr_output.shape[0], 1), dtype=np.int32)\n",
        "\n",
        "    #fr_output = np.concatenate((out_beg_token, fr_output), axis=1)\n",
        "\n",
        "    return fr_output,fr_output_len"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ehsowT7hwyjO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def batch_producer(raw_data, raw_data_len, batch_size):    \n",
        "    data_len = len(raw_data)    \n",
        "    batch_len = data_len // batch_size    \n",
        "    data = np.reshape(raw_data[0 : batch_size * batch_len, :], [batch_size, batch_len, -1])\n",
        "    data = np.transpose(data, (1,0,2))\n",
        "    \n",
        "    data_length = np.reshape(raw_data_len[0 : batch_size * batch_len], [batch_size, batch_len])\n",
        "    data_length = np.transpose(data_length, (1,0))\n",
        "    return data, data_length "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "juNYK867gw3B",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "en_oov_id = en_word_to_id['<OOV>']\n",
        "fr_oov_id = fr_word_to_id['<OOV>']\n",
        "\n",
        "def dropout_func(decode_input, dropout_prob, oov_id):\n",
        "  for i in range(decode_input.shape[0]):\n",
        "    for j in range(decode_input.shape[1]):\n",
        "        for k in range(1,decode_input.shape[2]):\n",
        "            if np.random.uniform() > dropout_prob:\n",
        "                decode_input[i,j,k] = oov_id        \n",
        "  return decode_input"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lq37O5wR21AC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Model"
      ]
    },
    {
      "metadata": {
        "id": "zcHfesh3uCDs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define parameters ######################\n",
        "\n",
        "max_length = 30\n",
        "\n",
        "batch_size = 100\n",
        "\n",
        "embed_size = 300\n",
        "\n",
        "infer_hidden_size = 1000\n",
        "\n",
        "latent_size = 150\n",
        "\n",
        "latent_num = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-FraHS_clcvu",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ###################### generate sentence #######################\n",
        "batch_size = 1\n",
        "\n",
        "latent_num = 50"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KJNVkwnhdg9Q",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "###################### define placeholder ######################\n",
        "\n",
        "input_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input')         # batch_size x max_length\n",
        "\n",
        "target_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target')       # batch_size x max_length\n",
        "\n",
        "in_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'in_len')              # batch_size x 1\n",
        "\n",
        "out_length_placeholder = tf.placeholder(tf.int32, [batch_size, ], 'out_len')            # batch_size x 1\n",
        "\n",
        "discount_placeholder = tf.placeholder(tf.float32, name='discount')\n",
        "\n",
        "lr_placeholder = tf.placeholder(tf.float32, name='learn_rate')\n",
        "\n",
        "input_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'input_drop')   # batch_size x max_length\n",
        "\n",
        "target_drop_placeholder = tf.placeholder(tf.int32, [batch_size, max_length], 'target_drop') # batch_size x max_length\n",
        "\n",
        "if_gene_placeholder = tf.placeholder(tf.bool, name='if_gene')\n",
        "\n",
        "latent_var_placeholder = tf.placeholder(tf.float32, [latent_num, batch_size, max_length, latent_size], 'la_var')       # batch_size x max_length x latent_size\n",
        "\n",
        "xavier_initializer = tf.contrib.layers.xavier_initializer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1dqobGQGkHNg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "##################### embedding look-up for input sentences ####################\n",
        "\n",
        "with tf.variable_scope('en_embedding'):\n",
        "    en_embedding = tf.get_variable('en_embeding',[en_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    inputs = tf.nn.embedding_lookup(en_embedding, input_placeholder)                      # batch_size x max_length x embed_size\n",
        "    inputs_drop = tf.nn.embedding_lookup(en_embedding, input_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    \n",
        "\n",
        "with tf.variable_scope('fr_embedding'):\n",
        "    fr_embedding = tf.get_variable('fr_embeding',[fr_vocab_size, embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    targets = tf.nn.embedding_lookup(fr_embedding, target_placeholder)                      # batch_size x max_length x embed_size\n",
        "    targets_drop = tf.nn.embedding_lookup(fr_embedding, target_drop_placeholder)                      # batch_size x max_length x embed_size\n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tmvdTgj5tSpT",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Inference Model - Encoder\n",
        "\n",
        "$q(z_1, z_2, ... , z_T|x,y)$\n",
        "\n",
        "Similar to the encoder of RNNSearch"
      ]
    },
    {
      "metadata": {
        "id": "DSPN85dailPC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#################### Inference model  #######################\n",
        "\n",
        "encode_inputs = tf.transpose(tf.concat([inputs, targets], axis=2), (1,0,2))\n",
        "\n",
        "with tf.variable_scope('encode'):\n",
        "    #basic_cell =tf.contrib.rnn.GRUCell(infer_hidden_size)\n",
        "    basic_cell = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "    init_state = basic_cell.zero_state(batch_size, tf.float32)\n",
        "    encode_outputs, encode_state = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell, \n",
        "                                                                   cell_bw=basic_cell, \n",
        "                                                                   inputs=encode_inputs,                                                                \n",
        "                                                                   initial_state_fw=init_state,\n",
        "                                                                   initial_state_bw=init_state,\n",
        "                                                                   dtype=tf.float32,\n",
        "                                                                   time_major=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6h0aG1lX1ug4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qo0Ycpq9invD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# ##################### Inference model  #######################\n",
        "\n",
        "# ##################### bi-direction lstm of source sentence ######################\n",
        "\n",
        "# encode_inputs_x = tf.transpose(inputs, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_x'):\n",
        "#     basic_cell_x = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_x = basic_cell_x.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_x, encode_state_x = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_x, \n",
        "#                                                                        cell_bw=basic_cell_x, \n",
        "#                                                                        inputs=encode_inputs_x,\n",
        "#                                                                        sequence_length=in_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_x,\n",
        "#                                                                        initial_state_bw=init_state_x,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_x: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ##################### bi-direction lstm of target sentence ######################\n",
        "\n",
        "# encode_inputs_y = tf.transpose(targets, (1,0,2))  # en_max_length x batch_size x embed_size\n",
        "  \n",
        "# with tf.variable_scope('encode_y'):\n",
        "#     basic_cell_y = tf.contrib.rnn.BasicLSTMCell(infer_hidden_size, forget_bias=1.0, state_is_tuple=True)\n",
        "#     init_state_y = basic_cell_y.zero_state(batch_size, tf.float32)\n",
        "#     encode_outputs_y, encode_state_y = tf.nn.bidirectional_dynamic_rnn(cell_fw=basic_cell_y, \n",
        "#                                                                        cell_bw=basic_cell_y, \n",
        "#                                                                        inputs=encode_inputs_y,\n",
        "#                                                                        sequence_length=out_length_placeholder,\n",
        "#                                                                        initial_state_fw=init_state_y,\n",
        "#                                                                        initial_state_bw=init_state_y,\n",
        "#                                                                        dtype=tf.float32,\n",
        "#                                                                        time_major=True)\n",
        "# #### encode_outputs_y: max_length x batch_size x infer_hidden_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RYfWtHL8irsr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### encode_outputs: max_length x batch_size x infer_hidden_size\n",
        "\n",
        "# en_outputs = tf.concat((encode_outputs[0],encode_outputs[1]),2)                             # max_length x batch_size x 2*infer_hidden_size\n",
        "\n",
        "# en_outputs_tran = tf.transpose(en_outputs, (1,0,2))                                         # batch_size x en_max_length x 2*infer_hidden_size\n",
        "\n",
        "# #en_outputs_resh = tf.reshape(en_outputs_tran, (batch_size*max_length, 2*infer_hidden_size)) # batch_size*max_length x 2*infer_hidden_size\n",
        "\n",
        "# ##################### concatenate the state of encoder of x and y ######################\n",
        "\n",
        "# fw_bw_en_state_x = tf.concat((encode_state_x[0][1],encode_state_x[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state_y = tf.concat((encode_state_y[0][1],encode_state_y[1][1]),1)     # en_max_length x  2*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state = tf.concat((fw_bw_en_state_x, fw_bw_en_state_y), 1)             # en_max_length x  4*infer_hidden_size\n",
        "\n",
        "# fw_bw_en_state = tf.tile(tf.expand_dims(fw_bw_en_state, axis=1), (1,30,1))\n",
        "\n",
        "\n",
        "# fw_bw_en = tf.concat((fw_bw_en_state, en_outputs_tran), axis=2)\n",
        "\n",
        "# fw_bw_en = tf.reshape(fw_bw_en, (batch_size*max_length, 6*infer_hidden_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8Yq7w0u-iyl-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('encode_projection'):\n",
        "    W_1 = tf.get_variable('W_1',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_1 = tf.get_variable('b_1',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_2 = tf.get_variable('W_2',[2*infer_hidden_size, latent_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    b_2 = tf.get_variable('b_2',[latent_size,], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gz4FQdg_i4b8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#fw_bw_en_outputs_norm = tf.contrib.layers.batch_norm(fw_bw_en_outputs_resh, center=True, scale=True)\n",
        "\n",
        "la_mean = tf.matmul(en_outputs_resh, W_1) + b_1                              # batch_size*max_length x latent_size \n",
        "\n",
        "la_log_var = tf.matmul(en_outputs_resh, W_2) + b_2                           # batch_size*max_length x latent_size \n",
        "la_var = tf.exp(la_log_var)\n",
        "la_std = tf.sqrt(la_var)\n",
        "\n",
        "kl_div_loss = 1 + la_log_var - tf.square(la_mean) - la_var                      # batch_size*max_length x latent_size\n",
        "kl_div_loss = -0.5 * tf.reduce_sum(kl_div_loss, axis=1)                         # batch_size*max_length x 1\n",
        "kl_div_loss = tf.reshape(kl_div_loss, (batch_size, max_length))                 # batch_size x max_length\n",
        "kl_div_loss = tf.reduce_sum(kl_div_loss, axis=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x2-aUlz0i7Ot",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# latent_variables_v = []\n",
        "# for _ in range(latent_num):\n",
        "#   eposida = tf.random_normal(tf.shape(la_std), mean=0.0,stddev=1)\n",
        "#   latent_variables_sample = la_mean + la_std*eposida\n",
        "#   latent_variables_sample = tf.reshape(latent_variables_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "#   latent_variables_v.append(latent_variables_sample)\n",
        "\n",
        "# latent_variables = latent_variables_v\n",
        "\n",
        "# def if_true():\n",
        "#   latent_v = []\n",
        "#   for h in range(latent_num):\n",
        "#     latent_v.append(latent_var_placeholder[h])\n",
        "#   return latent_v\n",
        "\n",
        "# def if_false():\n",
        "#   return latent_variables_v\n",
        "\n",
        "# latent_variables = tf.cond(if_gene_placeholder, if_true, if_false)\n",
        "\n",
        "# if latent_num == 1:\n",
        "#   new_latent_variables = []\n",
        "#   new_latent_variables.append(latent_variables)\n",
        "#   latent_variables = new_latent_variables"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vDVWXdubzXh0",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "latent_v = []\n",
        "for h in range(latent_num):\n",
        "  latent_v.append(latent_var_placeholder[h])\n",
        "    \n",
        "latent_variables = latent_v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "on6adzC8518v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Generation Model - Decoder\n",
        "\n",
        "$p_\\theta(x|z_1, z_2, ... , z_T)$\n",
        "\n",
        "$p_\\theta(y|z_1, z_2, ... , z_T)$"
      ]
    },
    {
      "metadata": {
        "id": "tiEDu_jlXEtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "filter_num = 150\n",
        "\n",
        "filter_size = 3\n",
        "\n",
        "filter_size_only_pre = 2\n",
        "\n",
        "filter_size_pad = filter_size - filter_size_only_pre\n",
        "\n",
        "filter_zero_pad = tf.zeros(shape=[filter_size_pad, embed_size+latent_size, filter_num], dtype=tf.float32)\n",
        "filter_zero_pad_2 = tf.zeros(shape=[1, filter_size_pad, filter_num, filter_num], dtype=tf.float32)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xjcUuz8EITGC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## X decoder"
      ]
    },
    {
      "metadata": {
        "id": "Mn0rAJKkpdDN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "atten_size = 1000\n",
        "maxout_size = 500\n",
        "\n",
        "with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "    f_x_1 = tf.get_variable(\"x_filter_1\", shape=[3, latent_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_1_dia = tf.concat([f_x_1, \n",
        "                           tf.zeros((2,latent_size+latent_size,filter_num))], axis=0)                                     \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_2[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_2[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((4,filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((3,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_3[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((3,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_3[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((8,filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((7,filter_num,filter_num)), \n",
        "                           tf.reshape(f_x_4[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((7,filter_num,filter_num)),\n",
        "                           tf.reshape(f_x_4[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((16,filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_x'):\n",
        "    proj_w_x = tf.get_variable('project_w_x', [maxout_size,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### sequence weight of x\n",
        "squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length\n",
        "\n",
        "#### attention model ####\n",
        "with tf.variable_scope('encode_projection_x'):\n",
        "    U_a_x = tf.get_variable('U_a_x',[latent_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_a_x = tf.get_variable('W_a_x',[embed_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_a_x = tf.get_variable('V_a_x',[atten_size,1], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### maxout model ####\n",
        "with tf.variable_scope('x_probability'):\n",
        "    U_o_x = tf.get_variable('U_o_x',[filter_num, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_o_x = tf.get_variable('V_o_x',[latent_size,  2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_o_x = tf.get_variable('C_o_x',[latent_size, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Tx5_58s2o8PQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "WaXi_1 = tf.matmul(tf.reshape(x_with_beg[:,:30,:], (batch_size*max_length, embed_size)), W_a_x)   # batch_size*max_length x atten_size\n",
        "\n",
        "WaXi_1 = tf.reshape(WaXi_1, (batch_size, max_length, atten_size))                               #  batch_size x max_length x atten_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gmCR8bcFp1Ws",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder(WaXi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_x)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaX0_allsteps = tf.tile(WaXi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaX0_allsteps + UaZj), V_a_x), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "      \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1,\n",
        "                                  f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, \n",
        "                                  f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, \n",
        "                                  f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(x_out_conv_dia, U_o_x) #+ tf.matmul(de_latent_resh, V_o_x) + tf.matmul(context_c_resh, C_o_x)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  x_out_project = tf.matmul(t_1, proj_w_x) + proj_b_x                                                           # batch_size*max_length x embed_size \n",
        "  \n",
        "  target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "  logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "  prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_x\n",
        "\n",
        "log_liki_x_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_x_to.append(x_decoder(WaXi_1,latent_variables[l]))\n",
        "log_liki_x_to = tf.stack(log_liki_x_to, axis=0)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EiFrjE6K3dWW",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def x_decoder_gene(WaXi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_x)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaX0_allsteps = tf.tile(WaXi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaX0_allsteps + UaZj), V_a_x), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "      \n",
        "  x_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1,\n",
        "                                  f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, \n",
        "                                  f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, \n",
        "                                  f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(x_out_conv_dia, U_o_x) #+ tf.matmul(de_latent_resh, V_o_x) + tf.matmul(context_c_resh, C_o_x)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  x_out_project = tf.matmul(t_1, proj_w_x) + proj_b_x                                                           # batch_size*max_length x embed_size \n",
        "  \n",
        "  logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "  return logits_x\n",
        "\n",
        "logits_gene_x_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_x_to.append(x_decoder_gene(WaXi_1,latent_variables[l]))\n",
        "logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GLbV5QibKQQK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Y decoder"
      ]
    },
    {
      "metadata": {
        "id": "PVd3Oe-0KRjT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "    f_y_1 = tf.get_variable(\"y_filter_1\", shape=[3, latent_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_1_dia = tf.concat([f_y_1, \n",
        "                           tf.zeros((2, latent_size+latent_size,filter_num))], axis=0)  \n",
        "                                    \n",
        "    # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "    f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,filter_num, filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_2[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((1,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_2[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((4,filter_num,filter_num)),], axis=0)\n",
        "    # 9 x filter_num x filter_num\n",
        "    \n",
        "    f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((3,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_3[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((3,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_3[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((8,filter_num,filter_num))], axis=0)\n",
        "    # 13 x filter_num x filter_num\n",
        "    \n",
        "    f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, filter_num, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((7,filter_num,filter_num)), \n",
        "                           tf.reshape(f_y_4[1],(1,filter_num,filter_num)), \n",
        "                           tf.zeros((7,filter_num,filter_num)),\n",
        "                           tf.reshape(f_y_4[2],(1,filter_num,filter_num)),\n",
        "                           tf.zeros((16,filter_num,filter_num))], axis=0)\n",
        "    # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "#### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "with tf.variable_scope('projection_y'):\n",
        "    proj_w_y = tf.get_variable('project_w_y', [maxout_size,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "#### sequence weight of y\n",
        "squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length\n",
        "\n",
        "#### attention model ####\n",
        "with tf.variable_scope('encode_projection_y'):\n",
        "    U_a_y = tf.get_variable('U_a_y',[latent_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    W_a_y = tf.get_variable('W_a_y',[embed_size, atten_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_a_y = tf.get_variable('V_a_y',[atten_size,1], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "#### maxout model ####\n",
        "with tf.variable_scope('y_probability'):\n",
        "    U_o_y = tf.get_variable('U_o_y',[filter_num, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    V_o_y = tf.get_variable('V_o_y',[latent_size,  2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    C_o_y = tf.get_variable('C_o_y',[latent_size, 2*maxout_size], dtype=tf.float32, initializer=xavier_initializer)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "dLMIovKTKp52",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#beg_token_x = tf.zeros((1,embed_size))\n",
        "beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "WaYi_1 = tf.matmul(tf.reshape(y_with_beg[:,:30,:], (batch_size*max_length, embed_size)), W_a_y)   # batch_size*max_length x atten_size\n",
        "\n",
        "WaYi_1 = tf.reshape(WaYi_1, (batch_size, max_length, atten_size))                               #  batch_size x max_length x atten_size"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b9l_wgIkK6Fv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder(WaYi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_y)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaY0_allsteps = tf.tile(WaYi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaY0_allsteps + UaZj), V_a_y), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_y_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(y_out_conv_dia_1, \n",
        "                                  f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(y_out_conv_dia_2,  \n",
        "                                  f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(y_out_conv_dia_3,  \n",
        "                                  f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  \n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(y_out_conv_dia, U_o_y) #+ tf.matmul(de_latent_resh, V_o_y) + tf.matmul(context_c_resh, C_o_y)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  y_out_project = tf.matmul(t_1, proj_w_y) + proj_b_y                                                           # batch_size*max_length x embed_size   \n",
        "  \n",
        "  \n",
        "  target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "  logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "  y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "  prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "  prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "  prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "  prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "  log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "  log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "  return log_liki_y\n",
        "\n",
        "log_liki_y_to = []\n",
        "for l in range(latent_num):\n",
        "  log_liki_y_to.append(y_decoder(WaYi_1,latent_variables[l]))\n",
        "log_liki_y_to = tf.stack(log_liki_y_to, axis=0)\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "5Z8Gt2FPCvqp",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def y_decoder_gene(WaYi_1, de_latent):\n",
        "\n",
        "  UaZj = tf.matmul(tf.reshape(de_latent, (batch_size*max_length, latent_size)), U_a_y)    # batch_size*max_length x atten_size\n",
        "\n",
        "  context_c = []       # batch_size x max_length x latent_size\n",
        "\n",
        "  for t in range(max_length):\n",
        "    WaY0_allsteps = tf.tile(WaYi_1[:,t,:], (max_length,1)) # batch_size*max_length x atten_size\n",
        "\n",
        "    e_1j = tf.reshape(tf.matmul(tf.nn.tanh(WaY0_allsteps + UaZj), V_a_y), (batch_size, max_length))\n",
        "  \n",
        "    alpha_1j = tf.nn.softmax(e_1j)\n",
        "    alpha_1j_reshaped = tf.reshape(alpha_1j,(batch_size*max_length,1))         # batch_size*en_max_length x 1\n",
        "  \n",
        "    c_1j = alpha_1j_reshaped*tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "    c_1 = tf.reduce_sum(tf.reshape(c_1j, (batch_size, max_length, latent_size)), axis=1)\n",
        "  \n",
        "    context_c.append(c_1)\n",
        "  \n",
        "  context_c = tf.stack(context_c, axis=1)\n",
        "  \n",
        "  y_out_conv_dia_1 = tf.nn.conv1d(tf.concat((de_latent, context_c), axis=2), \n",
        "                                  f_y_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_2 = tf.nn.conv1d(y_out_conv_dia_1, \n",
        "                                  f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_3 = tf.nn.conv1d(y_out_conv_dia_2,  \n",
        "                                  f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  y_out_conv_dia_4 = tf.nn.conv1d(y_out_conv_dia_3,  \n",
        "                                  f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "  \n",
        "  y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  de_latent_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  context_c_resh = tf.reshape(de_latent, (batch_size*max_length, latent_size))\n",
        "  \n",
        "  #### Maxout layer\n",
        "  tt_1 = tf.matmul(y_out_conv_dia, U_o_y) #+ tf.matmul(de_latent_resh, V_o_y) + tf.matmul(context_c_resh, C_o_y)       # batch_size*max_length x 2*maxout_size\n",
        "  t_1 = tf.contrib.layers.maxout(tt_1,maxout_size,axis=-1)                                                      # batch_size*max_length x maxout_size\n",
        "  \n",
        "  y_out_project = tf.matmul(t_1, proj_w_y) + proj_b_y                                                           # batch_size*max_length x embed_size   \n",
        "  \n",
        "  logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "  \n",
        "  return logits_y\n",
        "\n",
        "logits_gene_y_to = []\n",
        "for l in range(latent_num):\n",
        "  logits_gene_y_to.append(y_decoder_gene(WaYi_1,latent_variables[l]))\n",
        "logits_gene_y_to = tf.stack(logits_gene_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "BSBf8rYS9hX9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 Generation Model for source sentence $p_\\theta(x|z_1, z_2, ... , z_T)$\n"
      ]
    },
    {
      "metadata": {
        "id": "UNQlel3UksZ9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### concat beg token with input\n",
        "\n",
        "# #beg_token_x = tf.zeros((1,embed_size))\n",
        "# beg_token_x = tf.reshape(en_embedding[en_eos], [1,embed_size])\n",
        "\n",
        "# x_list = tf.split(inputs, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "# x_with_beg_list = [tf.concat((beg_token_x, input[0]), axis=0) for input in x_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "# x_with_beg = tf.stack(x_with_beg_list, axis=0)\n",
        "\n",
        "# #x_input_cnn_1 = tf.concat([latent_variables_1,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "# #x_input_cnn_2 = tf.concat([latent_variables_2,x_with_beg[:,:30,:]], axis=2)                     # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "# #x_input_cnn_4D = tf.expand_dims(x_input_cnn, axis=1)                                        # batch_size x max_length x (embed_size+latent_size)\n",
        "\n",
        "# x_input_cnn = []\n",
        "# for l in range(latent_num):\n",
        "#   x_input_cnn.append(tf.concat([latent_variables[l],x_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "J4x0f5AYajCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with tf.variable_scope('x_con_dialted_1D'):\n",
        "  \n",
        "#     f_x_1 = tf.get_variable(\"x_filter_1\", shape=[2, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_1_dia = tf.concat([f_x_1, \n",
        "#                            tf.zeros((1,embed_size+latent_size,filter_num))], axis=0)                                     \n",
        "#     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "#     f_x_2 = tf.get_variable(\"x_filter_2\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_2_dia = tf.concat([tf.reshape(f_x_2[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size, filter_num)), \n",
        "#                            tf.reshape(f_x_2[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_x_2[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((4,filter_num+latent_size,filter_num)),], axis=0)\n",
        "#     # 9 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_3 = tf.get_variable(\"x_filter_3\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_3_dia = tf.concat([tf.reshape(f_x_3[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_x_3[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_x_3[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((8,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 13 x filter_num x filter_num\n",
        "    \n",
        "#     f_x_4 = tf.get_variable(\"x_filter_4\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_x_4_dia = tf.concat([tf.reshape(f_x_4[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_x_4[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_x_4[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((16,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 21 x filter_num x filter_num\n",
        "    \n",
        "# #     f_x_5 = tf.get_variable(\"x_filter_5\", shape=[2, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "# #     f_x_5_dia = tf.concat([f_x_5, \n",
        "# #                            tf.zeros((1,filter_num+latent_size,filter_num))], axis=0)                                     \n",
        "# #     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "\n",
        "    \n",
        "# #### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "# with tf.variable_scope('projection_x'):\n",
        "#     proj_w_x = tf.get_variable('project_w_x', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     proj_b_x = tf.get_variable('project_b_x', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "# #### sequence weight of x\n",
        "# squence_weight_x= tf.sequence_mask(in_length_placeholder, maxlen=max_length, dtype=tf.float32)                       # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SQIFBsJnkJWA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder(de_input, de_latent):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "#                                   f_x_1_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(tf.concat((x_out_conv_dia_1, de_latent), axis=2),\n",
        "#                                   f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(tf.concat((x_out_conv_dia_2, de_latent), axis=2), \n",
        "#                                   f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(tf.concat((x_out_conv_dia_3, de_latent), axis=2), \n",
        "#                                   f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "#   x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   target_x = tf.reduce_sum(x_out_project*tf.reshape(inputs, (batch_size*max_length, embed_size)), axis=1)\n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "\n",
        "#   logits_x_re = tf.reshape(logits_x, (batch_size, max_length, en_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "#   x_max = tf.reshape(tf.reduce_max(logits_x_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "#   prob_unnorm_x = tf.exp(tf.reshape(target_x, (batch_size*max_length, 1)) - x_max)                                                                      # batch_size*max_length x 1\n",
        "#   prob_constant_x = tf.exp(logits_x - tf.tile(x_max,(1, en_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "#   prob_norm_x = prob_unnorm_x/tf.reshape(tf.reduce_sum(prob_constant_x, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "#   prob_norm_x = tf.reshape(prob_norm_x, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "#   log_prob_norm_x = tf.log(tf.clip_by_value(prob_norm_x,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "#   log_liki_x = tf.reduce_sum(log_prob_norm_x*squence_weight_x, axis=1)                                                    # batch_size x 1\n",
        "#   return log_liki_x\n",
        "\n",
        "# log_liki_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   log_liki_x_to.append(x_decoder(x_input_cnn[l],latent_variables[l]))\n",
        "# log_liki_x_to = tf.stack(log_liki_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "1qX8_07vbQoB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "    \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_gated_conv = x_out_conv_dia[:,:250]*tf.nn.sigmoid(x_out_conv_dia[:,250:])\n",
        "  \n",
        "#   x_out_project = tf.matmul(x_out_gated_conv, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0))) \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l], latent_variables[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "T4gCPfEbETNw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def x_decoder_gene(de_input, latent_var):\n",
        "  \n",
        "#   x_out_conv_dia_1 = tf.nn.conv1d(de_input, f_x_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_2 = tf.nn.conv1d(x_out_conv_dia_1, f_x_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_3 = tf.nn.conv1d(x_out_conv_dia_2, f_x_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_4 = tf.nn.conv1d(x_out_conv_dia_3, f_x_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   x_out_conv_dia_5 = tf.nn.conv1d(x_out_conv_dia_4, f_x_5_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   x_out_conv_dia = tf.reshape(x_out_conv_dia_5, (batch_size*max_length, filter_num))\n",
        "#   x_out_project = tf.matmul(x_out_conv_dia, proj_w_x) + proj_b_x                                       # batch_size*max_length x embed_size \n",
        "  \n",
        "#   logits_x = tf.matmul(x_out_project, tf.transpose(en_embedding,(1,0)))\n",
        "  \n",
        "#   return logits_x\n",
        "\n",
        "# logits_gene_x_to = []\n",
        "# for l in range(latent_num):\n",
        "#   logits_gene_x_to.append(x_decoder_gene(x_input_cnn[l]))\n",
        "# logits_gene_x_to = tf.stack(logits_gene_x_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gJjC0NuO9oE4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.2 Generation Model for target sentence $p_\\theta(y|z_1, z_2, ... , z_T)$\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "-H6P3ob3kPlM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# #### concat beg token with target\n",
        "\n",
        "# #beg_token_y = tf.zeros((1,embed_size))\n",
        "# beg_token_y = tf.reshape(fr_embedding[fr_eos], [1,embed_size])\n",
        "\n",
        "# y_list = tf.split(targets, axis=0, num_or_size_splits=batch_size)\n",
        "\n",
        "# y_with_beg_list = [tf.concat((beg_token_y, target[0]), axis=0) for target in y_list]              # batch_size x (max_length+1) x embed_size\n",
        "\n",
        "# y_with_beg = tf.stack(y_with_beg_list, axis=0)\n",
        "\n",
        "# y_input_cnn = []\n",
        "# for l in range(latent_num):\n",
        "#   y_input_cnn.append(tf.concat([latent_variables[l],y_with_beg[:,:30,:]], axis=2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VW1zf5TWPQnw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# with tf.variable_scope('y_con_dialted_1D'):\n",
        "  \n",
        "#     f_y_1 = tf.get_variable(\"y_filter_1\", shape=[2, embed_size+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_1_dia = tf.concat([f_y_1, \n",
        "#                            tf.zeros((1,embed_size+latent_size,filter_num))], axis=0)  \n",
        "                                    \n",
        "#     # 3 x (embed_size+latent_size) x filter_num\n",
        "    \n",
        "#     f_y_2 = tf.get_variable(\"y_filter_2\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_2_dia = tf.concat([tf.reshape(f_y_2[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_y_2[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_y_2[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((4,filter_num+latent_size,filter_num)),], axis=0)\n",
        "#     # 9 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_3 = tf.get_variable(\"y_filter_3\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_3_dia = tf.concat([tf.reshape(f_y_3[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_y_3[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((3,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_y_3[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((8,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 13 x filter_num x filter_num\n",
        "    \n",
        "#     f_y_4 = tf.get_variable(\"y_filter_4\", shape=[3, filter_num+latent_size, filter_num], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     f_y_4_dia = tf.concat([tf.reshape(f_y_4[0],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)), \n",
        "#                            tf.reshape(f_y_4[1],(1,filter_num+latent_size,filter_num)), \n",
        "#                            tf.zeros((7,filter_num+latent_size,filter_num)),\n",
        "#                            tf.reshape(f_y_4[2],(1,filter_num+latent_size,filter_num)),\n",
        "#                            tf.zeros((16,filter_num+latent_size,filter_num))], axis=0)\n",
        "#     # 21 x filter_num x filter_num\n",
        "\n",
        "    \n",
        "    \n",
        "# #### variablen of a FC layer to map the hidden state of the decoder-rnn in each time-step to the predicted next word\n",
        "# with tf.variable_scope('projection_y'):\n",
        "#     proj_w_y = tf.get_variable('project_w_y', [filter_num,embed_size], dtype=tf.float32, initializer=xavier_initializer)\n",
        "#     proj_b_y = tf.get_variable('project_b_y', [embed_size,], dtype=tf.float32, initializer=xavier_initializer)\n",
        "    \n",
        "    \n",
        "# #### sequence weight of y\n",
        "# squence_weight_y = tf.sequence_mask(out_length_placeholder, maxlen=max_length, dtype=tf.float32)                        # batch_size x max_length"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xpRsIIdCj5so",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# def y_decoder(de_input,de_latent):\n",
        "  \n",
        "#   y_out_conv_dia_1 = tf.nn.conv1d(de_input, \n",
        "#                                   f_y_1_dia, stride=1, padding='SAME')                     # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_2 = tf.nn.conv1d(tf.concat((y_out_conv_dia_1, de_latent), axis=2), \n",
        "#                                   f_y_2_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_3 = tf.nn.conv1d(tf.concat((y_out_conv_dia_2, de_latent), axis=2), \n",
        "#                                   f_y_3_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "#   y_out_conv_dia_4 = tf.nn.conv1d(tf.concat((y_out_conv_dia_3, de_latent), axis=2), \n",
        "#                                   f_y_4_dia, stride=1, padding='SAME')                # batch_size x max_length x filter_num\n",
        "  \n",
        "#   y_out_conv_dia = tf.reshape(y_out_conv_dia_4, (batch_size*max_length, filter_num))\n",
        "  \n",
        "#   y_out_project = tf.matmul(y_out_conv_dia, proj_w_y) + proj_b_y \n",
        "                                        \n",
        "\n",
        "#   target_y = tf.reduce_sum(y_out_project*tf.reshape(targets, (batch_size*max_length, embed_size)), axis=1)\n",
        "#   logits_y = tf.matmul(y_out_project, tf.transpose(fr_embedding,(1,0)))\n",
        "\n",
        "#   logits_y_re = tf.reshape(logits_y, (batch_size, max_length, fr_vocab_size))                                   # batch_size x max_length x fr_vocab_size\n",
        "#   y_max = tf.reshape(tf.reduce_max(logits_y_re, axis=2), (batch_size*max_length, 1))                            # batch_size*max_length x 1\n",
        "\n",
        "#   prob_unnorm_y = tf.exp(tf.reshape(target_y, (batch_size*max_length, 1)) - y_max)                              # batch_size*max_length x 1\n",
        "#   prob_constant_y = tf.exp(logits_y - tf.tile(y_max,(1, fr_vocab_size)))                                        # batch_size*max_length x fr_vocab_size\n",
        "                                           \n",
        "#   prob_norm_y = prob_unnorm_y/tf.reshape(tf.reduce_sum(prob_constant_y, axis=1), (batch_size*max_length, 1))              # batch_size*max_length x 1\n",
        "#   prob_norm_y = tf.reshape(prob_norm_y, (batch_size, max_length))                                                         # batch_size x max_length\n",
        "#   log_prob_norm_y = tf.log(tf.clip_by_value(prob_norm_y,1e-8,1.0))                                                        # batch_size x max_length\n",
        "\n",
        "#   log_liki_y = tf.reduce_sum(log_prob_norm_y*squence_weight_y, axis=1)                                                    # batch_size x 1\n",
        "#   return log_liki_y\n",
        "\n",
        "# log_liki_y_to = []\n",
        "# for l in range(latent_num):\n",
        "#   log_liki_y_to.append(y_decoder(y_input_cnn[l],latent_variables[l]))\n",
        "# log_liki_y_to = tf.stack(log_liki_y_to, axis=0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jb2g_kS1cKxa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### The lower bound of log-joint-likelihood, to maximize"
      ]
    },
    {
      "metadata": {
        "id": "5wHFYwB9jkDL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "nega_log_liki_x_y = 0\n",
        "\n",
        "nega_elbo = 0\n",
        "\n",
        "for l in range(latent_num):\n",
        "  nega_log_liki_x_y = nega_log_liki_x_y + tf.reduce_mean(- log_liki_x_to[l] - log_liki_y_to[l])\n",
        "  nega_elbo = nega_elbo - log_liki_x_to[l] - log_liki_y_to[l]\n",
        "  \n",
        "nega_log_liki_x_y = nega_log_liki_x_y/latent_num\n",
        "nega_elbo = nega_elbo/latent_num + discount_placeholder*kl_div_loss\n",
        "objective = tf.reduce_mean(nega_elbo) \n",
        "kl_div_loss_batch_mean = tf.reduce_mean(kl_div_loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "2Z8P6-XM38MV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# L2 reguralization for trainable variables\n",
        "#train_variables = tf.trainable_variables()\n",
        "#regularization_cost = tf.reduce_sum([tf.nn.l2_loss(variable) for variable in train_variables])\n",
        "#regular_rate = 0.00001\n",
        "#+ regular_rate*regularization_cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "UwMB6m32Yzfr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# optimizer = tf.train.AdamOptimizer(lr_placeholder)\n",
        "\n",
        "# gvs, var = zip(*optimizer.compute_gradients(objective))\n",
        "\n",
        "# #checked_gvs = [tf.where(tf.is_nan(grad), tf.zeros_like(grad), grad) for grad in gvs]\n",
        "\n",
        "# cliped_gvs, _ = tf.clip_by_global_norm(gvs, 1)\n",
        "\n",
        "# opt = optimizer.apply_gradients(zip(cliped_gvs, var))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "m4Glrs-GxHeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
        "#gvs = optimizer.compute_gradients(objective)\n",
        "#capped_gvs = [(tf.clip_by_norm(grad, 1), var) for grad, var in gvs]\n",
        "#opt = optimizer.apply_gradients(capped_gvs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ASMl3Fgnsyfy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#### save the model\n",
        "def save_model(session, path):\n",
        "    if not os.path.exists(\"./result_0827/\"):\n",
        "        os.mkdir('./result_0827/')\n",
        "    saver = tf.train.Saver()\n",
        "    saver.save(session, path)\n",
        "\n",
        "path1 = './result_0827/model_each_epch.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "E546I60IPRWK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Training "
      ]
    },
    {
      "metadata": {
        "id": "Bxq1rcyggfA5",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    return (1 / (1 + math.exp(-x)))\n",
        "\n",
        "\n",
        "def text_save(content,filename,mode='a'):\n",
        "    # Try to save a list variable in txt file.\n",
        "    file = open(filename,mode)\n",
        "    for i in range(len(content)):\n",
        "        file.write(str(content[i])+'\\n')\n",
        "    file.close()\n",
        "    \n",
        "def text_read(filename):\n",
        "    # Try to read a txt file and return a list.Return [] if there was a mistake.\n",
        "    try:\n",
        "        file = open(filename,'r')\n",
        "    except IOError:\n",
        "        error = []\n",
        "        return error\n",
        "    content = file.readlines()\n",
        " \n",
        "    for i in range(len(content)):\n",
        "        content[i] = content[i][:len(content[i])-1]\n",
        " \n",
        "    file.close()\n",
        "    return content"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RovgeCM_9-i3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "total_step = 0\n",
        "learning_rate = 0.001\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "\n",
        "init = tf.global_variables_initializer()\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    sess.run(init)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "#         en_input_drop_batches = dropout_func(en_input_batches, 0.7, en_oov_id)\n",
        "#         fr_output_drop_batches = dropout_func(fr_output_batches, 0.7, fr_oov_id)\n",
        "        \n",
        "        en_input_drop_batches = en_input_batches\n",
        "        fr_output_drop_batches = fr_output_batches\n",
        "        \n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = sigmoid(0.0025*(total_step-2500))\n",
        "            discount_rate = 0.0002*total_step\n",
        "            if discount_rate >1:\n",
        "              discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "        \n",
        "text_save(elbo_results, './result_0827/elbo_results.txt')\n",
        "text_save(kl_results, './result_0827/kl_results.txt')\n",
        "text_save(likei_results, './result_0827/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCryB1PjBEYF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 8478
        },
        "outputId": "0fdc6102-f3dd-48bb-e137-9a17a9645eb1"
      },
      "cell_type": "code",
      "source": [
        "max_epochs = 5\n",
        "learning_rate = 0.001\n",
        "total_step = 0\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "elbo_results=[]\n",
        "kl_results = []\n",
        "likei_results = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for epoc in range(max_epochs):\n",
        "      \n",
        "        print(\"learning rate\")\n",
        "        print(learning_rate)\n",
        "      \n",
        "        print('Epoch {}'.format(epoc))\n",
        "\n",
        "        ind_small_txt = epoc + 5\n",
        "        \n",
        "        en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "        fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "        \n",
        "        print(en_file)\n",
        "        print(fr_file)\n",
        "        \n",
        "        en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "        fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "        en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "        fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "        en_input_drop_batches = en_input_batches\n",
        "        fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "        batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "        ########### training ###########\n",
        "        for i in range(batch_len):\n",
        "          \n",
        "            #discount_rate = 0.0002*total_step\n",
        "            #if discount_rate >1:\n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         lr_placeholder: learning_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti,  _ = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective, opt], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "              \n",
        "            if i%100 == 0:\n",
        "              print(kl)\n",
        "              print(nage_likeli)\n",
        "              print(objecti)\n",
        "              print(discount_rate)\n",
        "              \n",
        "              print(llx_mean)\n",
        "              print(lly_mean)\n",
        " \n",
        "              elbo_results.append(objecti)\n",
        "              kl_results.append(kl)\n",
        "              likei_results.append(nage_likeli)\n",
        "            \n",
        "            total_step = total_step + 1\n",
        "            \n",
        "        save_model(sess, path1)\n",
        "\n",
        "text_save(elbo_results, './result_0827/elbo_results.txt')\n",
        "text_save(kl_results, './result_0827/kl_results.txt')\n",
        "text_save(likei_results, './result_0827/likei_results.txt')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 0\n",
            "../small_txt/5_en.txt\n",
            "../small_txt/5_fr.txt\n",
            "88.00195\n",
            "118.50188\n",
            "206.50383\n",
            "1\n",
            "60.067104\n",
            "58.43478\n",
            "94.82827\n",
            "125.11245\n",
            "219.94072\n",
            "1\n",
            "65.51787\n",
            "59.594585\n",
            "96.83912\n",
            "126.544426\n",
            "223.38356\n",
            "1\n",
            "66.62332\n",
            "59.921097\n",
            "99.121864\n",
            "132.78278\n",
            "231.90463\n",
            "1\n",
            "69.32318\n",
            "63.459595\n",
            "98.675354\n",
            "128.3542\n",
            "227.02956\n",
            "1\n",
            "67.92458\n",
            "60.42961\n",
            "92.270226\n",
            "120.95975\n",
            "213.22997\n",
            "1\n",
            "63.962944\n",
            "56.996796\n",
            "98.31989\n",
            "128.4535\n",
            "226.77339\n",
            "1\n",
            "67.29732\n",
            "61.15618\n",
            "91.90546\n",
            "115.60121\n",
            "207.50665\n",
            "1\n",
            "59.90189\n",
            "55.699326\n",
            "95.65855\n",
            "118.224144\n",
            "213.88268\n",
            "1\n",
            "62.05914\n",
            "56.16501\n",
            "97.58465\n",
            "128.43816\n",
            "226.0228\n",
            "1\n",
            "67.31044\n",
            "61.127716\n",
            "99.4332\n",
            "132.79594\n",
            "232.22914\n",
            "1\n",
            "70.10253\n",
            "62.69341\n",
            "98.52193\n",
            "129.53596\n",
            "228.05789\n",
            "1\n",
            "67.99583\n",
            "61.54015\n",
            "97.067535\n",
            "124.05039\n",
            "221.11795\n",
            "1\n",
            "66.256195\n",
            "57.794205\n",
            "95.21031\n",
            "117.723206\n",
            "212.93353\n",
            "1\n",
            "60.902565\n",
            "56.820633\n",
            "96.56882\n",
            "129.60023\n",
            "226.16907\n",
            "1\n",
            "67.15755\n",
            "62.44269\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 1\n",
            "../small_txt/6_en.txt\n",
            "../small_txt/6_fr.txt\n",
            "94.668304\n",
            "126.81277\n",
            "221.48105\n",
            "1\n",
            "64.67561\n",
            "62.13714\n",
            "93.631805\n",
            "120.11995\n",
            "213.75175\n",
            "1\n",
            "60.62185\n",
            "59.498096\n",
            "97.5058\n",
            "123.194046\n",
            "220.69984\n",
            "1\n",
            "64.0172\n",
            "59.17685\n",
            "94.42176\n",
            "127.333694\n",
            "221.75545\n",
            "1\n",
            "66.256905\n",
            "61.07678\n",
            "91.387634\n",
            "122.737206\n",
            "214.12485\n",
            "1\n",
            "62.41869\n",
            "60.31852\n",
            "97.128845\n",
            "123.03023\n",
            "220.15906\n",
            "1\n",
            "64.74345\n",
            "58.286777\n",
            "89.77409\n",
            "114.73006\n",
            "204.50414\n",
            "1\n",
            "59.9652\n",
            "54.764847\n",
            "87.82897\n",
            "114.61003\n",
            "202.43898\n",
            "1\n",
            "59.56264\n",
            "55.047386\n",
            "95.99574\n",
            "121.532326\n",
            "217.52806\n",
            "1\n",
            "62.564106\n",
            "58.96821\n",
            "96.49732\n",
            "121.35275\n",
            "217.85008\n",
            "1\n",
            "63.029522\n",
            "58.32323\n",
            "94.03466\n",
            "124.53089\n",
            "218.56555\n",
            "1\n",
            "63.916107\n",
            "60.61478\n",
            "90.31515\n",
            "118.54615\n",
            "208.86131\n",
            "1\n",
            "59.937416\n",
            "58.608734\n",
            "91.76776\n",
            "124.9351\n",
            "216.70287\n",
            "1\n",
            "64.427025\n",
            "60.50807\n",
            "86.79446\n",
            "110.44909\n",
            "197.24353\n",
            "1\n",
            "57.161987\n",
            "53.287098\n",
            "95.485085\n",
            "123.71258\n",
            "219.19766\n",
            "1\n",
            "64.153\n",
            "59.559574\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 2\n",
            "../small_txt/7_en.txt\n",
            "../small_txt/7_fr.txt\n",
            "95.74649\n",
            "119.812996\n",
            "215.55948\n",
            "1\n",
            "60.790928\n",
            "59.02205\n",
            "96.9848\n",
            "122.71096\n",
            "219.69576\n",
            "1\n",
            "63.303112\n",
            "59.40784\n",
            "91.26267\n",
            "115.58126\n",
            "206.84395\n",
            "1\n",
            "59.503643\n",
            "56.077618\n",
            "102.04155\n",
            "128.92833\n",
            "230.96988\n",
            "1\n",
            "67.69031\n",
            "61.238026\n",
            "95.25499\n",
            "129.9683\n",
            "225.22333\n",
            "1\n",
            "68.301765\n",
            "61.66656\n",
            "93.53001\n",
            "119.55206\n",
            "213.08208\n",
            "1\n",
            "61.664482\n",
            "57.887573\n",
            "95.88626\n",
            "119.12869\n",
            "215.01494\n",
            "1\n",
            "61.023888\n",
            "58.104805\n",
            "97.083565\n",
            "126.363884\n",
            "223.44745\n",
            "1\n",
            "65.61978\n",
            "60.744106\n",
            "96.46841\n",
            "122.95894\n",
            "219.42734\n",
            "1\n",
            "64.115326\n",
            "58.843616\n",
            "96.73876\n",
            "123.25888\n",
            "219.99762\n",
            "1\n",
            "64.36703\n",
            "58.89183\n",
            "99.64078\n",
            "129.46553\n",
            "229.1063\n",
            "1\n",
            "66.54517\n",
            "62.920334\n",
            "90.20081\n",
            "119.699295\n",
            "209.90012\n",
            "1\n",
            "61.861496\n",
            "57.837803\n",
            "94.405556\n",
            "112.031044\n",
            "206.4366\n",
            "1\n",
            "57.41835\n",
            "54.612686\n",
            "95.27457\n",
            "124.0469\n",
            "219.32144\n",
            "1\n",
            "64.53394\n",
            "59.51295\n",
            "92.48367\n",
            "111.35934\n",
            "203.843\n",
            "1\n",
            "56.713924\n",
            "54.645397\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 3\n",
            "../small_txt/8_en.txt\n",
            "../small_txt/8_fr.txt\n",
            "95.62741\n",
            "121.24816\n",
            "216.87556\n",
            "1\n",
            "63.070553\n",
            "58.177616\n",
            "94.958565\n",
            "117.57027\n",
            "212.52881\n",
            "1\n",
            "60.261036\n",
            "57.309235\n",
            "97.370895\n",
            "120.27884\n",
            "217.64972\n",
            "1\n",
            "61.879883\n",
            "58.39895\n",
            "93.84033\n",
            "117.83015\n",
            "211.67047\n",
            "1\n",
            "60.431053\n",
            "57.399094\n",
            "93.57589\n",
            "108.21914\n",
            "201.79504\n",
            "1\n",
            "55.99874\n",
            "52.220406\n",
            "99.31686\n",
            "121.30041\n",
            "220.61726\n",
            "1\n",
            "62.903076\n",
            "58.397335\n",
            "98.681885\n",
            "123.22381\n",
            "221.90569\n",
            "1\n",
            "62.393566\n",
            "60.830235\n",
            "98.48203\n",
            "123.53492\n",
            "222.01695\n",
            "1\n",
            "64.1489\n",
            "59.38602\n",
            "103.38423\n",
            "127.289795\n",
            "230.67403\n",
            "1\n",
            "66.134315\n",
            "61.155468\n",
            "95.11649\n",
            "116.47805\n",
            "211.59454\n",
            "1\n",
            "59.863995\n",
            "56.61406\n",
            "93.41294\n",
            "116.57581\n",
            "209.98875\n",
            "1\n",
            "61.3105\n",
            "55.265316\n",
            "92.80898\n",
            "115.7984\n",
            "208.6074\n",
            "1\n",
            "60.484108\n",
            "55.314293\n",
            "97.366035\n",
            "125.21422\n",
            "222.58026\n",
            "1\n",
            "65.23281\n",
            "59.981407\n",
            "97.522484\n",
            "117.92707\n",
            "215.44955\n",
            "1\n",
            "60.66243\n",
            "57.264637\n",
            "96.26448\n",
            "120.77899\n",
            "217.04346\n",
            "1\n",
            "62.954002\n",
            "57.824986\n",
            "learning rate\n",
            "0.001\n",
            "Epoch 4\n",
            "../small_txt/9_en.txt\n",
            "../small_txt/9_fr.txt\n",
            "91.51523\n",
            "119.50105\n",
            "211.01625\n",
            "1\n",
            "61.32477\n",
            "58.17628\n",
            "95.07838\n",
            "117.30714\n",
            "212.38553\n",
            "1\n",
            "60.49187\n",
            "56.81526\n",
            "104.006355\n",
            "127.921524\n",
            "231.92787\n",
            "1\n",
            "66.26216\n",
            "61.659348\n",
            "93.65664\n",
            "115.661964\n",
            "209.31859\n",
            "1\n",
            "59.97991\n",
            "55.682037\n",
            "95.32656\n",
            "117.17557\n",
            "212.50214\n",
            "1\n",
            "60.194534\n",
            "56.98102\n",
            "94.69077\n",
            "115.664345\n",
            "210.35512\n",
            "1\n",
            "59.988407\n",
            "55.675926\n",
            "93.57763\n",
            "108.57668\n",
            "202.1543\n",
            "1\n",
            "55.588974\n",
            "52.98769\n",
            "97.15478\n",
            "114.37906\n",
            "211.53384\n",
            "1\n",
            "59.19472\n",
            "55.184345\n",
            "94.063446\n",
            "117.755165\n",
            "211.81862\n",
            "1\n",
            "60.528667\n",
            "57.22651\n",
            "92.22353\n",
            "111.86772\n",
            "204.09125\n",
            "1\n",
            "57.718887\n",
            "54.148834\n",
            "100.16441\n",
            "116.834496\n",
            "216.9989\n",
            "1\n",
            "61.66117\n",
            "55.173336\n",
            "91.53296\n",
            "110.160126\n",
            "201.69307\n",
            "1\n",
            "56.330654\n",
            "53.82947\n",
            "97.7724\n",
            "119.7689\n",
            "217.5413\n",
            "1\n",
            "63.399437\n",
            "56.369457\n",
            "95.95069\n",
            "111.73707\n",
            "207.68776\n",
            "1\n",
            "57.299873\n",
            "54.4372\n",
            "98.6426\n",
            "121.73173\n",
            "220.37431\n",
            "1\n",
            "62.603844\n",
            "59.127884\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "DqH5fygZc0yl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Load test data and test the model"
      ]
    },
    {
      "metadata": {
        "id": "T3EsytHdPkpd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 1"
      ]
    },
    {
      "metadata": {
        "id": "Is72zrRCKFOB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "6abffc99-a064-41a7-b861-239e69ea009d"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_1 = []\n",
        "nage_likeli_test_1 = []\n",
        "objecti_test_1 = []\n",
        "llx_test_1 = []\n",
        "lly_test_1 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 12\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                           \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_1.append(kl)\n",
        "            nage_likeli_test_1.append(nage_likeli)\n",
        "            objecti_test_1.append(objecti)\n",
        "            llx_test_1.append(llx_mean)\n",
        "            lly_test_1.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_1))\n",
        "print(np.mean(nage_likeli_test_1))\n",
        "print(np.mean(objecti_test_1))\n",
        "print(np.mean(llx_test_1))\n",
        "print(np.mean(lly_test_1))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n",
            "94.73441\n",
            "134.34508\n",
            "229.07947\n",
            "68.76233\n",
            "65.58274\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "r7mH1flzQMlU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 2"
      ]
    },
    {
      "metadata": {
        "id": "BAKb79bddTJr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "b79de35d-cdbc-4537-c25e-995e8e9fc726"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 11\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_2.append(kl)\n",
        "            nage_likeli_test_2.append(nage_likeli)\n",
        "            objecti_test_2.append(objecti)\n",
        "            llx_test_2.append(llx_mean)\n",
        "            lly_test_2.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n",
            "94.63166\n",
            "134.99571\n",
            "229.62738\n",
            "69.32357\n",
            "65.67215\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-HIuHoabk85N",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Test 3"
      ]
    },
    {
      "metadata": {
        "id": "pNniNB7Qk5kj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "b996e0d9-3967-48a2-d599-3df6e7b83400"
      },
      "cell_type": "code",
      "source": [
        "################################################################################\n",
        "######################## load test data ########################################\n",
        "################################################################################\n",
        "\n",
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_2 = []\n",
        "nage_likeli_test_2 = []\n",
        "objecti_test_2 = []\n",
        "llx_test_2 = []\n",
        "lly_test_2 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 10\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_2.append(kl)\n",
        "            nage_likeli_test_2.append(nage_likeli)\n",
        "            objecti_test_2.append(objecti)\n",
        "            llx_test_2.append(llx_mean)\n",
        "            lly_test_2.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_2))\n",
        "print(np.mean(nage_likeli_test_2))\n",
        "print(np.mean(objecti_test_2))\n",
        "print(np.mean(llx_test_2))\n",
        "print(np.mean(lly_test_2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n",
            "96.2272\n",
            "133.75565\n",
            "229.98282\n",
            "68.707146\n",
            "65.0485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "n31e9XAOQ5hh",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Sub-Train Set"
      ]
    },
    {
      "metadata": {
        "id": "yn8UHiro-RTK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 123
        },
        "outputId": "d2a6e8f5-1a84-454d-990e-ae1457acd869"
      },
      "cell_type": "code",
      "source": [
        "zero_latent = np.zeros((latent_num, batch_size, max_length, latent_size))\n",
        "\n",
        "kl_test_3 = []\n",
        "nage_likeli_test_3 = []\n",
        "objecti_test_3 = []\n",
        "llx_test_3 = []\n",
        "lly_test_3 = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)   \n",
        "\n",
        "    ind_small_txt = 6\n",
        "        \n",
        "    en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "    fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "    en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "    fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "    en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "    fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)\n",
        "        \n",
        "    en_input_drop_batches = en_input_batches\n",
        "    fr_output_drop_batches = fr_output_batches\n",
        "                       \n",
        "    batch_len = en_input_batches.shape[0]\n",
        "        \n",
        "    ########### training ###########\n",
        "    for i in range(batch_len):\n",
        "          \n",
        "            discount_rate = 1\n",
        " \n",
        "            feed_dict = {input_placeholder: en_input_batches[i], \n",
        "                         target_placeholder: fr_output_batches[i],\n",
        "                         input_drop_placeholder:en_input_drop_batches[i],\n",
        "                         target_drop_placeholder:fr_output_drop_batches[i],\n",
        "                         in_length_placeholder: en_input_len_batches[i], \n",
        "                         out_length_placeholder: fr_output_len_batches[i],\n",
        "                         discount_placeholder: discount_rate,\n",
        "                         if_gene_placeholder: False,\n",
        "                         latent_var_placeholder: zero_latent}\n",
        "  \n",
        "      \n",
        "            kl, nage_likeli, llx, lly, objecti = sess.run([kl_div_loss_batch_mean, nega_log_liki_x_y, log_liki_x_to, log_liki_y_to, objective], feed_dict=feed_dict)       \n",
        "            \n",
        "            llx_mean = -np.mean(llx)\n",
        "            lly_mean = -np.mean(lly)\n",
        "            \n",
        "            kl_test_3.append(kl)\n",
        "            nage_likeli_test_3.append(nage_likeli)\n",
        "            objecti_test_3.append(objecti)\n",
        "            llx_test_3.append(llx_mean)\n",
        "            lly_test_3.append(lly_mean)\n",
        "\n",
        "\n",
        "print(np.mean(kl_test_3))\n",
        "print(np.mean(nage_likeli_test_3))\n",
        "print(np.mean(objecti_test_3))\n",
        "print(np.mean(llx_test_3))\n",
        "print(np.mean(lly_test_3))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n",
            "94.54011\n",
            "121.75517\n",
            "216.29527\n",
            "62.96242\n",
            "58.792747\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qYw-PZxCWtMM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Numerical Results"
      ]
    },
    {
      "metadata": {
        "id": "YO6RbSRWWsxr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 382
        },
        "outputId": "e4514499-17d5-49d1-c4bc-2002cf3e22a9"
      },
      "cell_type": "code",
      "source": [
        "elbo_read = text_read('./result_0827/elbo_results.txt')\n",
        "elbo_read = [-float(elbo) for elbo in elbo_read]\n",
        "\n",
        "kl_read = text_read('./result_0827/kl_results.txt')\n",
        "kl_read = [float(kl) for kl in kl_read]\n",
        "\n",
        "likei_read = text_read('./result_0827/likei_results.txt')\n",
        "likei_read = [-float(likei) for likei in likei_read]\n",
        "\n",
        "plt.plot(kl_read, color = 'C0')\n",
        "plt.plot(likei_read, color = 'C1')\n",
        "plt.plot(elbo_read, color = 'C2')\n",
        "plt.legend(['kl divergence','nage_log_like_x_n_y', 'nage_elbo'], fontsize=12)\n",
        "plt.title(\"Encoder_2_GDCNN_300_dropout_0.8\", fontsize=16)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5,1,'Encoder_2_GDCNN_300_dropout_0.8')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeoAAAFcCAYAAAAH/v1SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f994c246780>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "d9ljOnBFieQS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Generate sentence"
      ]
    },
    {
      "metadata": {
        "id": "Cj3oqBvyDN2O",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "ind_small_txt =  6\n",
        "        \n",
        "en_file = \"../small_txt/\" + str(ind_small_txt) + \"_en.txt\"\n",
        "fr_file = \"../small_txt/\" + str(ind_small_txt) + \"_fr.txt\"\n",
        "\n",
        "en_input, en_input_len = generate_input_en(en_file, en_word_to_id, max_length)\n",
        "fr_output, fr_output_len = generate_output_fr(fr_file, fr_word_to_id, max_length)\n",
        "\n",
        "en_input_batches, en_input_len_batches = batch_producer(en_input, en_input_len, batch_size) \n",
        "fr_output_batches, fr_output_len_batches = batch_producer(fr_output, fr_output_len, batch_size)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7fbx0ui5Ls9v",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Define functions"
      ]
    },
    {
      "metadata": {
        "id": "JwAPQJRDL0Bi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "x69fUUV9HiT1",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def output_sentence(idd,x_de,y_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  print(en_bleu)\n",
        "  \n",
        "  ########## \"French\" ##########\n",
        "  origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"y:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, 30)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"y_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  fr_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  print(fr_bleu)\n",
        "  return en_bleu, fr_bleu\n",
        "  \n",
        "def obleu_sentence(idd,x_de,y_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  print(en_bleu)\n",
        "  \n",
        "  ########## \"French\" ##########\n",
        "  origin_sens, ori_len = id_to_word(fr_output[idd], fr_word_to_id, max_length)\n",
        "  or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, 30)\n",
        "  fr_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(0.25, 0.25, 0.25, 0.25))\n",
        "  print(fr_bleu)\n",
        "  \n",
        "  return en_bleu, fr_bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "7FRb2Ko3LoRL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Case 9 \\ 64 \\ 68 \\ 77 \\ 78 \\ 99"
      ]
    },
    {
      "metadata": {
        "id": "xE56sEu7Tnzq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "ef6bb2f2-4189-4bc7-b883-96c1ae69024e"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(9,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I appeal for an in @-@ depth debate on this subject .\n",
            "x_re:\n",
            "  I call for this in this depth debate on this subject .\n",
            "0.75\n",
            "y:\n",
            "  Je demande qu&apos; un dbat approfondi soit men sur ce sujet .\n",
            "y_re:\n",
            "  Je demande qu&apos; un dbat europen a positif  ce sujet .\n",
            "0.6666666666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Y8o94Ce6t3Vl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "15c3a570-f1c9-48e0-fb4b-11a6ac15cc2b"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(64,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I also wish to thank Mr Almunia for the assistance he has given Cyprus all this time in achieving this objective .\n",
            "x_re:\n",
            "  I also like to thank Mr Barnier for the fact , has on it and this time in the this objective .\n",
            "0.6363636363636364\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Je voudrais galement remercier M . Almunia pour l&apos; aide qu&apos; il a apporte  Chypre pendant tout ce temps en vue d&apos; atteindre cet objectif .\n",
            "y_re:\n",
            "  Je voudrais galement remercier M . Andor pour l&apos; importance qu&apos; il a apporte  Cancn dans tout un rapport en vue d&apos; atteindre un accord .\n",
            "0.7037037037037037\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "x3Z8UXjowCYr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "f1deb345-7bf5-41e2-84ac-57086729ec59"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(89,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  It is a mechanism that we have criticised here , in this Parliament , and that , I think , we continue to criticise .\n",
            "x_re:\n",
            "  It is a matter that we have heard here , in this House , and that , I think , we continue to say .\n",
            "0.84\n",
            "y:\n",
            "  C&apos; est un mcanisme que nous avons critiqu ici , dans ce Parlement , et que , je pense , nous continuons de critiquer .\n",
            "y_re:\n",
            "  C&apos; est un sujet de nous avons fait , , dans ce Parlement , et , , je pense , nous sr de suivre .\n",
            "0.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Nvg92hyOIMYc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 194
        },
        "outputId": "cb940e0e-8795-4988-8e89-7a45e42118b8"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(77,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  The issue of the euro is no small matter for our fellow citizens : it is , in their hands , one of the European Union &apos;s most valuable\n",
            "x_re:\n",
            "  The issue of the euro is no an issue in the fellow citizens , it is , in their hands , one of the European Union &apos;s most important\n",
            "0.7931034482758621\n",
            "y:\n",
            "  L&apos; affaire de l&apos; euro n&apos; est pas une petite affaire pour nos concitoyens : c&apos; est , entre leurs mains , un des biens les plus prcieux de\n",
            "y_re:\n",
            "  L&apos; anne de l&apos; euro n&apos; est pas une grande fois pour nos concitoyens , c&apos; est que sans les mains , un des\n",
            "0.642782940702586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "hcsgFtuxcD2f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        },
        "outputId": "6b4c4fd0-67b7-40ca-a432-639958593351"
      },
      "cell_type": "code",
      "source": [
        "output_sentence(78,x_de,y_de)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  Since last year , however , since the opening of the debate on the accession of Lithuania , we have had the impression that it has become a debate\n",
            "x_re:\n",
            "  In that year , however , and the outcome of the debate on the accession of Bulgaria , we have had the impression that it has been a problem\n",
            "0.7586206896551724\n",
            "y:\n",
            "  Pourtant , depuis l&apos; anne dernire , depuis l&apos; ouverture du dbat sur l&apos; adhsion de la Lituanie , nous avons l&apos; impression qu&apos; il est devenu un dbat\n",
            "y_re:\n",
            "  Mais , aprs l&apos; anne dernire , aprs l&apos; anne du dbat sur l&apos; adhsion de la Bulgarie , nous avons l&apos; impression qu&apos; il est toujours un problme\n",
            "0.7586206896551724\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "mDFnWZP5F3OC",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Beam Search"
      ]
    },
    {
      "metadata": {
        "id": "O1_JCr1kabdc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3858
        },
        "outputId": "c9348f80-e53b-42be-fd85-29d58f04b6a0"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "x_bleu = []\n",
        "y_bleu = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  saver = tf.train.Saver()\n",
        "  saver.restore(sess, path1)\n",
        "  \n",
        "  for idd in range(20):\n",
        "  \n",
        "    beam_size = 30\n",
        "    decode_len = 30\n",
        "\n",
        "    eos_id = en_word_to_id['eos']\n",
        "    eos_prob = -float('Inf')\n",
        "\n",
        "    x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "    y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "    x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "    y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "    x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "    y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "    #########################################################\n",
        "    x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "    x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "    x_score = np.zeros((beam_size))\n",
        "\n",
        "    y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "    y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "    y_score = np.zeros((beam_size))\n",
        "\n",
        "    #########################################################\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)\n",
        "  \n",
        "    en_le, fr_le = output_sentence(idd,x_de,y_de)\n",
        "    x_bleu.append(en_le)\n",
        "    y_bleu.append(fr_le)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  In fact , quite the opposite is the case .\n",
            "x_re:\n",
            "  In fact , , the opposite for the case .\n",
            "0.5773502691896258\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  En ralit , c&apos; est mme plutt l&apos; inverse .\n",
            "y_re:\n",
            "  En ralit , c&apos; est tout dans l&apos; inverse .\n",
            "0.5253819788848316\n",
            "x:\n",
            "  It reflects the fact that the new Member States are catching up with the developed economies of the old Union .\n",
            "x_re:\n",
            "  It is the fact that the European Member States are set up with the respective regions of the European Union .\n",
            "0.2476165058078653\n",
            "y:\n",
            "  Cela montre que les nouveaux tats membres rattrapent les conomies dveloppes de la &quot; vieille &quot; Union .\n",
            "y_re:\n",
            "  Cela montre que les les tats membres et des conomies importantes de la la et &quot; europenne .\n",
            "0.20105373454060027\n",
            "x:\n",
            "  Thirdly , I would like to draw your attention to the fact that the definition of stability applied by the Commission and the European Central Bank in convergence reports\n",
            "x_re:\n",
            "  Lastly , I would like to draw your attention to the fact that the creation of transparency . by the EU and the European Union Bank in Europe ,\n",
            "0.5157115350821027\n",
            "y:\n",
            "  Troisimement , je voudrais attirer votre attention sur le fait que la dfinition de la stabilit des prix applique par la Commission et la Banque centrale europenne dans les\n",
            "y_re:\n",
            "  Enfin , je voudrais attirer votre attention sur le fait que la modification de la directive des prix et par la Commission , la Banque centrale europenne , les\n",
            "0.5272877650845328\n",
            "x:\n",
            "  Meanwhile , the Treaty text only contains one definition of price stability and we cannot have two different interpretations of this term .\n",
            "x_re:\n",
            "  Furthermore , the this , , is a text of financial regulation , we should be a different level of the report .\n",
            "0.36519346563474564\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Toujours est @-@ il que le texte du Trait ne contient qu&apos; une seule dfinition de la stabilit des prix et que nous ne pouvons avoir deux interprtations diffrentes\n",
            "y_re:\n",
            "  Il est @-@ il que le texte du rglement ne est qu&apos; une la efficace de la politique des\n",
            "0.22844993462524102\n",
            "x:\n",
            "  The inflation criteria currently means that some of the new Member States may not be able to join the eurozone for many years .\n",
            "x_re:\n",
            "  The financing of , means that some of the new Member States should not be able to use the euro in the years .\n",
            "0.42662196623863147\n",
            "y:\n",
            "  Le critre de l&apos; inflation fait que certains des nouveaux tats membres ne pourront peut @-@ tre pas rejoindre la zone euro avant de nombreuses annes .\n",
            "y_re:\n",
            "  Le rapport de l&apos; Europe montre que N les nouveaux tats membres ne peuvent peut @-@ tre pas pas la zone euro et de nombreuses annes .\n",
            "0.3265250050211476\n",
            "x:\n",
            "  This permanently divides the Member States into two categories , namely the eurozone countries and those which remain outside the eurozone .\n",
            "x_re:\n",
            "  It also on the Member States in the years , including the EU countries and those which are outside the EU .\n",
            "0.18728674627858763\n",
            "y:\n",
            "  Cela divise en permanence les tats membres en deux catgories : ceux qui font partie de la zone euro et ceux qui n&apos; en font pas partie .\n",
            "y_re:\n",
            "  Cela sont en particulier les tats membres de plusieurs parties de ceux qui en partie de la zone euro et\n",
            "0.23239847393542165\n",
            "x:\n",
            "  This situation threatens the cohesion of the Union and is at odds with the spirit of the Treaty .\n",
            "x_re:\n",
            "  This situation is the rights of the Union , is at stake with the spirit of the Treaty .\n",
            "0.44353395455270217\n",
            "y:\n",
            "  Cette situation menace la cohsion de l&apos; Union et est contraire  l&apos; esprit du Trait .\n",
            "y_re:\n",
            "  Cette situation est la politique de l&apos; Union , est essentielle  l&apos; esprit du Conseil .\n",
            "0.24797984721910182\n",
            "x:\n",
            "  The convergence criteria were drawn up N years ago , in entirely different circumstances .\n",
            "x_re:\n",
            "  The key criteria are drawn up N years ago , not a different case .\n",
            "0.36787632499277756\n",
            "y:\n",
            "  Les critres de convergence ont t dfinis il y a N ans , dans des circonstances totalement diffrentes .\n",
            "y_re:\n",
            "  Les objectifs de coopration ont t fixs il y a N ans , dans les circonstances trs diffrentes .\n",
            "0.39392473548207035\n",
            "x:\n",
            "  They should be adapted to the current situation .\n",
            "x_re:\n",
            "  They must be able to the current situation .\n",
            "0.4854917717073234\n",
            "y:\n",
            "  Ils devraient tre adapts  la situation actuelle .\n",
            "y_re:\n",
            "  Ils doivent tre accorde  la situation actuelle .\n",
            "0.4854917717073234\n",
            "x:\n",
            "  I appeal for an in @-@ depth debate on this subject .\n",
            "x_re:\n",
            "  I call for this in this depth debate on this subject .\n",
            "0.4617366309441026\n",
            "y:\n",
            "  Je demande qu&apos; un dbat approfondi soit men sur ce sujet .\n",
            "y_re:\n",
            "  Je demande qu&apos; un dbat europen a positif sur ce sujet .\n",
            "0.53107253497887\n",
            "x:\n",
            "  on behalf of the ALDE Group . - ( IT ) Mr President , ladies and gentlemen , first of all I would like to thank Mr Langen ,\n",
            "x_re:\n",
            "  on behalf of the ALDE Group . - ( DE ) Mr President , ladies and gentlemen , first of all I would like to thank Mr Karas Mr\n",
            "0.833078701050083\n",
            "y:\n",
            "  au nom du groupe ALDE . - ( IT ) Monsieur le Prsident , Mesdames et Messieurs , tout d&apos; abord , je voudrais remercier M . Langen ,\n",
            "y_re:\n",
            "  au nom du groupe ALDE . - ( EN ) Monsieur le Prsident , Mesdames et Messieurs , Messieurs d&apos; abord , je voudrais remercier M . Karas pourquoi\n",
            "0.7359287727151013\n",
            "x:\n",
            "  In fact , on N May N the Commission adopted its convergence report on the criteria for Malta and Cyprus to join the single currency and on N May\n",
            "x_re:\n",
            "  In fact , on N April N the Commission adopted the Council report on the Council for Iceland and Spain to join the single directive , on N May\n",
            "0.3903162887399135\n",
            "y:\n",
            "  En effet , le N mai N , la Commission a adopt son rapport de convergence sur les critres relatifs  la capacit de Malte et Chypre  adopter\n",
            "y_re:\n",
            "  En effet , le N mai N , la Commission a adopt son rapport de rsolution sur les accords relatifs  la Bulgarie de Schengen et N  examiner\n",
            "0.5826372195744199\n",
            "x:\n",
            "  Because of the short time available Parliament was not able to make a thorough assessment of the situation of these countries and the reports put forward by the Commission\n",
            "x_re:\n",
            "  Therefore , the last time , it is not able to make a comprehensive assessment of the situation of the countries and the Parliament have forward by the majority\n",
            "0.34315351162206226\n",
            "y:\n",
            "  Vu le peu de temps dont il disposait , le Parlement n&apos; a pas pu valuer de manire approfondie la situation de ces pays et les rapports proposs par\n",
            "y_re:\n",
            "  Dans le cas de temps , il sont , le Parlement n&apos; a pas pas lors une manire entre la situation de ces pays et les ngociations prises au\n",
            "0.39307487936445573\n",
            "x:\n",
            "  I would nonetheless express a favourable opinion on the adoption by Cyprus and Malta of the single currency , since the convergence criteria have been met .\n",
            "x_re:\n",
            "  I would also express a special opinion on the agreement of N and N , the European market , and the EU criteria have been met .\n",
            "0.22544215811932597\n",
            "y:\n",
            "  Je voudrais nanmoins exprimer un avis favorable quant  l&apos; adoption de la monnaie unique par Chypre et par Malte , puisque les critres de convergence ont t remplis\n",
            "y_re:\n",
            "  Je voudrais galement exprimer un engagement positif quant  l&apos; adoption de la plateforme N de N , de N , dans les critres de scurit ont t ralises\n",
            "0.2547296653715806\n",
            "x:\n",
            "  In fact , with regard to Malta , in the last N months the inflation rate has been N .N % , which is less than the reference value\n",
            "x_re:\n",
            "  In fact , with regard to N , in the last N years the N , has reached N .N % , which is more than the economic crisis\n",
            "0.4695966835778606\n",
            "y:\n",
            "  En effet , en ce qui concerne Malte , le taux d&apos; inflation est rest  N , N % au cours des douze derniers mois , ce qui\n",
            "y_re:\n",
            "  En effet , en ce qui concerne N , le taux d&apos; Irlande est N  N de N % au cours des N derniers mois , ce qui\n",
            "0.5531136683830926\n",
            "x:\n",
            "  The excessive deficit has been corrected through a sustained reduction in the budget deficit under the threshold of N % of GDP and the debt rate is falling ,\n",
            "x_re:\n",
            "  The average crisis has been made for a new reduction in the budget deficit by the period of N % of GDP and the financial rate is increasing ,\n",
            "0.37704209027116137\n",
            "y:\n",
            "  Le dficit excessif a t corrig grce  une rduction durable du dficit budgtaire sous le seuil des N % du PIB et le taux de la dette est\n",
            "y_re:\n",
            "  Le march budgtaire a t cr ,  une rduction significative du dficit budgtaire aprs le seuil des N % du PIB et le taux de la population mais\n",
            "0.5023049672447087\n",
            "x:\n",
            "  Until March N Malta &apos;s average long @-@ term interest rate was at N .N % , which is below the reference value of N .N % .\n",
            "x_re:\n",
            "  Until on N N &apos;s a long @-@ term growth , , at N .N % , it is not the biggest value of N .N % .\n",
            "0.36115429274024463\n",
            "y:\n",
            "  Jusqu&apos; en mars N , le taux d&apos; intrt  long terme de Malte s&apos; tablissait  N , N % , ce qui se situe nettement sous la\n",
            "y_re:\n",
            "  Nous en la N , le nombre d&apos; aide  long terme de N s&apos; lve  N , N % , ce qui se t pas sous du\n",
            "0.4278996058277188\n",
            "x:\n",
            "  The Maltese economy is highly integrated into the European Union and the balance of payments deficit fell to N .N % in N , partly thanks to direct foreign\n",
            "x_re:\n",
            "  The EU economy is very important in the European Union and the number of the and amounting to N .N % in N , and due to the economic\n",
            "0.35423985843000033\n",
            "y:\n",
            "  L&apos; conomie maltaise est largement intgre  celle de l&apos; Union europenne et le dficit de la balance des oprations courantes est tomb  N , N % en\n",
            "y_re:\n",
            "  L&apos; conomie europenne est particulirement importante  celle de l&apos; Union europenne , le budget de la production des missions N est N  N , N % qui\n",
            "0.3365737931901263\n",
            "x:\n",
            "  With regard to Cyprus , in the last N months the inflation rate has been N % , which is lower than the reference value of N % .\n",
            "x_re:\n",
            "  In regard to N , in the last N years the N , has been N % , which are more than the average cost of N % .\n",
            "0.4035660856614615\n",
            "y:\n",
            "  En ce qui concerne Chypre , le taux d&apos; inflation s&apos; est tabli  N % au cours des douze derniers mois , ce qui est nettement infrieur \n",
            "y_re:\n",
            "  En ce qui concerne N , le taux d&apos; emploi s&apos; est N de N % au cours des N derniers mois , ce qui est nettement infrieur puisque\n",
            "0.5399123546011343\n",
            "x:\n",
            "  For N the forecasts made by the Commission in spring this year are for an unchanged deficit of N .N % of GDP .\n",
            "x_re:\n",
            "  For N , results made by the Commission in December this year is for an average rate of N .N % of GDP .\n",
            "0.4581091965946312\n",
            "y:\n",
            "  Pour N , les prvisions ralises par la Commission au printemps de cette anne ont annonc un dficit inchang de N , N % du PIB .\n",
            "y_re:\n",
            "  Pour N , les rsultats ralises par la Commission au cours de cette anne a t un dficit N de N , N % du PIB .\n",
            "0.5403356450597102\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pfDgvkSNaS1I",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5707
        },
        "outputId": "67ef998c-ed48-4b7f-821c-896788a57fca"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "x_bleu = []\n",
        "y_bleu = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  saver = tf.train.Saver()\n",
        "  saver.restore(sess, path1)\n",
        "  \n",
        "  for idd in range(20,50):\n",
        "  \n",
        "    beam_size = 30\n",
        "    decode_len = 30\n",
        "\n",
        "    eos_id = en_word_to_id['eos']\n",
        "    eos_prob = -float('Inf')\n",
        "\n",
        "    x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "    y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "    x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "    y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "    x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "    y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "    #########################################################\n",
        "    x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "    x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "    x_score = np.zeros((beam_size))\n",
        "\n",
        "    y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "    y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "    y_score = np.zeros((beam_size))\n",
        "\n",
        "    #########################################################\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)\n",
        "  \n",
        "    en_le, fr_le = output_sentence(idd,x_de,y_de)\n",
        "    x_bleu.append(en_le)\n",
        "    y_bleu.append(fr_le)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  Since its entry into ERM II , the European exchange rate mechanism , the Cypriot pound has been exchanged in a stable manner at a satisfactory exchange rate .\n",
            "x_re:\n",
            "  In the entry into force II , the European exchange of &apos; , the European report has been used in a comprehensive , in a significant impact , .\n",
            "0.18852104513558068\n",
            "y:\n",
            "  Depuis son entre dans le mcanisme de change II , la livre chypriote a t change de manire relativement stable  un taux de change satisfaisant .\n",
            "y_re:\n",
            "  Pour son qui , le cadre de Ble II , la &quot; &quot; a t cr de manire plus &quot;  un moment de risque , .\n",
            "0.25130737267754294\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  In the last N months the average long @-@ term interest rate has been N .N % , which is below the reference value of N .N % .\n",
            "x_re:\n",
            "  In the past N years the more part @-@ term amount , has been N .N % , which is not the biggest value of N .N % may\n",
            "0.4117230572911877\n",
            "y:\n",
            "  Le taux d&apos; intrt  long terme s&apos; tablit  N , N % pour les douze derniers mois , ce qui se situe nettement sous la valeur de\n",
            "y_re:\n",
            "  Le nombre d&apos; ajustement  long terme qui N de N de N % pour les N tats membres , ce qui se trouve , dans la N aux\n",
            "0.2210097542933647\n",
            "x:\n",
            "  The Cypriot economy is highly integrated into the European Union &apos;s economy .\n",
            "x_re:\n",
            "  The European economy is very open in the European Union &apos;s economy .\n",
            "0.44082318755867267\n",
            "y:\n",
            "  L&apos; conomie chypriote est troitement intgre dans celle de l&apos; Union europenne .\n",
            "y_re:\n",
            "  L&apos; conomie europenne est devenue importante dans celle de l&apos; Union europenne .\n",
            "0.5344445934790545\n",
            "x:\n",
            "  The Commission considers that economic integration has been achieved , despite the increase in the balance of payments deficit .\n",
            "x_re:\n",
            "  The Commission considers the Roma inclusion has been made , in the increase in the number of financial , .\n",
            "0.2076047003130265\n",
            "y:\n",
            "  La Commission est d&apos; avis qu&apos; en dpit de l&apos; augmentation du dficit de la balance des paiements courants , l&apos; intgration conomique est un fait .\n",
            "y_re:\n",
            "  La Commission est d&apos; importance qu&apos; en dpit de l&apos; augmentation du problme de la hausse des budgets financiers , l&apos; Europe europenne est un fait .\n",
            "0.435277137794151\n",
            "x:\n",
            "  In order to avoid problems arising again in the future on procedural timetables , it is necessary to improve the method for consulting the European Parliament , by setting\n",
            "x_re:\n",
            "  In order to avoid problems achieved , in the future and the regulation , it is necessary to implement the mechanism for all the European Union , the setting\n",
            "0.31831184071993224\n",
            "y:\n",
            "  Afin d&apos; viter que des problmes surviennent encore  l&apos; avenir concernant le calendrier de procdure , il y a lieu d&apos; amliorer la mthode de consultation du Parlement\n",
            "y_re:\n",
            "  Afin d&apos; viter que les problmes sont ,  l&apos; avenir , le cadre de procdure , il y a lieu d&apos; amliorer la proposition de dcision du projet\n",
            "0.4615664305278026\n",
            "x:\n",
            "  This will make it possible to have the time necessary to make a proper assessment of the proposals by the Commission and the European Central Bank .\n",
            "x_re:\n",
            "  This will make it possible to be the measures and to make a necessary assessment of the proposals of the Commission and the European Investment Bank .\n",
            "0.46563348805256366\n",
            "y:\n",
            "  Cela permettra de disposer du temps ncessaire pour valuer de faon adquate les propositions de la Commission et de la Banque centrale europenne .\n",
            "y_re:\n",
            "  Cela permettra de disposer le temps , pour valuer de manire et des propositions de la Commission et de la Banque centrale europenne .\n",
            "0.5894159589207006\n",
            "x:\n",
            "  On this point , I believe that the exchange of views with Commissioner Almunia and the reply by Mr Barroso , President of the Commission , to the letter\n",
            "x_re:\n",
            "  In this point , I believe that the work of views with Commissioner , and the rapporteur of Mr Barroso , President , the Commission , to the question\n",
            "0.5065691202419944\n",
            "y:\n",
            "   cet gard , je crois que les changes de vues avec le commissaire Almunia et la rponse du prsident de la Commission , M . Barroso , \n",
            "y_re:\n",
            "   cet gard , je pense que les changes de vues avec le commissaire Barnier , la Commission du prsident de la Commission , M . Barroso , le\n",
            "0.6690849531405022\n",
            "x:\n",
            "  The Commission ought to notify Parliament very early of all requests for convergence reports submitted by the Member States and should decide together with Parliament and the Council on\n",
            "x_re:\n",
            "  The Commission intends to inform this , and of the proposals for these proposals tabled by the Member States , to be together with Parliament and the Council on\n",
            "0.31963619792524467\n",
            "y:\n",
            "  La Commission devrait notifier le plus tt possible au Parlement toutes les demandes de rapports de convergence prsentes par les tats membres et se concerter avec le Parlement et\n",
            "y_re:\n",
            "  La Commission doit demander le plus vite possible au Conseil . les demandes de demandes de travail et par les tats membres , se participer avec le Parlement et\n",
            "0.2492484032921466\n",
            "x:\n",
            "  on behalf of the UEN Group . - ( PL ) Mr President , by taking the floor in the debate on expanding the eurozone to include Cyprus and\n",
            "x_re:\n",
            "  on behalf of the ALDE Group . - ( PL ) Mr President , , taking the floor in the debate on joining the EU to increase this the\n",
            "0.5637996415430047\n",
            "y:\n",
            "  au nom du groupe UEN . - ( PL ) Monsieur le Prsident , en prenant la parole dans ce dbat sur l&apos; largissement de la zone euro \n",
            "y_re:\n",
            "  au nom du groupe ALDE . - ( EN ) Monsieur le Prsident , en N la parole de ce dbat sur l&apos; largissement de la zone euro ni\n",
            "0.5784954323743581\n",
            "x:\n",
            "  First of all , the Commission is once again reminding us of the need for the new Member States to meet all the Maastricht criteria before joining the eurozone\n",
            "x_re:\n",
            "  First , all , the Commission is once again for us , the need for the European Member States to be all the same criteria and joining the EU\n",
            "0.3720337795006881\n",
            "y:\n",
            "  Premirement , la Commission nous rappelle encore une fois la ncessit pour les nouveaux tats membres de remplir tous les critres de Maastricht avant de pouvoir rejoindre la zone\n",
            "y_re:\n",
            "  Premirement , la Commission , a , une fois de parole pour les les tats membres de respecter tous les critres de Copenhague et de faire dans la voix\n",
            "0.23368936312986366\n",
            "x:\n",
            "  At the same time , it is turning a blind eye to the fact that , when the euro was introduced , many of the old Member States did\n",
            "x_re:\n",
            "  At the same time , it is not a blind eye to the fact that , and the euro is made , many of the European Member States -\n",
            "0.5543226672204017\n",
            "y:\n",
            "  En mme temps , elle passe sous silence le fait qu&apos; au moment de l&apos; introduction de l&apos; euro , une grande partie des anciens tats membres ne remplissaient\n",
            "y_re:\n",
            "  En mme temps , elle a sous silence le fait qu&apos; au moment de l&apos; ensemble de l&apos; euro , une nouvelle partie des deux tats membres ne dpendent\n",
            "0.5784954323743581\n",
            "x:\n",
            "  Secondly , despite the revision of the Stability and Growth Pact , the Commission has remained lenient towards the biggest Member States in terms of their adherence to the\n",
            "x_re:\n",
            "  Secondly , in the revision of the Stability and Growth Pact , the Commission has been adopted on the European Member States in terms of the response to the\n",
            "0.5534502966615891\n",
            "y:\n",
            "  Deuximement , malgr la rvision du pacte de stabilit et de croissance , la Commission est reste trs indulgente  l&apos; gard des grands tats membres en ce qui\n",
            "y_re:\n",
            "  Deuximement , malgr la rvision du trait de stabilit et de croissance , la Commission est reste trs importante  l&apos; gard des deux tats membres en ce ont\n",
            "0.671683390199399\n",
            "x:\n",
            "  In the past , the Commission has tolerated , and appears to still tolerate , significant budget deficits and a level of public debt in particular , which often\n",
            "x_re:\n",
            "  In the past , the Commission has done , and is to be is a a new , in a crisis of public finances in particular , which also\n",
            "0.33205499938061867\n",
            "y:\n",
            "  Dans le pass , la Commission a tolr - et il semble qu&apos; elle tolre encore - des dficits budgtaires importants et , en particulier , un niveau de\n",
            "y_re:\n",
            "  Dans le pass , la Commission a t , , il semble qu&apos; elle est encore , des dficits trs importantes et , en particulier , un niveau au\n",
            "0.5023049672447087\n",
            "x:\n",
            "  Statistics confirm this .\n",
            "x_re:\n",
            "  We on this .\n",
            "0.6389431042462724\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Les statistiques le confirment .\n",
            "y_re:\n",
            "  Les tudes le fait .\n",
            "0.8801117367933934\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  In N , public debt in the countries of the old European Union was as high as N .N % of GDP and , in as many as half\n",
            "x_re:\n",
            "  In N , economic debt in the countries of the European European Union is is far for N .N % of GDP , , and as well , many\n",
            "0.3602421475072493\n",
            "y:\n",
            "  En N , la dette publique des pays de la &quot; vieille &quot; Union europenne s&apos; levait  N , N % du PIB et dans pas moins de\n",
            "y_re:\n",
            "  En N , la crise publique des travailleurs de la mer europenne et Union europenne s&apos; levait  N de N % du PIB , dans\n",
            "0.3765151846638952\n",
            "x:\n",
            "  Thirdly , the Commission &apos;s attitude towards countries wishing to join the eurozone varies greatly .\n",
            "x_re:\n",
            "  Thirdly , the Commission &apos;s work on women needs to join the EU are themselves .\n",
            "0.31535540524901323\n",
            "y:\n",
            "  Troisimement , l&apos; attitude de la Commission  l&apos; gard des pays qui souhaitent rejoindre la zone euro varie normment d&apos; un pays  l&apos; autre .\n",
            "y_re:\n",
            "  Enfin , l&apos; importance de la Commission  l&apos; gard des pays qui peuvent dans la zone qui sont\n",
            "0.3230640057306305\n",
            "x:\n",
            "  Very recently , Lithuania &apos;s application to join the zone was rejected , in spite of the fact that it had met the Maastricht criteria , and its inflation\n",
            "x_re:\n",
            "  It said , this &apos;s accession to join the directive and adopted , in spite of the fact that it has only the eligibility criteria , and the measures\n",
            "0.3384559194484391\n",
            "y:\n",
            "  Il y a peu , la demande d&apos; adhsion  la zone euro dpose par la Lituanie a t rejete , bien que celle @-@ ci remplissait les critres\n",
            "y_re:\n",
            "  Il y a dit , la demande d&apos; adhsion  la zone Europe N par la Commission a t N , et que celle @-@ ci dans les programmes\n",
            "0.4085187432723087\n",
            "x:\n",
            "  Therefore , the Commission &apos;s quick approval of Cyprus and Malta &apos;s membership of the eurozone might seem surprising in view of the fact that both countries &apos; public\n",
            "x_re:\n",
            "  Therefore , the Commission &apos;s its aspects of Croatia , Bulgaria &apos;s accession of the EU , be included in view of the fact that the countries &apos; public\n",
            "0.3508772012923553\n",
            "y:\n",
            "  Par consquent , l&apos; approbation rapide par la Commission de l&apos; adhsion de Chypre et de Malte  la zone euro peut surprendre , puisque la dette publique des\n",
            "y_re:\n",
            "  Par consquent , l&apos; absence et par la Commission de l&apos; adhsion de N et de N  la zone euro , N , si la majorit publique soient\n",
            "0.39615805576005414\n",
            "x:\n",
            "  In N , Cyprus &apos; public debt amounted to as much as N .N % and Malta &apos;s debt stood at N .N % of GDP .\n",
            "x_re:\n",
            "  In N , the and public workers amounted to as much as N .N % and N , N is at N .N % of GDP .\n",
            "0.560597610169143\n",
            "y:\n",
            "  En N , la dette publique de Chypre s&apos; levait  N , N % et celle de Malte  N , N % du PIB .\n",
            "y_re:\n",
            "  En N , la crise publique de N s&apos; lve  N , N % et N de N  N de N % du PIB .\n",
            "0.4406189608707621\n",
            "x:\n",
            "  Moreover , both countries are finding it difficult to provide Eurostat with statistics pertaining to their financial situation .\n",
            "x_re:\n",
            "  Furthermore , all countries are some , need to make them with information relating to the financial crisis .\n",
            "0.391080327529236\n",
            "y:\n",
            "  De plus , les deux pays rechignent  transmettre  Eurostat des statistiques relatives  leur situation financire .\n",
            "y_re:\n",
            "  De plus , les deux tats vont  ce  tous des informations relatives  la crise financire .\n",
            "0.26104909033290696\n",
            "x:\n",
            "  In spite of the doubts I have just expressed , I would still like to congratulate both Cyprus and Malta on joining the eurozone .\n",
            "x_re:\n",
            "  In spite of the report , have also mentioned , I would also like to emphasise all countries , this on joining the EU .\n",
            "0.19599458003391504\n",
            "y:\n",
            "  Malgr les rserves que je viens d&apos; exprimer , je voudrais tout de mme fliciter Chypre et Malte pour leur adhsion  la zone euro .\n",
            "y_re:\n",
            "  Dans les reprsentants , je rapport d&apos; accord , je voudrais tout de faire avec tout , , pour son adhsion  la zone euro .\n",
            "0.3387562718376491\n",
            "x:\n",
            "  on behalf of the Verts / ALE Group . - ( DE ) Mr President , Mr President @-@ in @-@ Office , Commissioner , ladies and gentlemen ,\n",
            "x_re:\n",
            "  on behalf of the Verts / ALE Group . - ( DE ) Mr President , Mr President , in @-@ Office , President , ladies and gentlemen but\n",
            "0.7801342480988807\n",
            "y:\n",
            "  au nom du groupe des Verts / ALE . - ( DE ) Monsieur le Prsident , Monsieur le Prsident en exercice du Conseil , Monsieur le Commissaire ,\n",
            "y_re:\n",
            "  au nom du groupe GUE Verts / ALE . - ( DE ) Monsieur le Prsident , Monsieur le Prsident en exercice du Conseil , Monsieur le Commissaire Tajani\n",
            "0.8696398662122882\n",
            "x:\n",
            "  It is therefore logical to enlarge the euro zone to include both countries .\n",
            "x_re:\n",
            "  It is therefore necessary to use the euro area to be all countries .\n",
            "0.35831291876413535\n",
            "y:\n",
            "  Il est donc logique d&apos; largir la zone euro  ces deux pays .\n",
            "y_re:\n",
            "  Il est donc ncessaire d&apos; tendre la zone euro  des deux pays .\n",
            "0.33649324423301513\n",
            "x:\n",
            "  If it were up to me and my group , more Member States of the European Union would be welcome to join the euro zone , provided that they\n",
            "x_re:\n",
            "  If it is up to me , my colleagues , the Member States of the European Union should be able to join the euro area , so that we\n",
            "0.3224199399071415\n",
            "y:\n",
            "  Si cela n&apos; appartenait qu&apos;  moi et  mon groupe , d&apos; autres tats membres seraient invits  rejoindre la zone euro ,  condition bien sr qu&apos;\n",
            "y_re:\n",
            "  Si cela n&apos; est qu&apos;  moi ,  mon groupe , d&apos; autres tats membres sont invits  tous la l&apos; euro ,  prsent , . il\n",
            "0.38558430772337343\n",
            "x:\n",
            "  That can only be good for the European Union .\n",
            "x_re:\n",
            "  There can not be good for the European Union .\n",
            "0.6606328636027614\n",
            "y:\n",
            "  Cela ne peut qu&apos; tre positif pour l&apos; Union europenne .\n",
            "y_re:\n",
            "  Cela ne peut @-@ tre clair de l&apos; Union europenne .\n",
            "0.3508439695638686\n",
            "x:\n",
            "  This event is , however , tinged with sadness and I should like to explain why : unfortunately , in Cyprus an opportunity has been missed to involve both\n",
            "x_re:\n",
            "  This agreement is , however , agree with us , I would like to see what , unfortunately , in a an opportunity has been done to do all\n",
            "0.22761431518822883\n",
            "y:\n",
            "  Cet vnement est cependant teint de tristesse et je voudrais vous expliquer pourquoi : malheureusement , Chypre a laiss passer l&apos; occasion de faire entrer les deux parties de\n",
            "y_re:\n",
            "  Cet accord est encore . de temps , je voudrais vous remercier , , , , nous a pu , l&apos; Europe de faire , des deux parties mais\n",
            "0.22068402053531555\n",
            "x:\n",
            "  That is very regrettable , because as a result the wall in Cyprus will be reinforced rather than weakened .\n",
            "x_re:\n",
            "  This is very clear , but as a result of power in Europe will be treated more than ever .\n",
            "0.28320384389628483\n",
            "y:\n",
            "  C&apos; est vraiment regrettable , car le mur de Chypre s&apos; en trouve renforc au lieu d&apos; tre affaibli .\n",
            "y_re:\n",
            "  C&apos; est trs vrai , car le dbut de Lisbonne s&apos; se trouve &quot; au service d&apos; tre utilis .\n",
            "0.28943182557959846\n",
            "x:\n",
            "  on behalf of the ITS Group . - ( DE ) Mr President , ladies and gentlemen , economic experts seem to agree that the euro zone is enjoying\n",
            "x_re:\n",
            "  on behalf of the ALDE Group . - ( DE ) Mr President , ladies and gentlemen , economic , is to say that the euro area is better\n",
            "0.6013051538056498\n",
            "y:\n",
            "  au nom du groupe ITS . - ( DE ) Monsieur le Prsident , Mesdames et Messieurs , les experts conomiques semblent s&apos; accorder  dire que la zone\n",
            "y_re:\n",
            "  au nom du groupe EFD . - ( DE ) Monsieur le Prsident , Mesdames et Messieurs , les dputs qui ont s&apos; exprimer  dire de la un\n",
            "0.5761982509413803\n",
            "x:\n",
            "  They are even talking about a sustained rise in employment .\n",
            "x_re:\n",
            "  They are now talking about a significant way in Europe .\n",
            "0.3816330911371337\n",
            "y:\n",
            "  Ils parlent mme d&apos; une hausse soutenue de l&apos; emploi .\n",
            "y_re:\n",
            "  Ils sont aussi d&apos; une solution et de l&apos; emploi .\n",
            "0.28997844147152074\n",
            "x:\n",
            "  It is my belief , however , that all too often it is only a rise in part @-@ time employment - or the new forms of work ,\n",
            "x_re:\n",
            "  We is my case , however , , all , , it is not a problem in part @-@ time age , and the new forms of work ,\n",
            "0.3253118880579605\n",
            "y:\n",
            "  Cependant , je pense que , trop souvent , ce n&apos; est qu&apos; une hausse de l&apos; emploi  temps partiel - ou des nouvelles formes du travail ,\n",
            "y_re:\n",
            "  Cependant , je pense que , plus , , ce n&apos; est qu&apos; une hausse de l&apos; UE  cours partiel , et les autres formes du travail ayant\n",
            "0.4942654936814536\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fJlFFY9ibKEP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4818
        },
        "outputId": "e2666bb3-e435-4d68-eb54-b9dfb257a8fb"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "x_bleu = []\n",
        "y_bleu = []\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "  saver = tf.train.Saver()\n",
        "  saver.restore(sess, path1)\n",
        "  \n",
        "  for idd in range(50,75):\n",
        "  \n",
        "    beam_size = 30\n",
        "    decode_len = 30\n",
        "\n",
        "    eos_id = en_word_to_id['eos']\n",
        "    eos_prob = -float('Inf')\n",
        "\n",
        "    x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "    y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "    x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "    y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "    x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "    y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "    #########################################################\n",
        "    x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "    x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "    x_score = np.zeros((beam_size))\n",
        "\n",
        "    y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "    y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "    y_score = np.zeros((beam_size))\n",
        "\n",
        "    #########################################################\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)\n",
        "  \n",
        "    en_le, fr_le = output_sentence(idd,x_de,y_de)\n",
        "    x_bleu.append(en_le)\n",
        "    y_bleu.append(fr_le)"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  Under these conditions the dwindling birth rate should come as no surprise .\n",
            "x_re:\n",
            "  In these reasons , the production rate will be as no doubt .\n",
            "0.4428500142691474\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 3-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  Dans ces conditions , la diminution du taux de natalit ne devrait pas nous surprendre .\n",
            "y_re:\n",
            "  Dans ces informations , la hausse du taux de natalit ne peuvent pas nous pas .\n",
            "0.32774568052975916\n",
            "x:\n",
            "  You cannot feed a family on a <OOV> !\n",
            "x_re:\n",
            "  We cannot afford a choice and the country .\n",
            "0.6865890479690392\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 2-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "y:\n",
            "  On ne peut pas nourrir une famille avec un <OOV> @-@ <OOV> !\n",
            "y_re:\n",
            "  On ne peut pas faire la dmocratie et le &quot; @-@ &quot; .\n",
            "0.2044800736021839\n",
            "x:\n",
            "  <OOV> about an upturn is a slap in the face for every single one of the millions @-@ strong army of jobless in the EU .\n",
            "x_re:\n",
            "  This is an issue is a whole in the right for a single one of the millions @-@ the inhabitants of people in the EU .\n",
            "0.3265516873506877\n",
            "y:\n",
            "  Cet enthousiasme  l&apos; gard d&apos; une soi @-@ disant reprise conomique est une gifle pour chacun des millions de chmeurs que compte l&apos; UE .\n",
            "y_re:\n",
            "  L&apos; objectif  l&apos; gard d&apos; une main @-@ la crise conomique est une Europe pour N des millions de chmeurs , les l&apos; Europe .\n",
            "0.29153831589857865\n",
            "x:\n",
            "  It <OOV> all of those millions of people who are labelled as the working poor .\n",
            "x_re:\n",
            "  It is true of the people of people who are known that the most groups .\n",
            "0.18207052811092134\n",
            "y:\n",
            "  C&apos; est se moquer ouvertement de ces millions de personnes qu&apos; on appelle les &quot; travailleurs pauvres &quot; .\n",
            "y_re:\n",
            "  C&apos; est le cas , de N millions de personnes , on sont les des plus vulnrables , .\n",
            "0.26104909033290696\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/translate/bleu_score.py:490: UserWarning: \n",
            "Corpus/Sentence contains 0 counts of 4-gram overlaps.\n",
            "BLEU scores might be undesirable; use SmoothingFunction().\n",
            "  warnings.warn(_msg)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  We are facing a problem that should not be underestimated : the fact that existing differences within the euro zone are widening , for example the southern European countries\n",
            "x_re:\n",
            "  We are facing a problem that will not be forgotten to the fact that the people within the euro area are subject , for all the EU European countries\n",
            "0.2950513807609976\n",
            "y:\n",
            "  Nous sommes confronts  un problme que nous ne devrions pas sous @-@ estimer . En effet , les diffrences existant au sein de la zone euro continuent \n",
            "y_re:\n",
            "  Nous sommes confronts  un problme de nous ne pouvons pas sous @-@ estimer . En effet , les normes , au sein de la zone euro , \n",
            "0.6160075734603386\n",
            "x:\n",
            "  The fact that the euro is not a universal remedy is proved by the example of the United Kingdom alone .\n",
            "x_re:\n",
            "  The fact that the EU is not a largest role is threatened by the end of the United States , .\n",
            "0.23632009599741205\n",
            "y:\n",
            "  Par exemple , les pays de l&apos; Europe du Sud voient continuellement leur position concurrentielle se dtriorer .\n",
            "y_re:\n",
            "  Par exemple , les pays de l&apos; Europe du travail et de la situation , se pays .\n",
            "0.4736415826167952\n",
            "x:\n",
            "  As we know , the British economy is in good shape , even without the euro .\n",
            "x_re:\n",
            "  So , know , the European Union is in its way , but not the euro .\n",
            "0.38538569180303145\n",
            "y:\n",
            "  L&apos; exemple du Royaume @-@ Uni suffit  prouver que l&apos; euro n&apos; est pas le remde universel .\n",
            "y_re:\n",
            "  L&apos; sr du Royaume @-@ Uni est  gagner de l&apos; euro n&apos; est pas un march unique .\n",
            "0.3411488281065382\n",
            "x:\n",
            "  The United Kingdom has the sixth largest manufacturing sector in the world and the eighth largest services sector .\n",
            "x_re:\n",
            "  In EU States , the largest European agricultural sector in the world and the European European services sector .\n",
            "0.33380800216772966\n",
            "y:\n",
            "  Comme nous le savons , l&apos; conomie britannique se porte bien , mme sans l&apos; euro . Le Royaume @-@ Uni possde le sixime plus grand secteur manufacturier du\n",
            "y_re:\n",
            "  par crit le savons , l&apos; conomie europenne se trouve , , N en l&apos; UE .\n",
            "0.12242090264096453\n",
            "x:\n",
            "  For this reason alone , no EU Member States should , in my opinion , be forced to introduce the euro .\n",
            "x_re:\n",
            "  In this reason , , the the Member State , , in my opinion , be able to use the euro .\n",
            "0.3127670021100431\n",
            "y:\n",
            "  Rien que pour cette raison , aucun tat membre de l&apos; UE ne devrait , selon moi , tre oblig d&apos; introduire l&apos; euro .\n",
            "y_re:\n",
            "  Tout , pour cette raison , le tat membre de l&apos; Union ne doit pas selon moi , tre favorable d&apos; utiliser l&apos; euro .\n",
            "0.3420941751050785\n",
            "x:\n",
            "  Fears about , for example , handing over sovereignty to the European Central Bank should be treated just as seriously as the price rises feared by consumers .\n",
            "x_re:\n",
            "  If , , for example , risk and assistance to the European Union Bank should be treated , as far as the price is caused by people .\n",
            "0.25383339228798274\n",
            "y:\n",
            "  Ainsi , les craintes relatives au transfert d&apos; une partie de la souverainet  la Banque centrale europenne devraient tre traites aussi srieusement que la peur de la hausse\n",
            "y_re:\n",
            "  Ainsi , les raisons , au cot d&apos; une partie de la pche  la Banque centrale europenne doivent tre seulement , , de la menace de la spculation\n",
            "0.31403822939177484\n",
            "x:\n",
            "  Many millions of citizens have in fact found the euro to have inflationary powers , since it has been a decisive factor in the increased price of everyday necessities\n",
            "x_re:\n",
            "  In all of women have in Europe on the euro to be new consequences , since it has been a decisive factor in the greater price of economic wealth\n",
            "0.40101980029129386\n",
            "y:\n",
            "  En ralit , des millions de citoyens ont estim que l&apos; euro avait des effets inflationnistes , puisqu&apos; il a constitu un facteur dcisif de la hausse des prix\n",
            "y_re:\n",
            "  En N , les millions de citoyens ont montr de l&apos; Europe a des consquences conomiques , il il a t un problme considrable de la hausse des\n",
            "0.19939198512186168\n",
            "x:\n",
            "  Neither official statistics showing the contrary nor image campaigns will do anything to change that .\n",
            "x_re:\n",
            "  Although all products and the right and work , to do everything to ensure that .\n",
            "0.37991784282579627\n",
            "y:\n",
            "  Ni les statistiques officielles prtendant le contraire , ni les campagnes visant  promouvoir l&apos; image de l&apos; euro ne pourront y changer quoi que ce soit .\n",
            "y_re:\n",
            "  Dans les pays qui et le gouvernement , et les consommateurs ,  tous l&apos; aide de l&apos; UE ne peut pas tre ce de ce soit . encore\n",
            "0.20130088157694537\n",
            "x:\n",
            "  Any EU country that is considering introducing the euro should as a general rule , in my view , always let the sovereign power , that is the people\n",
            "x_re:\n",
            "  In all authorities that is not &apos; the euro , not a good example , in my view , not given the the power , which is the people\n",
            "0.21202634869562412\n",
            "y:\n",
            "  Selon moi , tout pays europen qui envisage d&apos; introduire l&apos; euro devrait toujours , en rgle gnrale , laisser le pouvoir souverain , autrement dit les citoyens ,\n",
            "y_re:\n",
            "  Dans moi , ce pays fait qui est d&apos; viter l&apos; euro est , , en politique &quot; , &quot; le gouvernement europen , a dit les citoyens ,\n",
            "0.1375416583991847\n",
            "x:\n",
            "  ( EL ) Mr President , Commissioner , ladies and gentlemen , first of all I wish to thank the rapporteur , Mr Langen , and the members of\n",
            "x_re:\n",
            "  ( EL ) Mr President , Commissioner , ladies and gentlemen , first of all I wish to thank the rapporteur , Mr Karas , and the work of\n",
            "0.8409760644533056\n",
            "y:\n",
            "  ( EL ) Monsieur le Prsident , Monsieur le Commissaire , Mesdames et Messieurs , je voudrais tout d&apos; abord remercier le rapporteur , M . Langen , et\n",
            "y_re:\n",
            "  ( EL ) Monsieur le Prsident , Monsieur le Commissaire , Mesdames et Messieurs , je voudrais tout d&apos; abord remercier le rapporteur , M . Karas , si\n",
            "0.899160928885317\n",
            "x:\n",
            "  I also wish to thank Mr Almunia for the assistance he has given Cyprus all this time in achieving this objective .\n",
            "x_re:\n",
            "  I also like to thank Mr Barnier for the fact , has on us and this time in the this objective .\n",
            "0.4366835442847812\n",
            "y:\n",
            "  Je voudrais galement remercier M . Almunia pour l&apos; aide qu&apos; il a apporte  Chypre pendant tout ce temps en vue d&apos; atteindre cet objectif .\n",
            "y_re:\n",
            "  Je voudrais galement remercier M . Karas pour l&apos; ide qu&apos; il a accompli  Cancn dans tous ce rapport en vue d&apos; atteindre cet objectif .\n",
            "0.4644531655360721\n",
            "x:\n",
            "  Despite the tight timeframe which the Commission allowed Parliament and despite the reactions voiced , Mr Langen has demonstrated a huge sense of responsibility , has circumvented procedural issues\n",
            "x_re:\n",
            "  Despite a positive time , the Commission on Parliament , and the report . by Mr it has shown a great number of responsibility , has been a elements\n",
            "0.11839441618747341\n",
            "y:\n",
            "  Malgr le dlai trs court que la Commission a accord au Parlement et les ractions que cela a suscit , M . Langen a fait preuve d&apos; un grand\n",
            "y_re:\n",
            "  Dans le cadre de demande , la Commission a pris au Parlement et les travaux de ce a parl de M .\n",
            "0.136245584648786\n",
            "x:\n",
            "  My thanks once again to Mr Langen .\n",
            "x_re:\n",
            "  I again , again to Mr President .\n",
            "0.392814650900513\n",
            "y:\n",
            "  Encore merci , Monsieur Langen .\n",
            "y_re:\n",
            "  par demande , Monsieur , .\n",
            "0.5623413251903491\n",
            "x:\n",
            "  The European Parliament is today rewarding Cyprus &apos;s long @-@ term efforts to put its economy in order and converge with the indicators of the European Union .\n",
            "x_re:\n",
            "  The European Union is , on this &apos;s long @-@ term efforts to promote the economy in order and work with the implementation of the European Union . from\n",
            "0.4035660856614615\n",
            "y:\n",
            "  Aujourd&apos; hui , le Parlement europen rcompense les efforts  long terme consentis par Chypre pour mettre de l&apos; ordre dans son conomie et converger vers les indicateurs de\n",
            "y_re:\n",
            "  Aujourd&apos; hui , le Parlement europen et les efforts  long terme et de tout pour mettre de l&apos; Europe de son emploi , travers et les objectifs de\n",
            "0.38336182722382356\n",
            "x:\n",
            "  The European Parliament is giving the third institutional green light to the adoption of the euro in Cyprus .\n",
            "x_re:\n",
            "  The European Parliament is giving the European political comprehensive approach to the adoption of the EU in Lisbon .\n",
            "0.4580519369844352\n",
            "y:\n",
            "  Le Parlement europen donne le troisime feu vert institutionnel  l&apos; adoption de l&apos; euro par Chypre .\n",
            "y_re:\n",
            "  Le Parlement europen est le premier gouvernement @-@ &quot;  l&apos; adoption de l&apos; UE de N .\n",
            "0.28433291815307693\n",
            "x:\n",
            "  The adoption of the euro in Cyprus will bring the euro to the Middle East and forge a monetary link between Europe and Arabia via Cyprus .\n",
            "x_re:\n",
            "  The application of the EU in Zimbabwe to be the power in the Middle East and be a economic relationship between Europe and Europe and countries .\n",
            "0.15911783110981517\n",
            "y:\n",
            "  Cela permettra d&apos; introduire l&apos; euro au Moyen @-@ Orient et de forger un lien montaire entre l&apos; Europe et le monde arabe par l&apos; intermdiaire de Chypre .\n",
            "y_re:\n",
            "  Il signifie d&apos; une l&apos; accord au Proche @-@ Orient , de bien un rle stratgique , l&apos; Europe et le monde arabe par l&apos; UE de Chypre depuis\n",
            "0.29955728519520575\n",
            "x:\n",
            "  The monetary area of the European Union will extend from Brussels to the far end of the Eastern Mediterranean .\n",
            "x_re:\n",
            "  The financial area of the European Union will be from Brussels to the an people of the Eastern Partnership .\n",
            "0.435949382480739\n",
            "y:\n",
            "  L&apos; espace montaire de l&apos; Union europenne s&apos; tendra de Bruxelles  l&apos; extrmit de la Mditerrane orientale .\n",
            "y_re:\n",
            "  L&apos; espace europen de l&apos; Union europenne s&apos; est de Bruxelles  l&apos; Europe de la rgion , .\n",
            "0.37754323999245865\n",
            "x:\n",
            "  Cyprus has always had a strong economy ; even during the difficult years after the military invasion and the occupation of the north of Cyprus by Turkey and the\n",
            "x_re:\n",
            "  It has always had a strong economy , even during the recent years and the military crisis of the occupation of the region of Mexico and Bulgaria and the\n",
            "0.33617046856736504\n",
            "y:\n",
            "  Chypre a toujours eu une conomie forte . Mme pendant les annes difficiles de l&apos; invasion et de l&apos; occupation militaire de la partie nord de Chypre par la\n",
            "y_re:\n",
            "  Il a toujours eu une conomie forte .\n",
            "0.0609143320119521\n",
            "x:\n",
            "  Thus , everything that needs to be done by N January N on the part of Cyprus for full economy integration and convergence will have to be done and\n",
            "x_re:\n",
            "  Furthermore , all , needs to be done by N , N on the Group of Europe is the economic policy and Europe will have to be done and\n",
            "0.37821486365532614\n",
            "y:\n",
            "  Autrement dit , tout ce que doit faire Chypre d&apos; ici le Ner janvier N pour atteindre pleinement l&apos; intgration conomique et la convergence devra tre fait , et\n",
            "y_re:\n",
            "  Cela dit , tout , , nous faire N d&apos; ici le Ner mai N pour laquelle , l&apos; intgration europenne , la Commission doit tre pris , les\n",
            "0.15581581442301565\n",
            "x:\n",
            "  As far as Eurostat &apos;s comment on the provision of imperfect data is concerned , Mr Almunia &apos;s clarification satisfies us .\n",
            "x_re:\n",
            "  As far as this &apos;s vote on the application of the information are all , Mr Barroso &apos;s report on me .\n",
            "0.2626909894424158\n",
            "y:\n",
            "  En ce qui concerne le commentaire d&apos; Eurostat quant  la prsentation de donnes incompltes , l&apos; explication de M . Almunia nous satisfait .\n",
            "y_re:\n",
            "  En ce qui concerne le rapport d&apos; information ,  la suite de donnes disponibles , l&apos; aide de M .\n",
            "0.24339650136849453\n",
            "x:\n",
            "  I trust that by N January N the necessary basis will be in place for a proper resolution of the Cyprus problem and for Cyprus to reunite politically ,\n",
            "x_re:\n",
            "  I believe that that N , N the economic procedure will be in place for a clear solution of the European Union , for this to be people ,\n",
            "0.2203568379892381\n",
            "y:\n",
            "  Je suis confiant que , d&apos; ici le Ner janvier N , les bases ncessaires auront t mises en place afin de rsoudre de manire approprie le problme chypriote\n",
            "y_re:\n",
            "  Je suis convaincu que , d&apos; aprs le Ner mai N , les ngociations , ont t mises en place afin de rsoudre de manire et du niveau juridique\n",
            "0.39842406650170187\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "E71tUu9EF3OC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "eb46f580-6f7a-4e6d-dc5b-4d8084d70c8f"
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search gene x ###########\n",
        "\n",
        "beam_size = 30\n",
        "conti = True\n",
        "idd = 9\n",
        "t = 0\n",
        "decode_len = 30\n",
        "\n",
        "eos_id = en_word_to_id['eos']\n",
        "eos_prob = -float('Inf')\n",
        "\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "y_de = np.random.randint(low=0, high=fr_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "#########################################################\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "y_score = np.zeros((beam_size))\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    gene_feed_dict = {input_placeholder: x_in, \n",
        "                      target_placeholder: y_in,\n",
        "                      in_length_placeholder: x_len, \n",
        "                      out_length_placeholder: y_len}                           \n",
        "            \n",
        "    mean, std = sess.run([la_mean, la_std], feed_dict=gene_feed_dict)    # 1*max_len x fr_vocab_size\n",
        "    \n",
        "    la_var = []\n",
        "    log_prob_la = []\n",
        "    for _ in range(latent_num):\n",
        "      eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "      la_var_sample = mean + std*eposida\n",
        "      la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))    # batch_size x max_length x latent_size\n",
        "      la_var.append(la_var_sample)\n",
        "      log_prob_la.append(np.sum(np.log(norm.pdf(la_var_sample))))\n",
        "       \n",
        "    for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          gene_feed_dict = {if_gene_placeholder: True,\n",
        "                            latent_var_placeholder:la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length)),\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "            \n",
        "          x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "          \n",
        "          x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "          y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "          \n",
        "          y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "        \n",
        "        \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)\n",
        "        \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)\n",
        "        \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "        \n",
        "        \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "          \n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        y_de = copy.deepcopy(y_de_new)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Restoring parameters from ./result_0827/model_each_epch.ckpt\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: overflow encountered in exp\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:5: RuntimeWarning: invalid value encountered in true_divide\n",
            "  \"\"\"\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:6: RuntimeWarning: divide by zero encountered in log\n",
            "  \n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "6CKhxVAVMQQb",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Blue Score Functions"
      ]
    },
    {
      "metadata": {
        "id": "-flyQJg1YsX8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def count_ngram(candidate, references, n):\n",
        "    clipped_count = 0\n",
        "    count = 0\n",
        "    r = 0\n",
        "    c = 0\n",
        "    for si in range(len(candidate)):\n",
        "        # Calculate precision for each sentence\n",
        "        ref_counts = []\n",
        "        ref_lengths = []\n",
        "        # Build dictionary of ngram counts\n",
        "        for reference in references:\n",
        "            ref_sentence = reference[si]\n",
        "            ngram_d = {}\n",
        "            words = ref_sentence.strip().split()\n",
        "            ref_lengths.append(len(words))\n",
        "            limits = len(words) - n + 1\n",
        "            # loop through the sentance consider the ngram length\n",
        "            for i in range(limits):\n",
        "                ngram = ' '.join(words[i:i+n]).lower()\n",
        "                if ngram in ngram_d.keys():\n",
        "                    ngram_d[ngram] += 1\n",
        "                else:\n",
        "                    ngram_d[ngram] = 1\n",
        "            ref_counts.append(ngram_d)\n",
        "        # candidate\n",
        "        cand_sentence = candidate[si]\n",
        "        cand_dict = {}\n",
        "        words = cand_sentence.strip().split()\n",
        "        limits = len(words) - n + 1\n",
        "        for i in range(0, limits):\n",
        "            ngram = ' '.join(words[i:i + n]).lower()\n",
        "            if ngram in cand_dict:\n",
        "                cand_dict[ngram] += 1\n",
        "            else:\n",
        "                cand_dict[ngram] = 1\n",
        "        clipped_count += clip_count(cand_dict, ref_counts)\n",
        "        count += limits\n",
        "        r += best_length_match(ref_lengths, len(words))\n",
        "        c += len(words)\n",
        "    if clipped_count == 0:\n",
        "        pr = 0\n",
        "    else:\n",
        "        pr = float(clipped_count) / count\n",
        "    bp = brevity_penalty(c, r)\n",
        "    return pr, bp\n",
        "\n",
        "\n",
        "def clip_count(cand_d, ref_ds):\n",
        "    \"\"\"Count the clip count for each ngram considering all references\"\"\"\n",
        "    count = 0\n",
        "    for m in cand_d.keys():\n",
        "        m_w = cand_d[m]\n",
        "        m_max = 0\n",
        "        for ref in ref_ds:\n",
        "            if m in ref:\n",
        "                m_max = max(m_max, ref[m])\n",
        "        m_w = min(m_w, m_max)\n",
        "        count += m_w\n",
        "    return count\n",
        "\n",
        "\n",
        "def best_length_match(ref_l, cand_l):\n",
        "    \"\"\"Find the closest length of reference to that of candidate\"\"\"\n",
        "    least_diff = abs(cand_l-ref_l[0])\n",
        "    best = ref_l[0]\n",
        "    for ref in ref_l:\n",
        "        if abs(cand_l-ref) < least_diff:\n",
        "            least_diff = abs(cand_l-ref)\n",
        "            best = ref\n",
        "    return best\n",
        "\n",
        "\n",
        "def brevity_penalty(c, r):\n",
        "    if c > r:\n",
        "        bp = 1\n",
        "    else:\n",
        "        bp = math.exp(1-(float(r)/c))\n",
        "    return bp\n",
        "\n",
        "\n",
        "def geometric_mean(precisions):\n",
        "    return (reduce(operator.mul, precisions)) ** (1.0 / len(precisions))\n",
        "\n",
        "\n",
        "def sel_sentence_bleu(candidate, references):\n",
        "    precisions = []\n",
        "    for i in range(4):\n",
        "        pr, bp = count_ngram(candidate, references, i+1)\n",
        "        precisions.append(pr)\n",
        "    bleu = geometric_mean(precisions) * bp\n",
        "    return bleu"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcCYvSrNMpDp",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Translate Sentence"
      ]
    },
    {
      "metadata": {
        "id": "gJtlxW4cR0Qv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def cal_kl_loss(mean,log_var,var ):\n",
        "  kl_loss = 1 + log_var - np.square(mean) - var                      # batch_size*max_length x latent_size\n",
        "  kl_loss = -0.5 * np.sum(kl_loss)                               # batch_size*max_length x 1\n",
        "#   kl_div_loss = np.reshape(kl_div_loss, (batch_size, max_length))        # batch_size x max_length\n",
        "#   kl_div_loss = np.sum(kl_div_loss, axis=1)\n",
        "  return kl_loss\n",
        "\n",
        "def id_to_word(words, word_to_id, max_length):\n",
        "  k=0\n",
        "  sen_len=30\n",
        "  sens = [\"\" for x in range(max_length+1)]\n",
        "  for key in word_to_id.keys():\n",
        "    for p in range(max_length):\n",
        "      if words[p] == word_to_id[key]:\n",
        "        sens[p] = key\n",
        "      if words[p] == word_to_id['eos'] and k==0:\n",
        "        sen_len = p\n",
        "        k=k+1\n",
        "  return sens, sen_len\n",
        "\n",
        "def find_next_word_beam_gene(logits_y, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] \n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave\n",
        "\n",
        "def find_next_word_beam_gene_first(logits_y, log_prob_y, log_prob_la, t):\n",
        "\n",
        "  lower_ob = []\n",
        "  for l in range(latent_num):           \n",
        "    prob_y = np.exp(logits_y[l,t])/np.sum(np.exp(logits_y[l,t]))    \n",
        "    log_prob_y_t = np.log(prob_y)     \n",
        "    lower_ob.append(log_prob_y_t)\n",
        "  \n",
        "  lower_to = 0\n",
        "  for l in range(latent_num):\n",
        "    lower_to = lower_to + lower_ob[l] + (log_prob_y[l] + log_prob_la[l])[0]\n",
        "  lower_ave = lower_to/latent_num\n",
        "  \n",
        "  return lower_ave\n",
        "\n",
        "def output_sentence(idd,x_de):\n",
        "  ########## \"English\" ##########\n",
        "  origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(ori_len):\n",
        "     or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "  print(\"x:\")\n",
        "  print(or_sens_str)\n",
        "\n",
        "  or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "  or_sens_str = \" \"\n",
        "  for p in range(or_len):\n",
        "    or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "  print(\"x_re:\")\n",
        "  print(or_sens_str)\n",
        "  \n",
        "  en_bleu = sentence_bleu([origin_sens[:ori_len]],or_sens[:or_len],weights=(1, 0, 0, 0))\n",
        "  print(en_bleu)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WSc3TADSi3EK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "756f68e0-a3d9-480a-904f-356b1281d8c6"
      },
      "cell_type": "code",
      "source": [
        "########## \"English\" ##########\n",
        "idd = 144\n",
        "origin_sens, ori_len = id_to_word(en_input[idd], en_word_to_id, max_length)\n",
        "or_sens_str = \" \"\n",
        "for p in range(ori_len):\n",
        "  or_sens_str = or_sens_str + \" \" + origin_sens[p]\n",
        "print(\"x:\")\n",
        "print(or_sens_str)"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "x:\n",
            "  I strongly urge those other businesses that have not as yet joined this initiative to do so , without delay .\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vu5ZNzW8RIZN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "########## Beam Search translate x ###########\n",
        "\n",
        "beam_size = 50\n",
        "idd = 144\n",
        "decode_len = 30\n",
        "\n",
        "#### input y\n",
        "x_true = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "x_true_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "\n",
        "y_in = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "\n",
        "#### initialize x\n",
        "#x_in = np.concatenate((np.random.randint(low=0, high=en_vocab_size, size=(beam_size, x_len[0])), np.reshape(np.asarray(en_word_to_id['eos']),[1,1]), en_word_to_id['pad']*np.ones((1,max_length-x_len[0]-1),dtype=np.int32)), axis=1)\n",
        "#x_in = np.random.randint(low=0, high=en_vocab_size, size=(1,  max_length))\n",
        "x_in = en_word_to_id['<OOV>']*np.ones((1,  max_length),dtype=np.int32)\n",
        "x_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#### define useful variables\n",
        "x_prob_next_word = np.ones((beam_size, en_vocab_size),dtype=np.float32)\n",
        "x_de_new = np.zeros(x_de.shape, dtype=np.int32)\n",
        "x_score = np.zeros((beam_size))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for h in range(15):\n",
        "      \n",
        "      x_score = np.zeros((beam_size))\n",
        "      \n",
        "      #### sample latent variables\n",
        "      gene_feed_dict = {input_placeholder: x_in, \n",
        "                        target_placeholder: y_in}                           \n",
        "            \n",
        "      mean, logvar, var, std = sess.run([la_mean, la_log_var, la_var, la_std], feed_dict=gene_feed_dict)                    # 1*max_len x fr_vocab_size\n",
        "      kl_div = cal_kl_loss(mean, logvar, var)\n",
        "      \n",
        "      la_vari = []\n",
        "      log_prob_la = []\n",
        "      for _ in range(latent_num):\n",
        "        eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "        la_var_sample = mean + std*eposida\n",
        "        la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))   # batch_size x max_length x latent_size\n",
        "        la_vari.append(la_var_sample)   \n",
        "        log_prob_la.append(np.sum(norm.logpdf(la_var_sample)))\n",
        "\n",
        "      la_vari = np.stack(la_vari,axis=0)                                                     # latent_num x batch_size x max_length x latent_size\n",
        "      log_prob_la = np.stack(log_prob_la,axis=0)                                           # latent_num x 1\n",
        "    \n",
        "      #### obtain log p(y|z)\n",
        "      gene_feed_dict = {latent_var_placeholder:la_vari,\n",
        "                        target_placeholder: y_in,\n",
        "                        input_placeholder: x_true,\n",
        "                        in_length_placeholder: x_true_len,\n",
        "                        out_length_placeholder: y_len}\n",
        "\n",
        "      log_prob_x, log_prob_y = sess.run([log_liki_x_to,log_liki_y_to], feed_dict=gene_feed_dict)                       # latent_num x 1\n",
        "      \n",
        "      ind = np.argsort(log_prob_y[:,0])[-10:]\n",
        "      new_la_var = copy.deepcopy(la_vari[ind])\n",
        "      for i in range(4):\n",
        "        new_la_var = np.concatenate((new_la_var,la_vari[ind]),axis=0)\n",
        "      \n",
        "      print(kl_div)\n",
        "      print(np.sum(log_prob_la)/latent_num)\n",
        "      print(np.sum(log_prob_x)/latent_num)\n",
        "      print(np.sum(log_prob_y)/latent_num)\n",
        "      \n",
        "      #### beam search for x\n",
        "    \n",
        "      for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          gene_feed_dict = {latent_var_placeholder:new_la_var,\n",
        "                            input_placeholder: np.reshape(x_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_x = sess.run(logits_gene_x_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          if t==1:\n",
        "            x_prob_next_word[j] = find_next_word_beam_gene_first(logits_x, log_prob_y, log_prob_la, t)          \n",
        "            x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          else:  \n",
        "            x_prob_next_word[j] = find_next_word_beam_gene(logits_x, t)          \n",
        "            x_prob_next_word[j] = x_prob_next_word[j] + x_score[j]\n",
        "          \n",
        "        x_beam_id = np.argmax(x_prob_next_word, axis=0)        \n",
        "        x_prob_next_word_beam = np.max(x_prob_next_word, axis=0)     \n",
        "        x_next_word_id = np.argsort(x_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "              \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          x_beam_id_j = x_beam_id[x_next_word_id[j]]\n",
        "          \n",
        "          x_word_id_j = x_next_word_id[j]\n",
        "          \n",
        "          x_de_new[j] = copy.deepcopy(x_de[x_beam_id_j])\n",
        "              \n",
        "          x_de_new[j,t] = copy.deepcopy(x_word_id_j)\n",
        "          \n",
        "          x_score[j] = copy.deepcopy(x_prob_next_word_beam[x_word_id_j])\n",
        "        \n",
        "        x_de = copy.deepcopy(x_de_new)\n",
        "        \n",
        "      #### at the end of beam search\n",
        "      or_sens, or_len = id_to_word(x_de[-1], en_word_to_id, max_length)\n",
        "      or_sens_str = \" \"\n",
        "      for p in range(max_length):\n",
        "        or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "      print(\"x_tran:\")\n",
        "      print(or_sens_str)\n",
        "    \n",
        "      x_in[0] = copy.deepcopy(x_de[-1])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Uoe0cOtZYGmi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "######### Beam Search translate x ###########\n",
        "\n",
        "beam_size = 50\n",
        "idd = 144\n",
        "decode_len = 30\n",
        "\n",
        "#### input x\n",
        "x_in = np.reshape(np.copy(en_input[idd]), (1, max_length))\n",
        "x_len = np.reshape(np.copy(en_input_len[idd]), (1,))\n",
        "y_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "y_true = np.reshape(np.copy(fr_output[idd]), (1, max_length))\n",
        "y_true_len = np.reshape(np.copy(fr_output_len[idd]), (1,))\n",
        "\n",
        "#### initialize x\n",
        "y_in = fr_word_to_id['<OOV>']*np.ones((1,  max_length),dtype=np.int32)\n",
        "y_de = np.random.randint(low=0, high=en_vocab_size, size=(beam_size, max_length))\n",
        "\n",
        "#### define useful variables\n",
        "y_prob_next_word = np.ones((beam_size, fr_vocab_size),dtype=np.float32)\n",
        "y_de_new = np.zeros(y_de.shape, dtype=np.int32)\n",
        "y_score = np.zeros((beam_size))\n",
        "\n",
        "\n",
        "#########################################################\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  \n",
        "    saver = tf.train.Saver()\n",
        "    saver.restore(sess, path1)\n",
        "    \n",
        "    for h in range(15):\n",
        "      \n",
        "      y_score = np.zeros((beam_size))      \n",
        "            \n",
        "      #### sample latent variables\n",
        "      gene_feed_dict = {input_placeholder: x_in, \n",
        "                        target_placeholder: y_in}                           \n",
        "            \n",
        "      mean, logvar, var, std = sess.run([la_mean, la_log_var, la_var, la_std], feed_dict=gene_feed_dict)                    # 1*max_len x fr_vocab_size\n",
        "      kl_div = cal_kl_loss(mean, logvar, var)\n",
        "      \n",
        "      la_vari = []\n",
        "      log_prob_la = []\n",
        "      for _ in range(latent_num):\n",
        "        eposida = np.random.normal(size = np.shape(la_std), loc=0.0, scale=1)\n",
        "        la_var_sample = mean + std*eposida\n",
        "        la_var_sample = np.reshape(la_var_sample, (batch_size, max_length, latent_size))   # batch_size x max_length x latent_size\n",
        "        la_vari.append(la_var_sample)   \n",
        "        log_prob_la.append(np.sum(norm.logpdf(la_var_sample)))\n",
        "\n",
        "      la_vari = np.stack(la_vari,axis=0)                                                     # latent_num x batch_size x max_length x latent_size\n",
        "      log_prob_la = np.stack(log_prob_la,axis=0)                                           # latent_num x 1\n",
        "    \n",
        "      #### obtain log p(y|z)\n",
        "      gene_feed_dict = {latent_var_placeholder:la_vari,\n",
        "                        target_placeholder: y_true,\n",
        "                        input_placeholder: x_in,\n",
        "                        in_length_placeholder: x_len,\n",
        "                        out_length_placeholder: y_true_len}\n",
        "\n",
        "      log_prob_x, log_prob_y = sess.run([log_liki_x_to,log_liki_y_to], feed_dict=gene_feed_dict)                       # latent_num x 1\n",
        "      \n",
        "      ind = np.argsort(log_prob_x[:,0])[-10:]\n",
        "      new_la_var = copy.deepcopy(la_vari[ind])\n",
        "      for i in range(4):\n",
        "        new_la_var = np.concatenate((new_la_var,la_vari[ind]),axis=0)\n",
        "      \n",
        "#       new_la_var = la_vari\n",
        "    \n",
        "      print(kl_div)\n",
        "      print(np.sum(log_prob_la)/latent_num)\n",
        "      print(np.sum(log_prob_x)/latent_num)\n",
        "      print(np.sum(log_prob_y)/latent_num)\n",
        "      \n",
        "      #### beam search for y\n",
        "    \n",
        "      for t in range(decode_len):\n",
        "      \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          gene_feed_dict = {latent_var_placeholder: new_la_var,\n",
        "                            target_placeholder: np.reshape(y_de[j], (1,max_length))}\n",
        "                                                                           \n",
        "          logits_y = sess.run(logits_gene_y_to, feed_dict=gene_feed_dict)\n",
        "          \n",
        "          if t==1:\n",
        "            y_prob_next_word[j] = find_next_word_beam_gene_first(logits_y, log_prob_x, log_prob_la, t)          \n",
        "            y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "          else:  \n",
        "            y_prob_next_word[j] = find_next_word_beam_gene(logits_y, t)          \n",
        "            y_prob_next_word[j] = y_prob_next_word[j] + y_score[j]\n",
        "         \n",
        "        y_beam_id = np.argmax(y_prob_next_word, axis=0)        \n",
        "        y_prob_next_word_beam = np.max(y_prob_next_word, axis=0)     \n",
        "        y_next_word_id = np.argsort(y_prob_next_word_beam[2:])[-beam_size:] + 2\n",
        "              \n",
        "        for j in range(beam_size):\n",
        "          \n",
        "          y_beam_id_j = y_beam_id[y_next_word_id[j]]\n",
        "          \n",
        "          y_word_id_j = y_next_word_id[j]\n",
        "          \n",
        "          y_de_new[j] = copy.deepcopy(y_de[y_beam_id_j])\n",
        "              \n",
        "          y_de_new[j,t] = copy.deepcopy(y_word_id_j)\n",
        "          \n",
        "          y_score[j] = copy.deepcopy(y_prob_next_word_beam[y_word_id_j])\n",
        "        \n",
        "        y_de = copy.deepcopy(y_de_new)\n",
        "        \n",
        "      #### at the end of beam search\n",
        "      or_sens, or_len = id_to_word(y_de[-1], fr_word_to_id, max_length)\n",
        "      or_sens_str = \" \"\n",
        "      for p in range(max_length):\n",
        "        or_sens_str = or_sens_str + \" \" + or_sens[p]\n",
        "      print(\"y_tran:\")\n",
        "      print(or_sens_str)\n",
        "      \n",
        "      y_in[0] =  y_de[-1]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}